diff --git a/src/llama-quant.cpp b/src/llama-quant.cpp
index a00af7a1..fc895ef8 100644
--- a/src/llama-quant.cpp
+++ b/src/llama-quant.cpp
@@ -178,6 +178,13 @@ static void llama_tensor_dequantize_impl(
 static ggml_type llama_tensor_get_type(quantize_state_impl & qs, ggml_type new_type, const ggml_tensor * tensor, llama_ftype ftype) {
     const std::string name = ggml_get_name(tensor);
 
+    static std::unordered_map<std::string, bool> fallback_set;
+
+    // If fallback was set for this tensor, do not allow override
+    if (fallback_set.count(name) && fallback_set[name]) {
+        return new_type;
+    }
+
     // TODO: avoid hardcoded tensor names - use the TN_* constants
     const llm_arch arch = qs.model.arch;
     const auto       tn = LLM_TN(arch);
@@ -460,6 +467,7 @@ static ggml_type llama_tensor_get_type(quantize_state_impl & qs, ggml_type new_t
         }
         LLAMA_LOG_WARN(" - using fallback quantization %s\n", ggml_type_name(new_type));
         ++qs.n_fallback;
+        fallback_set[name] = true;
     }
 
     return new_type;
@@ -1047,3 +1055,4 @@ uint32_t llama_model_quantize(
 
     return 0;
 }
+
