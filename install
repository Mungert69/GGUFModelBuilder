#!/usr/bin/env bash
set -euo pipefail

# Defaults
VENV_DIR="venv"
LLAMA_DIR_FROM_FLAG=""
NONINTERACTIVE=0

usage() {
  cat <<'USAGE'
Usage: ./install.sh [--llama-dir /path/to/llama.cpp] [--venv venv] [--yes]
  --llama-dir   Path to your local llama.cpp checkout (will be saved to .env)
  --venv        Virtualenv directory name (default: venv)
  --yes         Non-interactive; skip prompts and apt confirmations
USAGE
}

while [[ $# -gt 0 ]]; do
  case "$1" in
    --llama-dir) LLAMA_DIR_FROM_FLAG="$2"; shift 2;;
    --venv) VENV_DIR="$2"; shift 2;;
    --yes|-y) NONINTERACTIVE=1; shift;;
    -h|--help) usage; exit 0;;
    *) echo "Unknown arg: $1"; usage; exit 1;;
  esac
done

# Ensure python3-venv is available
if ! python3 -c "import venv" &>/dev/null; then
  echo "[INFO] python3-venv is not installed. Installing..."
  if [[ $NONINTERACTIVE -eq 1 ]]; then
    sudo apt update -y && sudo apt install -y python3-venv
  else
    sudo apt update && sudo apt install -y python3-venv
  fi
else
  echo "[INFO] python3-venv is already installed."
fi

echo "[1/7] Creating virtual environment at '$VENV_DIR'..."
python3 -m venv "$VENV_DIR"

echo "[2/7] Activating virtual environment..."
# shellcheck disable=SC1090
source "$VENV_DIR/bin/activate"

echo "[3/7] Upgrading pip..."
python -m pip install --upgrade pip setuptools wheel

echo "[4/7] Installing base Python deps for your project..."
python -m pip install \
  python-dotenv huggingface_hub llama-cpp-python gguf redis hf_xet

echo "[5/7] Installing system deps for audio (optional, used by your installer)..."
if [[ $NONINTERACTIVE -eq 1 ]]; then
  sudo apt-get update -y && sudo apt-get install -y espeak libsndfile1 ffmpeg
else
  sudo apt-get update && sudo apt-get install -y espeak libsndfile1 ffmpeg
fi

# Resolve llama.cpp dir (prompt only if not provided)
if [[ -z "$LLAMA_DIR_FROM_FLAG" ]]; then
  if [[ $NONINTERACTIVE -eq 1 ]]; then
    echo "[WARN] --llama-dir not provided in non-interactive mode; skipping llama.cpp requirements."
  else
    read -rp "[INPUT] Path to your llama.cpp repo (or leave blank to skip): " LLAMA_DIR_FROM_FLAG
  fi
fi

LLAMA_DIR_TRIMMED="${LLAMA_DIR_FROM_FLAG%/}"
if [[ -n "${LLAMA_DIR_TRIMMED}" ]]; then
  if [[ ! -d "$LLAMA_DIR_TRIMMED" ]]; then
    echo "[ERROR] llama.cpp dir not found: $LLAMA_DIR_TRIMMED"
    exit 1
  fi

  REQ_UPDATE="$LLAMA_DIR_TRIMMED/requirements/requirements-convert_hf_to_gguf_update.txt"
  REQ_BASE="$LLAMA_DIR_TRIMMED/requirements/requirements-convert_hf_to_gguf.txt"

  echo "[6/7] Installing llama.cpp converter requirements..."
  if [[ -f "$REQ_UPDATE" ]]; then
    python -m pip install -r "$REQ_UPDATE"
  elif [[ -f "$REQ_BASE" ]]; then
    python -m pip install -r "$REQ_BASE"
  else
    echo "[WARN] No converter requirements file found; installing minimal fallback deps..."
    # minimal set used by recent Tekken-capable converters
    python -m pip install mistral_common tiktoken safetensors
  fi

  # Ensure a new-enough Transformers/Tokenizers for Mistral-3 + Tekken
  # (override any older pins a req file might have pulled)
  python -m pip install --upgrade "transformers>=4.46" "tokenizers>=0.22.0"

  # Persist llama.cpp path for downstream scripts/tools
  echo "[INFO] Writing LLAMA_CPP_DIR to .env"
  touch .env
  if grep -q '^LLAMA_CPP_DIR=' .env; then
    sed -i "s|^LLAMA_CPP_DIR=.*|LLAMA_CPP_DIR=$LLAMA_DIR_TRIMMED|" .env
  else
    echo "LLAMA_CPP_DIR=$LLAMA_DIR_TRIMMED" >> .env
  fi
else
  echo "[INFO] Skipping llama.cpp requirements (no directory provided)."
fi

echo "[7/7] Running install_dependencies.py (your original step)..."
python install_dependencies.py <<EOF
1
EOF

echo "âœ… Setup complete."

